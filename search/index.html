<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>| BOGHTW</title><link rel=icon href=/images/favicon.png><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css></head><body><nav><ul class=menu><li><a href=/>主页</a></li><li><a href=/series/>专栏</a></li><li><a href=/categories/>分类</a></li><li><a href=/tags/>标签</a></li><li><a href=/archives/>归档</a></li><li><a href=/search/>搜索</a></li><li><a href=/about/>关于</a></li></ul><hr></nav><p style="margin:1.8rem 0 -1rem;color:#191919">可以根据标题、分类、标签、系列等条目检索本站文章</p><div class=search-container><input type=text id=search-input placeholder=输入关键词搜索... aria-label=搜索框 autocomplete=off><div id=search-debug style=margin-bottom:10px;color:#999></div><div id=search-results></div></div><script src=https://cdn.jsdelivr.net/npm/fuse.js@6.6.2></script><script>function debug(e){document.getElementById("search-debug").textContent=e}let searchIndex=[];debug("索引初始化中..."),searchIndex.push({title:'"CalculationOfCij"',permalink:'"/moleculardynamics/calculation_cij/"',content:`"应力应变法求解弹性系数 现在我要用分子动力学求解有限温度下弹性系数，由于势函数是机器学习势。所以首先，我们应该验证势函数的可靠性。\\n也就是说，首先验证，在0K下，对象是小胞时，通过DFT与MLIP-MD计算得到的Cij的差距。然后在高温下跑大胞的MD，计算有限温度下的Cij\\n原理及编程 这里以四方晶系-1为例，也就是有六个独立的弹性系数。目前，我所见过的所有，应力应变法求解弹性系数的计算思路都是拟合。\\n我们先看一下所要求解的弹性系数矩阵的样子。\\n根据线弹性假设，\\n$$\\\\sigma = C * \\\\varepsilon$$，给出应变，通过线性组合，可以表示出应力，即公式\\n但是，要想使用numpy.linalg.lstsq进行拟合，还需要重新表达一下这关系。\\n因为lstsq的工作原理是B = A * X，根据已知应变A、应力B，用最小二乘法拟合一个为列向量的系数，X，（这里即Cij）\\n现在，我们专注于A矩阵的写法。\\n可以看到，应变A矩阵是由所施加的应变，也即是变形模式，以及弹性系数矩阵的形状共同决定的。（Cij矩阵决定了A矩阵的形状，而变形模式决定了它具体的元素值）\\nImportant 有几点需要澄清，待求解的Cij可能不止6个。比如说，7个的时候，需要X列向量是7行，A矩阵是七列吗？显然，不需要也不能。因为一种应变模式只能提供六个线性方程组，6个应力最多只能包含6个弹性系数的有效信息。这也是为什么，低对称晶系需要更多变形模式的原因。因此，我们在选择变形模式的时候，要合理。对于大于6个弹性系数的体系，要让弹性系数间尽量解耦——保证一个变形模式内，涉及的弹性系数最多为6个。通过选取多个变形模式，比如说两种，这样有2个A矩阵，12个方程，把两个A矩阵堆叠起来，就能求解7个Cij了\\n这里我放上一个具体的脚本，提供一些思路\\nimport numpy as np #针对四方相计算弹性常数C11,C12,C13,C33,C44,C66 #变形幅度 ups = [-0.03,-0.01,0.01,0.03] #两种变形模式 modes = [0,1] Ms = [] for mode in modes: for up in ups: #第一种变形模式 if mode == 0: strain_voigt = np.array([up,0.,0.,0.,0.,0.]) strain_matrix = strain_matrix_1 = np.array([[up, 0., 0.], [0., 0., 0.], [0., 0., 0.]]) #有了应变模式和应变幅度，即可确定M矩阵 #M矩阵的列数是求解弹性常数的个数，行数则是6 M_matrix = np.array([[up,0.,0.,0.,0.,0.], [0.,up,0.,0.,0.,0.], [0.,0.,up,0.,0.,0.], [0.,0.,0.,0.,0.,0.], [0.,0.,0.,0.,0.,0.], [0.,0.,0.,0.,0.,0.]]) Ms.append(M_matrix) #第二种变形模式 if mode == 1: strain_voigt = np.array([0.,0.,up,up,0.,up]) strain_matrix = strain_matrix_1 = np.array([[0., up/2, 0.], [up/2, 0., up/2], [0., up/2, up]]) M_matrix = np.array([[0.,0.,up,0.,0.,0.], [0.,0.,up,0.,0.,0.], [0.,0.,0.,up,0.,0.], [0.,0.,0.,0.,up,0.], [0.,0.,0.,0.,0.,0.], [0.,0.,0.,0.,0.,up]]) Ms.append(M_matrix) Ms = np.array(Ms) Ms_2d = Ms.reshape(-1, 6) # 自动把 8*6=48 行展开 print(Ms_2d.shape) # (48, 6) print(Ms_2d) 代码中的M，就是上述的A矩阵。除此之外，代码是先把变形模式1的不同幅值的A矩阵堆叠起来，然后再把变形模式2的不同幅值的A矩阵堆叠起来。\\n顺序是没有影响的。本质上就是拟合2 * 4 * 6 = 48个应力应变关系涉及的6个Cij。就是把矩阵行交换，是不影响有效信息的。\\n最后，有了堆叠后的应变A矩阵和应力向量，简单的使用lstsq拟合即可。\\n此外，也可以观察到，这些A矩阵中的每一行中，只有一个不为零的值，其实是因为我们的四方晶系比较简单，合适的应变模式选取让Cij之间完全解耦。\\n这种情况下，如果不嫌弃麻烦，也可以从这个48个方程中挑选出对应的，某个Cij与有些应力的关系，单独拟合。\\n0K下的弹性系数 0K下DFT计算略过，使用MLIP也不需要跑MD，只需要对变形进行评估得出应力即可。\\n"`,categories:'["MolecularDynamics"]',tags:[],series:'["gpumd"]',date:'"2025-11-24"'}),searchIndex.push({title:'"机器学习势nep"',permalink:'"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%BF/nep/"',content:`"机器学习势nep 前言 nep的优势在于计算资源上的节省，可以模拟十几万个原子体系的分子动力学。这里放一些处理脚本。\\n处理脚本 处理loss.out 这个脚本是根据官方提供的matlib脚本丢给gpt改的。\\nimport numpy as np import matplotlib.pyplot as plt # 读取数据 loss = np.loadtxt(\\u0026quot;loss.out\\u0026quot;) plt.figure() plt.loglog(loss[:, 1:6], linewidth=2) plt.xlabel('Generation/100', fontsize=15) plt.ylabel('Loss functions', fontsize=15) plt.tick_params(labelsize=15, length=6) plt.legend(['Total','L1-Reg','L2-Reg','Energy-train','Force-train']) plt.tight_layout() plt.show() 主动学习流程 这里，在樊老师建议下，我打算用主动学习的完整流程，做一个纯氢气的机器学习势。算是练手，同时也为了采样。\\n构建初始训练集 这里我没有用AIMD采样，而是采用mace-off的通用势对氢气进行了采样。\\n在20 * 20 * 15 埃的胞里使用 packmol 随机添加了50个氢分子。\\n先是10-300K升温，再300K保温0.5ps。保温区采样了100个结构。\\n然后80个结构正常保留，后20个结构进行扰动生成了40个结构。\\n一共120个结构，用vasp进行单点计算以后，作为我的初始训练集。\\n后续可以发现，我的这120的结构的H-H键局限于0.74左右，没有0.6和0.8或者更偏远的数据。\\n一个turn里要做些什么 训练nep 文件准备：初始训练集，测试集，nep.in，nep.slurm（超算提交任务的脚本）\\n训练一个nep势\\nversion 4 type 1 H cutoff 6 5 n_max 6 6 basis_size 8 8 l_max 4 2 1 neuron 60 lambda_1 0 lambda_2 0.04 lambda_e 1 lambda_f 1 lambda_v 0.1 batch 5000 generation 50000 gpumd 使用nep势采样，MD的初始结构可以变化一些，这里我加大了氢气的密度，选择添加了70个氢分子。\\n由于势的不完美，MD轨迹跑到后期可能会出现很严重的非物理构型。我们需要截取轨迹\\n这是一个很简单的脚本，方便复制粘贴\\nfrom ase.io import read,write db = read('dump.xyz','::') write('waitselect.xyz',db) 由于我的初始训练集纰漏很大，所以我觉得第一个turn的可用构型都用来加入训练集，并对他们中的一部分也进行了微扰。\\n这里是处理轨迹并微扰的脚本，微扰的程度记得根据实际修改，可以通过ovito的RDF计算简单判断一下H-H的范围是否足够多样。\\n#workflow2 #将前80个结构保存，后20个结构扰动，一共得到120个结构 from ase.io import read,write from hiphive.structure_generation import generate_mc_rattled_structures db = read('Hsample5.xyz',':100:') H2trainset = [] for n,at in enumerate(db): if n \\u0026lt; 80: H2trainset.append(at) else: H2trainset.extend(generate_mc_rattled_structures(at,2,rattle_std=0.02,d_min=0.65,n_iter=5)) write('H2trainset.xyz',H2trainset) neptrain 正常来说，对于gpumd跑出来的构型，应该是使用某种主动学习方法筛选特定结构加入到训练集。\\n我所知道的方法有，远点采样和委员会法。\\n而neptrain的select功能可以很方便得实现远点采样。具体来说，就是将gpumd步骤产生的结构按照远点采样原则筛选出一些来。\\n再加入训练集。\\n确保当前路径有名为train.xyz的训练集，nep.txt\\nneptrain select trajectory.xyz -max 100 -d 0.001 -o selected.xyz -d parameter 控制采样的多少，越小采样越多。 -max控制采样上限\\n"`,categories:'["机器学习势"]',tags:[],series:'["nep"]',date:'"2025-11-13"'}),searchIndex.push({title:'"网站更新的command"',permalink:'"/%E7%BD%91%E7%AB%99%E6%9B%B4%E6%96%B0/command/"',content:`"命令备忘录 网页更新 git add . git commit -m 'add a new' git push "`,categories:'["command"]',tags:[],series:'["command"]',date:'"2025-11-13"'}),searchIndex.push({title:'"频率计算"',permalink:'"/dft/frequency/"',content:`"零点能计算 前言 我对零点能的理解非常肤浅，以下仅是从网上捕风捉影而来，用于个人记录。\\n在计算自由能时，看到一些文献的做法是：DFT总能+零点能+熵。\\n零点能是0K下，体系的振动能，可以通过声子频率计算而来。\\n声子计算可能由phonopy完成，但是也有通过单vasp计算的。\\n这里记录一下，只用vasp计算gamma点声子频率的步骤，用于零点能的粗略计算。\\nINCAR ISMEAR = 0 IBRION = 5 NFREE = 2 POTIM = 0.02 NSW = 1 频率计算的关键参数。\\nIBRION = 5启用有限位移法计算声子模。\\nNFREE = 2 设置原子振动的自由度。\\nPOTIM = 0.02 设置振动的幅度。\\nNSW = 1 离子步为1\\n"`,categories:'["DFT"]',tags:[],series:[],date:'"2025-09-08"'}),searchIndex.push({title:'"DOS"',permalink:'"/dft/dos/"',content:`"电子态密度（DOS）计算 INCAR Start Parameters for this run ISTART = 0 ICHARG = 2 PREC = Accurate ALGO = Normal ISMEAR = -5 LCHARG = .TRUE. LREAL = .FALSE. LELF = .TRUE. ISYM = 0 GGA = PS LORBIT = 12 NSW = 0 NBANDS = 520 NEDOS = 1000 Electronic minimisation ENCUT = 480 NELM = 100 NELMIN = 4 EDIFF = 1E-6 ISPIN = 1 Ionic relaxation IBRION = -1 DOS的质量 k点 k点太稀疏，会导致尖锐的点出现。\\n"`,categories:'["DFT"]',tags:[],series:'["后处理"]',date:'"2025-09-03"'}),searchIndex.push({title:'"LAMMPS-thermo"',permalink:'"/moleculardynamics/thermo/"',content:`"thermo的一些特点 出现的位置 虽然fix、thermo、dump、run之间的顺序可能有很多选择，但我更喜欢这个顺序。\\n继承 每个新的run都会默认继承上一个run的thermo配置，如果需要改变，覆盖一下即可。\\n"`,categories:'["MolecularDynamics"]',tags:[],series:'["LAMMPS"]',date:'"2025-08-23"'}),searchIndex.push({title:'"超晶格材料原子尺度建模"',permalink:'"/dft/%E6%8C%87%E5%AE%9Aposcar%E5%8E%9F%E5%AD%90%E7%9A%84%E8%87%AA%E7%94%B1%E5%BA%A6/"',content:`"根据原子的z轴坐标，指定哪些是自由的，哪些是受限制的。\\nfrom ase.constraints import FixAtoms from ase.io import write, read fixed_indices = [] # 要固定的原子索引 atoms = read('POSCAR_BiTe1_Pt') for i in range(len(atoms)): position_z = atoms[i].position[2] if 40 \\u0026gt; position_z \\u0026gt; 15.5: indice = atoms[i].index fixed_indices.append(indice) if 3 \\u0026lt; position_z \\u0026lt; 7: indice = atoms[i].index fixed_indices.append(indice) atoms.set_constraint(FixAtoms(indices=fixed_indices)) write('POSCAR_BiTe1_Pt_sel', atoms, format='vasp') "`,categories:'["DFT"]',tags:[],series:[],date:'"2025-08-15"'}),searchIndex.push({title:'"晶体表面建模"',permalink:'"/dft/themodelingofsurface/"',content:`"晶体表面建模 前言 晶体表面建模中有很多细节。\\n比如说，要建模(111)面，首先要给出切表面时所采用的晶胞，是primitive cell ？还是conventional cell ？还是自己定义的晶胞。\\n以四方氧化锆为例， 在这篇文献中 ，计算得到的最稳定表面是(111)，作者就明确指出，他切表面时采用的是重构后的类CaF2型的晶胞，而非原胞。\\n实际上，对于四方氧化锆，这种晶胞的(111)面与原胞的(101)面是同一个面，这也就是为什么有很多文献说，四方氧化锆的最稳定表面是(101)面。\\n虽然不指出具体的切割的晶胞时，人们可能会默认为是用的原胞，但我个人认为还是明确指出为好，防止引起不必要的误解。\\n其次，除了密勒指数，UV矢量也很重要，即面内的两个坐标轴，他觉得\\n"`,categories:'["DFT"]',tags:[],series:[],date:'"2025-08-14"'}),searchIndex.push({title:'"超晶格材料原子尺度建模"',permalink:'"/dft/themodelingofsuperlattice/"',content:`"超晶格材料原子尺度建模 前言 虽然异质结的建模攻略很多，但大都以Material Studio为主，且对象是表界面。对于周期性的超晶格材料的建模，特别是异质结情况，参考较少，这里我分享一下个人的经验。采用ASE进行建模。\\n异质结的两种材料接触时，哪两个面间的接触，是需要确定的，比如说，根据实验确定，又或者无实验时，根据晶格匹配度确定，尽量保证失配度较低。\\n这里，假设已经确定了，两个材料的晶胞要沿着z轴堆叠。\\n整体思路：\\n1.确定好要合并的两个晶格的具体结构（用translate平移，surface切面，这个顺序好像也能反过来）\\n2.合并晶格（stack用起来还是蛮需要经验的）\\n如何切一个晶面，并生成周期性结构 from ase.build import surface,stack,make_supercell from ase.io import read,write from ase.io.vasp import write_vasp BiTe = read('POSCAR_BiTe') BiTe = make_supercell(BiTe,P=[[2,0,0],[0,1,0],[0,0,1]]) BiTe1 = BiTe.copy() BiTe2 = BiTe.copy() BiTe2.translate([0,0,-2.05985]) BiTe3 = BiTe.copy() BiTe3.translate([0,0,-3.79553]) BiTe1 = surface(BiTe1,indices=(0,0,1),layers=1,periodic=True) BiTe2 = surface(BiTe2,indices=(0,0,1),layers=1,periodic=True) BiTe3 = surface(BiTe3,indices=(0,0,1),layers=1,periodic=True) write_vasp('POSCAR_BiTe1',BiTe1,direct=True,sort=True) write_vasp('POSCAR_BiTe2',BiTe2,direct=True,sort=True) write_vasp('POSCAR_BiTe3',BiTe3,direct=True,sort=True) ase的surface函数可以很简单的实现，这里不赘述了。\\n此外，关于如何确定一个合适的新晶格的大小，也有很多视频讲解，不再赘述。\\n改变晶体结构的原子层顺序 即便是只有一种原子的晶体，其沿某个面的堆垛的时候，可能有不同的层。比如FCC晶体沿001面的堆垛方式就是\\u0026mdash;ABAB\\u0026mdash;，显然与别的物质形成异质结时，会面临一个问题，即是A面还是B面与别的物质接触。\\n我们建模时，需要把A面或B面调整出来。这需要对原子进行整体位移\\n此外，异质结平面内的原子对齐（比如说xy面），也需要对原子进行整体位移。\\n而ASE实现原子整体位移非常简单，只需要用atoms类的translate方法即可，注意使用绝对坐标。\\nfrom ase.build import surface,stack,make_supercell from ase.io import read,write from ase.io.vasp import write_vasp BiTe = read('POSCAR_BiTe') BiTe = make_supercell(BiTe,P=[[2,0,0],[0,1,0],[0,0,1]]) BiTe1 = BiTe.copy() BiTe2 = BiTe.copy() BiTe2.translate([0,0,-2.05985]) BiTe3 = BiTe.copy() BiTe3.translate([0,0,-3.79553]) ASE实现超晶格材料建模 from ase.build import surface,stack,make_supercell from ase.io import read,write from ase.io.vasp import write_vasp BiTe1 = read('POSCAR_BiTe1') BiTe2 = read('POSCAR_BiTe2') BiTe3 = read('POSCAR_BiTe3') Ag = make_supercell(read('POSCAR_Ag'),P=[[2,0,0],[0,2,0],[0,0,3]]) BiTe1_Ag = stack(Ag,BiTe1,axis=2,fix=0.5) BiTe2_Ag = stack(Ag,BiTe2,axis=2,fix=0.5) BiTe3_Ag = stack(Ag,BiTe3,axis=2,fix=0.5) write('POSCAR_BiTe1_Ag',BiTe1_Ag,format='vasp') write('POSCAR_BiTe2_Ag',BiTe2_Ag,format='vasp') write('POSCAR_BiTe3_Ag',BiTe3_Ag,format='vasp') 关于ase中stack函数的原理。 将两个周期性的晶胞合并成一个周期性的晶胞，有一个必须要面对的问题，那就是接触面将不再有周期性。\\n简单来讲，原本的四个周期性面，现在仅剩余两个。\\n然后，作为晶体结构文件的惯例，我们总是把原子放在周期性面上，即晶格边缘处，这样在显示时，会有重复显示的情况。\\n具体来讲，如图，红色格子和蓝色格子，其实只有两层原子，一层z坐标为0.0（分坐标），一层z为0.5，但显示时，z为1的情况也因为周期性显示出来了。\\nstack合并时，其实是把蓝色格子，（也就是第一个atoms对象），它的z=1的周期性面，保留到了最上方，即good情况。\\n那么什么是坏情况呢，坏情况是，红色格子的晶体结构文件中，两层原子坐标为z=0.0，z=1.0。\\n这种bad情况就会导致提到上方的红色原子与蓝色原子出现在同一平面上，不是我们期望的情况。\\nImportant 所以，请保证第二个atoms中，z=1.0或者几乎接近于1.0的原子，全部通过周期性换算成0，这很重要！\\n其实就是，两个晶胞的原子结构文件中，原子堆砌方向应该一致，不能一个从下堆砌到上，而另一个从上堆砌到下。\\n关于distance在控制些什么 distance其实是控制两个接触面间异质结原子的最小距离，会影响最终cell的z方向尺寸以及对原子进行微扰，所以最上和最下的两个面上的对称原子可能由于周期性，变成一层。\\n"`,categories:'["DFT"]',tags:[],series:[],date:'"2025-08-05"'}),searchIndex.push({title:'"随机取代晶体结构中的元素"',permalink:'"/dft/%E9%9A%8F%E6%9C%BA%E5%8F%96%E4%BB%A3%E5%85%83%E7%B4%A0/"',content:`"从一个实例中学习python中的排列组合的实现 ZrO2中掺杂Y元素 一个96原子的晶胞，去掉一个氧空位，再取代两个Zr原子，就是一个通用的YSZ模型（95 原子）。\\n但是Y与Vo的相对位置，是一个麻烦。\\nY的掺杂会不会带来晶胞大小的变化。我这里的处理是根据以往的文献，确定一个可能的构型。\\n然后考虑一下掺杂对晶胞尺寸带来的影响，然后就定了，Y再换其他位置也认为不会再影响了。（因为比起相对位置，肯定有无Y原子才是影响的大头）\\n确定了晶胞尺寸，结构优化后，再把Y换成Zr。然后随机取代，最终确定$C_{32}^2$个结构，计算单点能，用于机器学习势学习。\\n代码 由大G老师支持。\\nfrom ase.io import read from ase.io.vasp import write_vasp from itertools import combinations import os at = read('POSCAR') # 找到所有 Zr 原子的索引 zr_indices = [i for i, atom in enumerate(at) if atom.symbol == 'Zr'] print(f\\u0026quot;找到 {len(zr_indices)} 个 Zr 原子\\u0026quot;) # 创建保存目录 os.makedirs(\\u0026quot;ZrO_Y2_structures\\u0026quot;, exist_ok=True) # 枚举所有 Zr 的两两组合，逐个替换为 Y for count, (i, j) in enumerate(combinations(zr_indices, 2)): new_at = at.copy() new_at[i].symbol = 'Y' new_at[j].symbol = 'Y' # 写入文件 subdir = f\\u0026quot;ZrO_Y2_structures/{count}\\u0026quot; os.makedirs(subdir, exist_ok=True) filename = os.path.join(subdir, \\u0026quot;POSCAR\\u0026quot;) write_vasp(filename, new_at,direct=True,sort=True) print(f\\u0026quot;写入结构：{filename}\\u0026quot;) print(f\\u0026quot;共生成 {count + 1} 个结构。\\u0026quot;) combations类是python中实现组合的高效方式。参数为一个列表，一个长度。返回一个包含若干元组的列表。\\nfor count, (i, j) in enumerate(combinations(zr_indices, 2)):\\n这里用（i，j）对一个元组进行了解包。\\n"`,categories:'["DFT"]',tags:[],series:[],date:'"2025-07-16"'}),searchIndex.push({title:'"COHP"',permalink:'"/dft/cohp/"',content:`"COHP 前言 VASP文件的准备 LOBSTER要求ISYM=0或-1。其他参数没有特殊要求。\\n结构优化\\n略过\\n静态自洽\\n以较为简单的K点设置计算波函数、电荷密度。\\n用于得到WAVECAR、CHGCAR\\n#关键参数 ISTART = 0 ICHARG = 2 NSW = 0 NBANDS = 500\\t#这个值要大，多试试 NEDOS = 1000 后续的非静态自洽的NBANDS和NEDOS尽量和这里保持一致。\\n非静态自洽\\n不再更新波函数和电荷密度，进行一次单次的求解。\\n这里可以设置更密的K点来求态密度，也可以设置特殊路径的K点来求能带结构。\\n#关键参数 ISTART = 1 # 使用前一轮的波函数 ICHARG = 11 # 使用前一轮的电荷密度，不再更新 NSW = 0 LORBIT = 12 NBANDS = 500\\t#这个值要大，多试试 NEDOS = 1000 特别说一下LORBIT参数，这个参数是后处理参数，和VASP的电子计算无关。\\n指示VASP生成：DOSCAR and lm-decomposed PROCAR + phase factors (not recommended)\\n所以只需要最后非静态自洽的时候设置即可。\\n另外，如果对精度不高，或者不考虑成本，我看也有人直接用静态自洽的结果（这里直接设置比较高的K点密度）进行LOBSTER。\\n如果用的ISMEAR = 0，后续要在lobsterin文件中指出。\\nLOBSTER LOBSTER程序完全被一个lobsterin文件控制。\\nCOHPstartEnergy -10 COHPendEnergy 5 usebasisset pbeVaspFit2015 gaussianSmearingWidth 0.05 # 这里就是前面提到如果你INCAR里面ISMEAR=0或者1，这里就要设置gaussianSmearingWidth basisfunctions Ga basisfunctions As cohpGenerator from 2.483 to 2.485 type Ga type As LobsterPy 一个用于后处理LOBSTER输出文件的python软件包。\\nLobsterPy自动键分析的核心算法依赖于LobserEnv。\\n不同于传统的Env，（如Voronoi，筛选近邻依靠角和距离的截断），LobserEnv只靠ICOHPs来确定原子的近邻。\\nLobsterEnv needs only one parameter that influences neighbor selection\\n"`,categories:'["DFT"]',tags:[],series:'["后处理"]',date:'"2025-06-28"'}),searchIndex.push({title:'"用VASP进行第一性原理分子动力学（AIMD）"',permalink:'"/dft/aimd/"',content:`"AIMD nvt系综 ISTART = 0 ENCUT = 480 PREC = Normal ALGO = Fast LREAL = Auto LWAVE = .F. LCHARG = .F. GGA = PS ISMEAR = 0 SIGMA = 0.05 ISPIN = 1 #electronstep NELM = 200 NELMIN = 4 EDIFF = 1E-4 #AIMD IBRION = 0 MDALGO = 2 ISIF = 2 TEBEG = 1500 NSW = 5000 POTIM = 0.5 SMASS = 0.5 IBRION是控制离子步优化算法的参数，进行分子动力学是设置为0。\\nMDALGO这个参数控制恒温器选项。\\nnpt系综 ISTART = 0 ENCUT = 480 PREC = Normal ALGO = Fast LREAL = Auto LWAVE = .F. LCHARG = .F. GGA = PS ISMEAR = 0 SIGMA = 0.05 ISPIN = 1 #electronstep NELM = 200 NELMIN = 4 EDIFF = 1E-4 #AIMD IBRION = 0 MDALGO = 3 ISIF = 3 TEBEG = 1500 NSW = 5000 POTIM = 0.5 PMASS = 50 #O Zr Y H LANGEVIN_GAMMA = 15 5 5 30 LANGEVIN_GAMMA_L = 1 MDALGO = 3使用拉格朗日热浴，支持npt系综。\\nPMASS控制\\n把OUTCAR压缩成.tar.gz文件 这个命令总是忘记， = =。\\ntar -czvf OUTCAR.tar.gz OUTCAR\\n电子步收敛异常的现象 在进行AIMD的时候，出于某些未知的原因，一些构型在进行电子结构优化时，会陷入局部最小值或者未收敛。\\n具体表现为：1.达到了电子步的最大值（未收敛的情况），能量值与其他相似构型有明显差异（未收敛或陷入局部最小）。\\n这样的AIMD数据是坏的，不能进入ML-IAP的训练集。不然训练势的时候会有异常，如图：\\n可以看到，在拟合能量的时候（图3），有两个异常点，这是因为构型的电子结构计算没有收敛，得到的能量也是错误的。\\n当MACE开启第二阶段训练（加大能量权重）时，就会在损失函数上出现很严重的波动现象。\\n所以，在利用AIMD数据时，要进行一个检查，确保电子步没有达到最大值。\\n因为AIMD是一个采样加计算DFT的方法，所以个人觉得，有一两个构型出现不收敛，并不影响其他构型作为数据，把异常点剔除就行了。\\n检查OSZICAR 从OSZICAR中检查控温效果以及能量。\\n具体效果看注释。\\nfrom pymatgen.io.vasp.outputs import Oszicar from matplotlib import pyplot as plt import numpy as np oszicar = Oszicar(\\u0026quot;YSZH/d3/d31/OSZICAR\\u0026quot;) stepVStemp = [] stepVSenergy = [] #遍历轨迹的温度和能量 for trajectory, ionic_step in enumerate(oszicar.ionic_steps): stepVStemp.append((trajectory,ionic_step['T'])) stepVSenergy.append((trajectory,ionic_step['F'])) # 用于打印是否能量有超出特定值 # if ionic_step['F'] \\u0026gt; -920: # print(trajectory) #判断某一个轨迹的电子步是否达到最大，300是根据INCAR调整 for trajectory,electronic_step in enumerate(oszicar.electronic_steps): if len(electronic_step) == 300: print(trajectory) print(len(electronic_step)) print(\\u0026quot;this AIMD data is not converged\\u0026quot;) # 检查第二步电子步中能量与最后一步的变化是否大于5eV，可能是数据异常，这个判据好用，5eV根据体系调整 for trajectory,electronic_step in enumerate(oszicar.electronic_steps): if abs(electronic_step[1]['E']-electronic_step[-1]['E']) \\u0026gt; 5: print(trajectory) print('this AIMD data possibly is Local Minimum') # 绘制温度与步数的关系 plt.figure() #plt.plot(*zip(*stepVStemp), label='Temperature (K)') plt.scatter(*zip(*stepVSenergy), label='Free Energy (eV)') plt.xlabel('MD Step') plt.ylabel('Value') plt.legend() plt.show() # 曾用于电子步中检查dE确保收敛性，但不好用 #for trajectory,electronic_step in enumerate(oszicar.electronic_steps): # electronic_step_bool_lsit = [] # for single_step in electronic_step: # if abs(single_step['dE']) \\u0026lt; 1E-5: # electronic_step_bool = 1 # else: # electronic_step_bool = 0 # electronic_step_bool_lsit.append(electronic_step_bool) # is_non_decreasing = np.all(np.diff(electronic_step_bool_lsit) \\u0026gt;= 0) # if is_non_decreasing == False: # print(trajectory) # print(\\u0026quot;this AIMD data possibly is Local Minimum\\u0026quot;) 通过画图，可以看到AIMD中电子结构优化出现异常（未收敛或陷入局部最小）是比较常见的现象。尽管在一条轨迹中出现较少，但当轨迹数量增多时，难免会混入训练集中。\\n如何把他们提前筛选出来呢？\\n这里，可以检查某一构型对应的电子步是否达到最大，即检查是否收敛。\\n然后对于如何筛选陷入局部最小的结构，则是经验之谈。\\n如图，我发现对于陷入局部最优的结构，他们的能量变化，往往一开始是在全局最优波动，然后出于某些原因，来到了局部最优。\\n这样他们最初几个电子步的能量和最后的电子步能量差会比较大。对于我的结构，我用5eV作为门槛来判断。\\n发现效果不错。\\nImportant 所以检查AIMD的能量是非常有必要的。\\n用python脚本准备单点能的高通量计算 这个脚本被我放在了macetools文件夹下了（自我提醒用）\\n在训练机器学习势时，可能需要对一段轨迹重新进行单点能DFT计算。\\n这至少有两个应用背景：\\n用粗糙AIMD采样后得到的构型，需要重新进行高精度的DFT评估。 用ML-IAP进行MD后的构型，进行DFT评估，用于主动学习。 假设，这个脚本在一个路径下，这个路径放置了需要计算的轨迹（train.xyz）和这一批高通量计算所共用的INCAR、KPOINTS、POTCAR、以及提交任务的脚本（vasp.pbs）\\n我们先在这个路径下创建一个名为singlepoint的文件夹，然后运行脚本如下：\\nfrom ase.io import read from ase.io.vasp import write_vasp import os import shutil path = os.getcwd() singlepoint_path = os.path.join(path,'singlepoint') db = read('train.xyz',':') for number,at in enumerate(db): number_path = os.path.join(singlepoint_path,str(number)) os.makedirs(number_path) #通用文件夹还是在超算上复制吧，自己电脑上复制后再上传太慢了 #INCAR_origin_path = os.path.join(path,'INCAR') #INCAR_path = os.path.join(number_path,'INCAR') #shutil.copy(INCAR_origin_path,INCAR_path) #POTCAR_origin_path = os.path.join(path,'POTCAR') #POTCAR_path =os.path.join(number_path,'POTCAR') #shutil.copy(POTCAR_origin_path,POTCAR_path) #KPOINTS_origin_path = os.path.join(path,'KPOINTS') #KPOINTS_path = os.path.join(number_path,'KPOINTS') #shutil.copy(KPOINTS_origin_path,KPOINTS_path) #PBS_origin_path = os.path.join(path,'vasp.pbs') #PBS_path = os.path.join(number_path,'vasp.pbs') #shutil.copy(PBS_origin_path,PBS_path) POSCAR_path = os.path.join(number_path,'POSCAR') write_vasp(POSCAR_path,at,direct=True,sort=True) 单点能计算INCAR #Start Parameters PREC = N ALGO = Fast ISTART = 0 ICHARG = 2 GGA = PS ISPIN = 1 LREAL = Auto #Electronic Relaxation NELM = 60 NELMIN = 4 EDIFF = 1E-5 LREAL = AUTO ENCUT = 480 #Ionic Relaxation NSW = 0 ISIF = 2 ISMEAR = 0 SIGMA = 0.05 #K KSPACING = 0.5 经过测试，KSPACING=0.5和333的KPOINTS对于我的体系而言，计算得到的能量是差不多的，但是更快些。\\nENCUT尝试等于600，但是时间翻了三倍，算了。\\nsh脚本提交批量提交超算任务 #!/bin/bash # 提交 VASP 任务的循环脚本 # 文件夹名称从 0 到 199 for i in $(seq 0 199); do echo \\u0026quot;进入文件夹 $i\\u0026quot; cd \\u0026quot;$i\\u0026quot; || { echo \\u0026quot;无法进入文件夹 $i\\u0026quot;; exit 1; } echo \\u0026quot;提交任务：qsub vasp.pbs\\u0026quot; cp ../INCAR ./ cp ../KPOINTS ./ cp ../POTCAR ./ cp ../vasp.pbs ./ chmod +x vasp.pbs qsub vasp.pbs cd .. || exit done 结构优化 # Start Parameters PREC = N ALGO = Fast ISTART = 0 ICHARG = 2 GGA = PS ISPIN = 1 LREAL = Auto # Electronic Relaxation NELM = 60 NELMIN = 4 EDIFF = 1E-5 ENCUT = 480 # Ionic Relaxation NSW = 100 IBRION = 2 ISIF = 2 EDIFFG = -0.02 # 力收敛准则 ISMEAR = 0 SIGMA = 0.05 # K-points KSPACING = 0.5 "`,categories:'["DFT"]',tags:[],series:'["AIMD"]',date:'"2025-06-28"'}),searchIndex.push({title:'"Honey"',permalink:'"/%E7%94%A8%E5%BF%83%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/honey/"',content:`"Honey maker 原味轻乳酪芝士蛋糕 来自“糙柔纪软面包”。\\n本次是冰镇口感、不知道新鲜出炉的会怎么样。\\n非常丝滑，入口即化。与常规的蛋糕相比，水感十足。\\n足够甜，包裹了一点点酸。\\n底部硬硬的，据说是饼干。\\n碱水棒 据说是来自德国的碱水面包。\\n大美食家本人的评价是，甚不豪赤。\\n很干，面包本身没有味道，全靠红豆支撑。（是不是隔夜放了一天的缘故？）\\n没有下次，避雷！\\n"`,categories:[],tags:[],series:'["用心记录生活"]',date:'"2025-06-26"'}),searchIndex.push({title:'"DeepMD入门"',permalink:'"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%BF/deepmd%E5%85%A5%E9%97%A8/"',content:`"DeepMD入门 前言 之前一直用的mace，现在来了解一下deepmd吧。\\n1.dp的数据集结构 与mace相对简单的数据集结构相比（只需提供打标签后xyz文件），dp组织它的数据集以一种相对不自由的形式，但也更加清楚明了。\\ndp的数据集是由dpdata处理DFT结果得到的。一个标准的dp数据集路径下，包含了三种文件，分别是：type.raw、type_map.raw和set.000。\\ntype.raw其实就是POSCAR中的原子，只不过把元素符号换成了数字。type_map.raw则是保留了数字到元素符号的映射关系。\\n很困惑为什么要这样设计\\nset.000文件夹则是存放DFT标签的，具体如坐标、能量、力等。\\n2.如何生成数据集 任务：想象一下，你有多个OUTCAR，分别是：\\n1000K下：C、O体系；C、O、H体系\\n2000K下：C、O体系；C、O、H体系\\n如何把他们整合成训练集和数据集呢？\\n每个体系AIMD的OUTCAR包含10000个轨迹。但因为相邻轨迹的结构比较接近，我们决定每100步取一个结构作为数据集。\\n现在你有以下路径，1000K/CO/OUTCAR、1000K/COH/OUTCAR、2000K/CO/OUTCAR、2000K/COH/OUTCAR\\n此外呢，如果把这四体系合并后再划分训练集、验证集，随机性会更大，比如说某个体系的训练集取得很多，而某个体系则几乎全被当作验证集。\\n为了避免这样的事情发生，我们可以先把每个体系划分成小的训练集、验证区，然后把这些小的训练集、验证集合并。\\n这就是总体思路，来看看怎么实现吧。\\nimport dpdata import numpy as np #生成一个列表用于dpdata取子集的索引，dpdata的system并不支持直接的切片规则，很无语 #从第1个结构到1w个，每100步取一个，很灵活，可以自由改变起始，结束和步长 IndicesInitial = list(range(0,10000,100)) #从100个结构的索引中随机取20个作为验证集，20% np.random.seed(42) ValidIndices = list(np.random.choice(IndicesInitial,20,replace=None)) #从数据集中剔除验证集，得到训练集，这里为了实现列表元素进行集合运算，先把他们变成集合 TrainIndices = list(set(IndicesInitial)-set(ValidIndices)) #只读了一个OUTCAR作为例子，适当修改 dpSystem = dpdata.LabeledSystem('1000K/CO/OUTCAR') dpSystemTrain = dpSystem.sub_system(TrainIndices) dpSystemTrain.to(\\u0026quot;deepmd/npy\\u0026quot;, \\u0026quot;trainset/1000K/CO\\u0026quot;, set_size=dpSystemTrain.get_nframes()) dpSystemValid = dpSystem.sub_system(ValidIndices) dpSystemValid.to(\\u0026quot;deepmd/npy\\u0026quot;, \\u0026quot;validset/1000K/CO\\u0026quot;, set_size=dpSystemValid.get_nframes()) set_size=dpSystemTrain.get_nframes(),关于set的尺寸为什么要正好等于所有结构，这个问题追溯起来非常远古。\\n曾经dp的开发者应该希望一个system下有多个set.00x，最后一个set.00x作为测试集。但后来他们放弃了。\\n所以现在的用法是，只要一个set.000即可，至于这个system下的数据用于训练、验证，在input.json中说明即可。\\n顺带一提，dp训练模型时并不要求测试集，mace的话则可以提供。\\n下图是input.json的一部分：\\n把相应路径替换成\\u0026quot;trainset/1000K/CO\\u0026quot;等等即可\\ndp中system的概念，同一个system中，结构涉及的元素和原子个数应该完全相同。\\n3.利用dpdata生成扰动结构 dpdata本身已经提供了很好的 示例 , 这里简单搬运一下，并添加了一个更深入的应用场景。\\nimport dpdata perturbed_system = dpdata.System(\\u0026quot;CONTCAR\\u0026quot;).perturb( pert_num=3, cell_pert_fraction=0.05, atom_pert_distance=0.6, atom_pert_style=\\u0026quot;normal\\u0026quot;, ) print(perturbed_system.data) 利用class system的perturb方法就可以轻松实现。\\n扰动晶胞大小可以模拟受力变形的情况。随机扰动是种很好的方法。\\n但根据计算弹性系数的变形模式施加应变，是不是会让数据集更有意义，同时让训练出的势在计算弹性系数时更具有优势？\\n那么，如何根据晶型确定需要施加的变形模式呢？这需要一定的物理知识，不过好在已经有人帮我们把它做成了软件，名叫 elastool 。\\n这个软件是为Linux设计的，如果装到Windows下要改一下源码才能正确运行。\\n不过我们可以在它的strain_matrix.py文件中，找到我们所需的变形模式。（当然，可以问问大G老师）\\n最后，这些微扰后生成的结构，是拿去做单点能计算得到能量、力（个人认为不需要结构优化，因为本质是提供DFT数据、而不是DFT理论下合理的结构）。\\n如果生成100个结构，怎么把他们整合到一起呢，总不能在dp的input.json中提供一百个训练集的路径吧！\\n其实很简单，用一个循环，把他们的LabeledSystem对象加起来就行了，是的，class system支持直接进行加和。就像ase中的atoms一样。\\n写一个简单的测试：\\nimport dpdata #把拓展名写成OUTCAR是为了让dpdata猜它是OUTCAR，不用指定格式 structure1 = dpdata.LabeledSystem('1.OUTCAR') print(structure1.data['energies']) structure2 = dpdata.LabeledSystem('2.OUTCAR') print(structure2.data['energies']) StructureSet = structure1 + structure2 print(StructureSet.data['energies']) 可以看到，很成功。另外，因为dp对于system的规定是相同原子类型且数量相等，所以如果不满足这个条件的数据，不能加在一起，会报错，也是理所当然的。\\n"`,categories:'["机器学习势"]',tags:[],series:'["deepmd"]',date:'"2025-06-18"'}),searchIndex.push({title:'"LAMMPS-dump"',permalink:'"/moleculardynamics/dump/"',content:`"有用的dump 1. 如何由lammps的轨迹转化为POSCAR 构建机器学习势时，通常需要主动学习。主动学习就需要将用MLIAP跑出来的结果返回到DFT中进行计算。\\n具体来讲，要将分子动力学轨迹转化为第一性原理的输入格式。（如从lammps到vasp）\\nlammps并不直接支持输出POSCAR格式，比较容易想到的思路是让lammps输出xyz文件，再有xyz文件转化为POSCAR。\\n不过麻烦的一点是，lammps输出xyz文件并不带有晶格大小的信息，无法转化为POSCAR。\\n这里，我目前的方案是，先用custom style的dump输出文件，如下：\\ndump 1 all custom 1 dump20nvt id element x y z dump_modify 1 sort id element O Zr Y H 为了避免输出lammps中的type，而不是元素符号，这里修饰一下dump（即 element O Zr Y H），这里O、Zr、Y、H分别对应 type 1、2、3、4\\nsort id 必不可少，保证原子对应。\\n这样得到的dump文件格式，如下：\\n可以看到，同时包含了元素符号、晶格和原子坐标信息。其实这个自定义风格，custom，就是稍加修改的atom风格。\\n剩下的问题就是，如何将lammps的dump文件转化为xyz文件，再由xyz文件转化为POSCAR。\\nOVITO模块读取dump文件比较好用，个人觉得比ase的强，所以这里采用ovito模块，如下：\\nfrom ovito.io import import_file,export_file pipeline = import_file('dump') for frame,data in enumerate(pipeline.frames): if frame \\u0026lt; 10: export_file(data,\\u0026quot;xyz{}\\u0026quot;.format(frame),format=\\u0026quot;xyz\\u0026quot;,columns =[\\u0026quot;Particle Identifier\\u0026quot;, \\u0026quot;Particle Type\\u0026quot;, \\u0026quot;Position.X\\u0026quot;, \\u0026quot;Position.Y\\u0026quot;, \\u0026quot;Position.Z\\u0026quot;]) 这里简单测试一下，只输出了前10个结构。结果很成功。\\n然后就是把xyz文件转化为POSCAR，这里用ase模块就行，如下：\\nfrom ase.io import read,write for i in range(10): at = read(\\u0026quot;xyz{}\\u0026quot;.format(i)) write('POSCAR{}'.format(i),at,format='vasp') "`,categories:'["MolecularDynamics"]',tags:[],series:'["LAMMPS"]',date:'"2025-06-17"'}),searchIndex.push({title:'"LAMMPS中的常见命令"',permalink:'"/moleculardynamics/lammps%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/"',content:`"LAMMPS中的常见命令 前言 虽然说是常见，但也未必常见吧，可能只是我遇到的不懂的、或者觉得有趣的。\\n1.labelmap lablemap atom 1 H 2 O\\n这个命令用于给atom type指定一个映射关系。在用write_data写当前帧的data文件时，文件会含有额外的信息，如下：\\n但只限于data文件，无法影响dump的轨迹文件，所以想要写带有元素符号的轨迹，还是用dump的custom风格比较好。\\n2. variable 想要精通LAMMPS中的variable总是要费一番功夫的，今日有幸认真研究一番（本身又涉及很多别的命令，真似高中时看牛津字典，遇到一个个新单词，好爽快）。\\n不同风格的variable的定义和使用，会有一些差异。\\nvariable name style args ... 这是官方给出的variable语法，简明扼要，variable由名字、风格、参数组成。\\nequal风格\\n它后接一个公式，可以包含：数字、常数、数学算符、内置函数、原子值（atom values）、原子矢量（atom vectors）以及compute/fix/variable的引用。\\natom value = id[i], mass[i], type[i], mol[i], x[i], y[i], z[i], vx[i], vy[i], vz[i], fx[i], fy[i], fz[i], q[i] atom vector = id, mass, type, mol, radius, q, x, y, z, vx, vy, vz, fx, fy, f 这里，原子值是某个原子的信息，而原子矢量则是包含全体原子的信息。\\n值得注意的是，如果涉及到原子值，在atom_style之后，还要打开atom_modify map yes。\\n此外，lammps中i是从1开始的，和python的从0开始规则不一样。\\nequal风格存储了一个公式，当被使用时，将会输出一个单值，它的使用场景：\\nprint、fix print、run every\\n热动力学输出，由thermo_style控制\\nvariable alpha equal mass[1] thermo 100 thermo_style custom step temp pe ke etotal press v_alpha 输出如下：\\n平均fix的输入 atom风格\\n它的定义是和equal风格相同的，不同的是它的使用场景：\\ndump custom 风格时，作为==每原子（per-atom）==的性质输出。\\nvariable alpha atom x+y+z dump\\t1 all custom 100 out1 id element x y z v_alpha 把变量定义为atom风格，最大的用处就是可以直接调用一些per-atom性质，比如原子的x、y、z坐标。\\nLAMMPS会知道，你写的x，是指原子的x坐标，并且在用dump custom输出per-atom性质时，还会随着原子的变化而变化。\\n利用以上两行，我们可以额外输出每个原子的x、y、z坐标和（虽然可能没有物理意义），如下：\\n平均fix的输入（ fix ave/chunk and fix ave/atom ）\\nvector风格\\n它的定义是和equal风格相同的，不同的是它的使用场景：\\n它可以作为各种平均fix的输入。\\n作为一个矢量，也可以看作python中的列表理解，它的元素可以作为热动力学量输出。\\nvariable alpha vector [1,2,3,4] thermo 100 thermo_style custom step temp pe ke etotal press v_alpha[4] 这里，对于vector风格的variable，热动力学输出只接受它的元素，而不能直接输出它本身。\\n这里也牵出了对于vector风格的数据中的元素的调用方式，就是用[i]。\\n“immediate” variables\\nLAMMPS中的$有大学问。\\n$可以跟{}，此时{}内的内容需要是一个多字母变量名。 比如说定义了一个variable，名叫name，那么\${name}就可以调用它。\\n这里强调，{}内必须且只能是变量名，不能进行计算，不可以是公式。\\n$可以跟()，这里才是所谓的定义一个“immediate” variables，并且它是equal风格的。 ()内可以进行计算，也可以引用变量（需要用v_name或内置变量）\\n比如，在用nvt系综时，热浴的温度阻尼常用100个时间步。，当我们timestep 0.001定义完时间步以后，就会有一个内置变量dt。\\n我们可以用如下命令\\nfix 1 all nvt temp 300.0 300.0 $(100.0*dt)\\n但如果用：fix 1 all nvt temp 300.0 300.0 \${100.0*dt}，就会报错。\\n{}和()是完全不一样的概念，一定不要搞混。\\n$可以直接跟一个字母，这个字母需要是已定义的单字母变量的名。 3. fix ave/time fix ID group-ID ave/time Nevery Nrepeat Nfreq value1 value2 ... keyword args ...\\n用于统计平均。\\n这里以fix ave all ave/time 1 1 1000 v_tdiff ave running为例子，\\nNfreq控制多少步统计一次，\\nNrepeat控制某次（每经历Nfreq步时）会有多少个数据用于计算平均，\\nNevery控制某次（每经历Nfreq步时）用于取平均的数据的步长间隔。\\n在这个例子中，其实是每1000步取一个数据。\\n如果改成1 3 1000，就是每1000步，取第998，999，1000步的数据的平均值。\\nave running的作用是：\\n最后输出的f_ave，是所有Nfreq间隔的平均的平均。\\n4. fix ave/chunk fix ID group-ID ave/chunk Nevery Nrepeat Nfreq chunkID value1 value2 ... keyword args ...\\n这个命令本质上还是求时间的平均，只是可以很方便的对chunk中所有的区域同时计算。\\n以fix 2 all ave/chunk 10 100 1000 layers v_temp file profile.mp为例子，\\n这里是每一千步，取100个数据的平均，数据间隔为10.\\n可以看到，这是输出间隔为1000的profile.mp\\n第三列是原子数，第四列是温度。\\n这些值其实是这1000步内的平均，平均方法是10，100，1000.\\n5. change_box change_box group-ID parameter args ... keyword args ...\\n这个命令我一般用于npt保温后，根据npt中盒子平均尺寸，调整盒子大小，用于下一步的nvt过程。\\n具体代码如下，自定义变量\\n每2000步统计一次盒子尺寸，统计方式为取这2000步内间隔为10的200个数据的平均。\\n我们最终拟定的盒子平均尺寸（lxavg等）是由npt过程中最后一个2000步决定的。\\nvariable mylx equal lx variable myly equal ly variable mylz equal lz fix boxout all ave/time 10 200 2000 v_mylx v_myly v_mylz file box_size.dat # #npt run # variable lxavg equal f_boxout[1] variable lyavg equal f_boxout[2] variable lzavg equal f_boxout[3] change_box all x final 0.0 \${lxavg} y final 0.0 \${lyavg} z final 0.0 \${lzavg} remap units box unfix boxout "`,categories:'["MolecularDynamics"]',tags:[],series:'["LAMMPS"]',date:'"2025-06-17"'}),searchIndex.push({title:'"LAMMPS剪切模拟"',permalink:'"/moleculardynamics/lammps%E5%89%AA%E5%88%87%E6%A8%A1%E6%8B%9F/"',content:`"LAMMPS用于剪切模拟的in文件 # 3d metal shear simulation units\\tmetal boundary\\ts s p atom_style\\tatomic lattice\\tfcc 3.52 region\\tbox block 0 16.0 0 10.0 0 2.828427 create_box\\t3 box lattice\\tfcc 3.52 orient\\tx 1 0 0 orient y 0 1 1 orient z 0 -1 1 \\u0026amp; origin 0.5 0 0 create_atoms\\t1 box pair_style\\team pair_coeff\\t* * Ni_u3.eam neighbor\\t0.3 bin neigh_modify\\tdelay 5 region\\tlower block INF INF INF 0.9 INF INF region\\tupper block INF INF 6.1 INF INF INF group\\tlower region lower group\\tupper region upper group\\tboundary union lower upper group\\tmobile subtract all boundary set\\tgroup lower type 2 set\\tgroup upper type 3 # void #region\\tvoid cylinder z 8 5 2.5 INF INF #delete_atoms\\tregion void # temp controllers compute\\tnew3d mobile temp compute\\tnew2d mobile temp/partial 0 1 1 # equilibrate velocity\\tmobile create 300.0 5812775 temp new3d fix\\t1 all nve fix\\t2 boundary setforce 0.0 0.0 0.0 fix\\t3 mobile temp/rescale 10 300.0 300.0 10.0 1.0 fix_modify\\t3 temp new3d thermo\\t25 thermo_modify\\ttemp new3d timestep\\t0.001 run\\t100 # shear velocity\\tupper set 1.0 0 0 velocity\\tmobile ramp vx 0.0 1.0 y 1.4 8.6 sum yes unfix\\t3 fix\\t3 mobile temp/rescale 10 300.0 300.0 10.0 1.0 fix_modify\\t3 temp new2d #dump\\t1 all atom 100 dump.shear #dump\\t2 all image 100 image.*.jpg type type \\u0026amp; #\\taxes yes 0.8 0.02 view 0 0 zoom 1.5 up 0 1 0 adiam 2.0 #dump_modify\\t2 pad 4 #dump\\t3 all movie 100 movie.mpg type type \\u0026amp; #\\taxes yes 0.8 0.02 view 0 0 zoom 1.5 up 0 1 0 adiam 2.0 #dump_modify\\t3 pad 4 thermo\\t100 thermo_modify\\ttemp new2d reset_timestep\\t0 run\\t3000 compute ID group-ID temp 计算某个原子群的温度\\ncompute\\tnew3d mobile temp\\n一个名为new3d的compute，计算mobile的温度。\\ncompute\\tnew2d mobile temp/partial 0 1 1\\n因为lammps的温度与动能挂钩，可以实现只统计特定方向的动能来计算温度。\\n这段代码，指的是只统计y、z方向的动能并计算温度（不统计x方向，所以对应 0）\\nvelocity velocity\\tmobile ramp vx 0.0 1.0 y 1.4 8.6 sum yes\\nvelocity group-ID ramp style\\n设置速度均匀变化。vx 0.0 1.0 指x方向的速度从0均匀变化到1.0； y 1.4 8.6 是指当y方向坐标从1.4变化到8.6时，速度发生变化。sum yes 表示这个命令产生的是速度会加到之前已有的速度上，而不是取代。\\ntemp 关键词\\n用法：temp value = temperature compute ID\\n例子1：velocity\\tmobile create 300.0 5812775 temp new3d\\n因为速度和温度挂钩，使用velocity时，可以自定义温度的计算方式，就是使用temp关键词。\\n例子2：\\ncompute\\tnew2d mobile temp/partial 0 1 1\\nfix\\t3 mobile temp/rescale 10 300.0 300.0 10.0 1.0 fix_modify\\t3 temp new2d\\n这三段代码可以实现对y、z方向的控温。fix涉及温度时，也可以用fix_modify自定义温度。\\n"`,categories:'["MolecularDynamics"]',tags:[],series:'["LAMMPS"]',date:'"2025-06-14"'}),searchIndex.push({title:'"超算操作"',permalink:'"/%E8%B6%85%E7%AE%97/%E8%B6%85%E7%AE%97%E6%93%8D%E4%BD%9C/"',content:`"超算中心 安装nvcc和pytorch 先用conda安装cuda-toolkit，-不能省略。这里我选择的12.6版本，之前试了12.9，编译lammps的时候报错了。\\n再用pip安装pytorch。\\n用conda安装nvcc后，lammps的nvcc_wrapper可能找不到cuda_runtime.h，可以手动指定\\nexport CPLUS_INCLUDE_PATH=/home-ssd/Users/nsgm_zcx/miniconda3/envs/mace/targets/x86_64-linux/include:$CPLUS_INCLUDE_PATH\\n加载模块 source /home-ssd/hpc/ini_module.bash\\nexport MODULEPATH=/home-ssd/hpc/modulefiles:/home-ssd/Soft/modules/modulefiles/Soft\\nexport MODULEPATH=/home-ssd/hpc/modulefiles\\nmodule av查看当前可用模块\\n联网 ssh gn001进入编译节点\\nsource /home-ssd/Soft/modules/bashrc\\nmodule load proxy/proxy\\nopenmpi 下载后解压，进入发行版的文件夹，然后创建一个build文件夹用于编译和安装。\\n安装命令，在build文件夹中运行：../configure --prefix=/home-ssd/Users/nsgm_zcx/openmpi-5.0.5/build --with-slurm 2\\u0026gt;\\u0026amp;1 | tee config.out\\nexport PATH=/home-ssd/Users/nsgm_zcx/openmpi-5.0.5/build/bin:$PATH\\nexport LD_LIBRARY_PATH=/home-ssd/Users/nsgm_zcx/openmpi-5.0.5/build/lib:$LD_LIBRARY_PATH\\nLAMMPS cmake的编译缓存可能会记录错误的依赖路径，即使自己下载了新的openmpi，也会因为没有及时更新而报错找不到。\\n所以要删干净，再进行一次cmake。\\nexport PATH=/home-ssd/Users/nsgm_zcx/lammps-develop/build-mliap:$PATH\\nzlib 安装完cuda-toolkit和pytorch后，conda自动安装的zlib可能只是libzlib，只有库，没有头文件，需要手动安装一个完整的zlib，保证lammps编译的时候不报错。\\nconda install zlib\\n提交任务 gpu任务\\n#!/bin/bash #SBATCH -o job.%j.out #SBATCH -e job.%j.out #SBATCH -J lmp_job_gpu #SBATCH --partition=gpu #SBATCH -N 1 #SBATCH -G 1 #SBATCH --gres=gpu:1 #SBATCH --exclusive export PATH=/home-ssd/Users/nsgm_zcx/openmpi-5.0.5/build/bin:$PATH export LD_LIBRARY_PATH=/home-ssd/Users/nsgm_zcx/openmpi-5.0.5/build/lib:$LD_LIBRARY_PATH export PATH=/home-ssd/Users/nsgm_zcx/lammps-develop/build-mliap:$PATH source /home-ssd/Users/nsgm_zcx/miniconda3/etc/profile.d/conda.sh conda activate cuda mace_run_train --config parameters.yaml cpu任务\\n#!/bin/bash #SBATCH -o job.%j.out #SBATCH -J lmp_job_gpu #SBATCH --partition=gpu #SBATCH -N 1 #SBATCH -G 1 #SBATCH --exclusive export PATH=/home-ssd/Users/nsgm_zcx/openmpi-5.0.5/build/bin:$PATH export LD_LIBRARY_PATH=/home-ssd/Users/nsgm_zcx/openmpi-5.0.5/build/lib:$LD_LIBRARY_PATH export PATH=/home-ssd/Users/nsgm_zcx/lammps-manybody/lammps-develop/build:$PATH source /home-ssd/Users/nsgm_zcx/miniconda3/etc/profile.d/conda.sh conda activate cuda mpirun -np 64 lmp -in contactangle.in "`,categories:'["超算"]',tags:[],series:[],date:'"2025-06-04"'}),searchIndex.push({title:'"绘制DFT数据和MACE预测值的关系图"',permalink:'"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%BF/dft%E6%95%B0%E6%8D%AEvsmace%E9%A2%84%E6%B5%8B%E5%80%BC/"',content:`"绘制DFT数据和MACE预测值的关系图 前言 本文主要搬运一下 MACE教程其一 ，记录一下如何绘制DFT数据和MACE预测值的关系图，用于评估模型的准确性，加上一些自己的理解。\\n软件：ASE、 aseMolec 、matplotlib、numpy\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\n准备评估函数 MACE官方写好了评估的命令行脚本，我们这里给它一个虚拟参数，方便以编程的方式调用它。\\nfrom mace.cli.eval_configs import main as mace_eval_configs_main import sys def eval_mace(configs, model, output): sys.argv = [\\u0026quot;program\\u0026quot;, \\u0026quot;--configs\\u0026quot;, configs, \\u0026quot;--model\\u0026quot;, model, \\u0026quot;--output\\u0026quot;, output] mace_eval_configs_main() 这里的eval_mace函数接受三个参数，数据集如训练集、测试集的路径、训练好的模型的路径以及输出的文件命。\\n评估数据集 #evaluate the training set eval_mace(configs=\\u0026quot;data/solvent_xtb_train_200.xyz\\u0026quot;, model=\\u0026quot;MACE_models/mace01_run-123_stagetwo.model\\u0026quot;, output=\\u0026quot;tests/mace01/solvent_train.xyz\\u0026quot;) #evaluate the test set eval_mace(configs=\\u0026quot;data/solvent_xtb_test.xyz\\u0026quot;, model=\\u0026quot;MACE_models/mace01_run-123_stagetwo.model\\u0026quot;, output=\\u0026quot;tests/mace01/solvent_test.xyz\\u0026quot;) 这样，MACE就会对数据集进行评估，因为其实DFT数据是已有的，主要是在输出文件中，补上MACE的预测值。\\n画图 然后就可以用aseMolec的如下代码进行画图了\\nfrom aseMolec import pltProps as pp from ase.io import read import matplotlib.pyplot as plt from aseMolec import extAtoms as ea import numpy as np def plot_RMSEs(db, labs): ea.rename_prop_tag(db, 'MACE_energy', 'energy_mace') #Backward compatibility ea.rename_prop_tag(db, 'MACE_forces', 'forces_mace') #Backward compatibility plt.figure(figsize=(9,6), dpi=100) plt.subplot(1,3,1) pp.plot_prop(ea.get_prop(db, 'bind', '_xtb', True).flatten(), \\\\ ea.get_prop(db, 'bind', '_mace', True).flatten(), \\\\ title=r'Energy $(\\\\rm eV/atom)$ ', labs=labs, rel=False) plt.subplot(1,3,2) pp.plot_prop(ea.get_prop(db, 'info', 'energy_xtb', True).flatten(), \\\\ ea.get_prop(db, 'info', 'energy_mace', True).flatten(), \\\\ title=r'Energy $(\\\\rm eV/atom)$ ', labs=labs, rel=False) plt.subplot(1,3,3) pp.plot_prop(np.concatenate(ea.get_prop(db, 'arrays', 'forces_xtb')).flatten(), \\\\ np.concatenate(ea.get_prop(db, 'arrays', 'forces_mace')).flatten(), \\\\ title=r'Forces $\\\\rm (eV/\\\\AA)$ ', labs=labs, rel=False) plt.tight_layout() return train_data = read('tests/mace01/solvent_train.xyz', ':') test_data = train_data[:3]+read('tests/mace01/solvent_test.xyz', ':') #append the E0s for computing atomization energy errors plot_RMSEs(train_data, labs=['XTB', 'MACE']) plot_RMSEs(test_data, labs=['XTB', 'MACE']) plot_RMSEs函数中首先把标签重命名了一下，因为MACE的源代码mace_eval_configs_main()部分默认打的标签是MACE_+什么什么的。\\n涉及到的比较高级的Python语法 画DFT forces VS MACE forces的代码，有一段如下\\ndef get_prop(db, type, prop='', peratom=False, E0={}): if peratom: N = lambda a : a.get_global_number_of_atoms() else: N = lambda a : 1 if type == 'info': return np.array(list(map(lambda a : a.info[prop]/N(a), db))) if type == 'arrays': return np.array(list(map(lambda a : a.arrays[prop]/N(a), db)), dtype=object) if type == 'cell': return np.array(list(map(lambda a : a.cell/N(a), db))) if type == 'meth': return np.array(list(map(lambda a : getattr(a, prop)()/N(a), db))) if type == 'atom': if not E0: E0 = get_E0(db, prop) return np.array(list(map(lambda a : (np.sum([E0[s] for s in a.get_chemical_symbols()]))/N(a), db))) if type == 'bind': if not E0: E0 = get_E0(db, prop) return np.array(list(map(lambda a : (a.info['energy'+prop]-np.sum([E0[s] for s in a.get_chemical_symbols()]))/N(a), db))) 这里，forces在ASE的atoms类中属于arrays型信息。\\n他是把db，也就是数据集（list[atoms]），中的每一个构型(atoms)，传递给了匿名函数lambda，这个匿名函数会提取相应标签的arrays型信息。\\n显示指定dtype=object，避免了不同构型中原子数不同带来的生成数组时的报错。\\n之后再沿axis=0方向拼接，然后展开，就变成了一维数据，方便计算处理。\\n"`,categories:'["机器学习势"]',tags:[],series:'["ASE","MACE"]',date:'"2025-05-29"'}),searchIndex.push({title:'"图池化"',permalink:'"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E6%B1%A0%E5%8C%96/"',content:'""',categories:'["深度学习"]',tags:[],series:'["图神经网络"]',date:'"2025-05-28"'}),searchIndex.push({title:'"晶界建模"',permalink:'"/pymatgen/gainbroundary/"',content:`"晶界建模 前言 基于 共格点阵模型 （Coincidence Site Lattice），使用pymatgen进行晶界建模。\\n晶界的类型 1. 扭转（twist）晶界 旋转轴垂直于晶界面，也就是两者的密勒指数应该成比列。\\n2. 倾斜（tilt）晶界 旋转轴平行于晶界面，也就是两者的密勒指数的点乘为0。\\n3. 混合（mixed）晶界 旋转轴既不垂直也不平行于晶界面。\\n基于共格点阵模型的晶界命名法，Σ+number(must be odd)+(hkl)/[uvw]。\\n举一个栗子， Σ13 (510)/[001] symmetric tilt grain-boundary。\\n这里Σ的大小，是指旋转后重合点阵的单胞的提及是原始晶体单胞的体积的多少倍。通常，这个数字越大，代表两个晶粒的取向相差越远，晶界能也往往越远。\\nΣ1则代表趋向一致，那些小角度晶界也被认为Σ的值近似于1。\\n当指明是twist or tilt晶界时，有时可以省略晶向，也不会造成歧义，比如，the Σ5(310) tilt GB，这是一个YSZ中典型的低能量晶界。\\n但它没有给出旋转轴，因为没有必要，tilt GB 要求晶界面与旋转轴平行，所以只能是[001]。\\n使用pymatgen进行晶界建模 首先，我们假设一个应用场景，就是说，我们建模肯定是根据实验来的，实验上对哪些晶界感兴趣，我们就去建模研究。\\n所以在这个假设的基础上，我们是知道Σ的值以及旋转轴、晶界面的。\\n这样，用以下代码我们可以得到旋转角。\\nfrom pymatgen.core import Structure from pymatgen.core.interface import GrainBoundaryGenerator # 1. 读取结构文件 structure = Structure.from_file(\\u0026quot;ZrO.cif\\u0026quot;) structure = structure.to_conventional() #创建一个晶界生成器，实例化需要一个晶体结构，最好是conventional cell。 gb_gen = GrainBoundaryGenerator(structure) # 2. 构建 Σ5 晶界，参数分别对应Σ的值、旋转轴、晶格类型，对非立方体系需要指定轴比 #其实这里感觉很奇怪，轴比和晶格类型，pymatgen不应该自己判断吗，感觉这块代码写的不好 rotation_anglen = gb_gen.get_rotation_angle_from_sigma(5,(0,0,1),lat_type='c') print(rotation_anglen) 这里的输出是，[36.86989764584402, 53.13010235415597, 126.86989764584402, 143.13010235415598]。\\n第一个值，36.8°与[参考文献]( Sci-Hub | Structure and Chemistry of Yttria-Stabilized Cubic-Zirconia Symmetric Tilt Grain Boundaries. Journal of the American Ceramic Society, 84(6), 1361–1368 | 10.1111/j.1151-2916.2001.tb00842.x )一致。\\n同样的，如果我们用rotation_anglen = gb_gen.get_rotation_angle_from_sigma(13,(0,0,1),lat_type='c')\\n则会得到输出，[22.61986494804043, 67.38013505195957, 112.61986494804043, 157.3801350519596]，第一个值与24°差1.4。\\n#指定旋转轴，旋转角，晶界面 gb = gb_gen.gb_from_parameters( rotation_axis=(0,0,1), rotation_angle=rotation_anglen[0], expand_times=1, vacuum_thickness=0, plane=(3,1,0) ) # 3. 获取晶界结构 gb.to(\\u0026quot;POSCAR\\u0026quot;, \\u0026quot;poscar\\u0026quot;) 再写上这块代码就可以生成晶界了。\\n"`,categories:'["pymatgen"]',tags:[],series:[],date:'"2025-05-24"'}),searchIndex.push({title:'"Pytorch中的张量操作"',permalink:'"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C/"',content:`"Pytorch中的张量操作 数学符号 1. * 运算符 如果张量的形状完全一样，张量 * 张量，其实就是对应元素相乘，也可以$\\\\odot$表示。\\n如果张量的形状不一样，则要求他们的形状只能在一个维度上不一样，且其中一个张量这个维度的size为1，这样可以用广播机制补全后相乘。\\n如果张量 * 数字，则是全体元素乘以这个数字。\\n函数、方法 1. permute() 可以作为张量对象的方法使用，接受置换后的维度。\\nimport torch table = torch.randn(2,3,4) print(table) print(table.permute(2,1,0)) 这段代码就是将第一维和第三维进行置换。\\n由已知的张量还有置换后的顺序，怎么写出新的张量呢，这个问题一直让我困扰。\\n今天的学习，我脑子里突然蹦出一个邻居的概念。对于一个四维的张量元素，它应该有四个不同维度的邻居，但如果仅仅把视角局限在矩阵里，那它就只有两个邻居，一个行维度上的，一个列维度上的，这很不对。得把视野打开，通道维度和批次维度的邻居可能隔了很远，但它们也是邻居。\\n2. reshape()和view() 深度学习中，经常会把高维张量降维，计算后再展开，要小心翼翼，错位会带来灾难性的错误。\\n对于一个（N，F，M）的张量，F是其特征维度。如果想把N和M合并，应该先把F变成最高维度或者最低维度，让需要合并的维度接壤。\\nimport torch table = torch.randn(2,3,4) print(table) a = table.reshape(3,8) b = table.permute(1,0,2).reshape(3,8) print(a) print(b) 这里先把张量形状变成了（3，2，4），再合并，可以保证张量维度不会错位，如果直接改写性质为（3，8），则会错位。\\nreshape()和view()是一样的，前者可以作用于numpy数组。\\n3. flatten() 将高维张量展平为一维张量。\\n4. torch.cat() 把张量按给定的顺序和维度进行拼接，除了拼接维度外，张量必须具有相同的形状。\\n可用于特征维度的拓展，加入新的特征。\\n5. unsqueeze() torch.unsqueeze torch.unsqueeze(input, dim) → Tensor\\n为张量插入一个新的size为1的维度。用于升维。配合expand()食用更佳。\\n6. expand() torch.Tensor.expand 这个只有张量方法，没有函数。\\nTensor.expand(*sizes) → Tensor\\n将张量沿着某个size为1的维度进行复制拓展。\\ntotal_nbr_fea = torch.cat( [atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len), atom_nbr_fea, nbr_fea], dim=2) atom_in_fea张量的形状本是(N,self.atom_fea_len)，通过unsqueeze(1)，变成(N,1,self.atom_fea_len)。\\n再通过expand(N, M, self.atom_fea_len),变成(N, M, self.atom_fea_len)形状。\\n7. chunk() torch.chunk torch.chunk(input: Tensor , chunks: int , dim: int = 0) → Tuple[Tensor, \\u0026hellip;]\\n尝试把张量分成指定数量的块。也就是分割张量。可指定维度分割。但，数量可能会比指定的少，一般最好成倍数时使用。\\n可用于分割特征维度。\\n8. torch.sum() torch.sum torch.sum(input, dim, keepdim=False, ***, dtype=None) → Tensor\\n将张量元素沿指定维度进行加和。\\n切片 1. 简单切片 import torch a = torch.tensor(range(12)).view(3,4) b = a[1,2] print(a) print(b) 在张量切片中，逗号表示维度间隔。在这个简单切片中，整数代表索引。\\n也可以接受start:stop:step格式的索引进行切片，一个简单:代表全选这一维度。\\nimport torch a = torch.tensor(range(24)).view(2,3,4) b = a[1,:] print(a) print(b) 这里是一个三维张量，但是只用了一个逗号，这是一种简写，其实是逗号后续的所有维度都全选。\\n2. 高级切片 张量的某个维度的索引也可以是张量。\\n如果索引是一维张量，据测试，其实是列表也行。就是多个整数索引组成的列表或者一维张量。根据这些整数确定这个维度怎么被取。\\nimport torch import numpy as np atom_in_fea = torch.randn(4,5,6) nbr_fea_idx = torch.tensor([0, 1, 4]) atom_nbr_fea1 = atom_in_fea[:,nbr_fea_idx,:] atom_nbr_fea2 = atom_in_fea[:,[0,1,4],:] print(atom_nbr_fea1==atom_nbr_fea2) 这里dim=1维度的切片索引就是一个一维张量，里面的整数是0，1，4。说明切片时，取这个维度的第1，第2，第5个数据。\\n如果索引是二维张量，在切片的同时其实进行了升维。\\nimport torch import numpy as np atom_in_fea = torch.randn(4,5,6) #shape(2,2) nbr_fea_idx = torch.tensor([[0, 1],[1,2]]) atom_nbr_fea = atom_in_fea[:,nbr_fea_idx,:] print(atom_nbr_fea.shape) #torch.Size([4, 2, 2, 6]) 这里dim=1维度的切片索引就是一个二维张量，它会按照内部每一行的整数对这个维度进行切片，并根据内部的行数形成新的一维。\\n听起来很复杂，这里给出一个具体的应用场景，在CGCNN的卷积操作中。\\natom_in_fea的性质为（N，features），N是批次内的总原子数\\nnbr_fea_idx是一个二维张量（N，M），记录了N个原子的邻居原子的原子id。\\natom_nbr_fea = atom_in_fea[nbr_fea_idx, :] 从形状上来看，这样的切片结果就是把（N，features）中的N替换成（N，M），最终变成（N，M，features）\\n这样，就得到了，每个原子的M个邻居原子的原子特征。\\n这里nbr_fea_idx的记录很关键，CIFData给出的是每个晶体内的，collate_pool函数会把它更新成batch内的。模型接受的参数是经collate_pool函数集成后的。\\n"`,categories:'["深度学习"]',tags:[],series:[],date:'"2025-05-20"'}),searchIndex.push({title:'"用于搭建神经网络的函数"',permalink:'"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch2/"',content:`"用于搭建神经网络的函数 前言 此处记录常见的神经网络函数，排名不分先后。\\n全连接层 1. nn.Linear() 对输入数据施加一个仿射线性变换，一般使用指定两个参数，是输入矩阵和输出矩阵的最高维的size。（因为最高维一般是特征维度，这个函数就是用来控制特征维度size大小的）\\n这里是CGCNN的图卷积操作公式，这里的W（权重），b（偏置）其实全连接层决定的。看到这样的写法就要明白其实是经历了一个全连接层。\\ntotal_gated_fea = self.fc_full(total_nbr_fea) total_gated_fea = self.bn1(total_gated_fea.view( -1, self.atom_fea_len*2)).view(N, M, self.atom_fea_len*2) nbr_filter, nbr_core = total_gated_fea.chunk(2, dim=2) nbr_filter = self.sigmoid(nbr_filter) nbr_core = self.softplus1(nbr_core) 对应代码就是进过全连接层后，（再进行批标准化，chunk分割，这些公式里没有体现），然后施加$\\\\sigma$函数和激活函数（g）。\\n激活函数 1. nn.ReLU() 这个激活函数不用指定输入输出特征的维度，它只是把所有特征变为非负，对于正值保留原始值，对于负值则转化为0。\\n2. nn.Softmax() 对一个n维的张量施加Softmax()函数，使得其沿某个维度的元素值的和为1。所以接受一个dim参数来指定维度。 $$ \\\\mathrm{Softmax}(x_i)=\\\\frac{\\\\mathrm{exp}(x_i)}{\\\\sum_{j}\\\\mathrm{exp}(x_j)} $$ 这里简单插入一下Pytorch中有关dim的实践。\\n比如说一个张量的size是(4,2,3)，那么他的dim=0指的是4，dim=1指的是2，dim=2指的是3。\\n比如说对于这个张量，我有个和Pytorch相反的习惯，我习惯先看每行有多少个元素，是3。我就误以为它的dim=0对应的是3。\\n其实不然，深度学习中，dim最大值对应维度的size，往往对应样本的特征数。\\n一个简单的二维的深度学习的输入张量的size一般是这样的：(batch_size,features)。\\ntensor([[[0.3299, 0.4336, 0.2365], [0.0695, 0.0668, 0.8638]], [[0.8114, 0.1116, 0.0770], [0.3142, 0.1086, 0.5772]], [[0.3178, 0.4508, 0.2315], [0.1620, 0.2610, 0.5770]], [[0.4454, 0.4082, 0.1464], [0.2974, 0.5297, 0.1729]]]) 3. nn.LogSoftmax() 对一个n维的张量施加log(Softmax())函数，通常用于获取对数概率，并与损失函数nn.NLLLoss()一起使用\\n4. nn.Softmax2d() 针对3维(C,H,W)或4维(N,C,H,W)的输入，它总是沿着C对应的维度对张量施加Softmax()函数\\n5. nn.Softplus() 对每个张量元素施加 Softplus 函数。这是一个ReLU函数的光滑近似，用于保证正的输出。\\n默认$\\\\beta$值为1，当输入值*$\\\\beta$大于一定门槛（默认值20）时，会变成线性。\\n$\\\\mathrm{Softplus}(x)=\\\\frac{1}{\\\\beta}*\\\\log(1+\\\\exp(\\\\beta * x))$\\n6. nn.Sigmoid() 对张量的每个元素施加Sigmoid函数。对于小于0的输入，输出0-0.5；对于大于0的输入，输出0.5-1。整体是非负的。\\n$\\\\mathrm{Sigmoid}(x)=\\\\sigma(x)=\\\\frac{1}{1+\\\\exp(-x)}$\\n7. nn.Tanh 对张量的每个元素施加双曲正切函数，输出值范围为(-1,1)。\\n$\\\\mathrm{Tanh}(x)=\\\\mathrm{tanh}(x)=\\\\frac{\\\\exp(x)-\\\\exp(-x)}{\\\\exp(x)+\\\\exp(-x)}$\\n批标准化 1. nn.BatchNorm1d() 对二维(N,C)或三维(N,C,L)的输入进行批标准化。\\n对于二维输入，其实N是batch大小，C是特征数，这里叫Channels，是沿着列求平均和方差的。并且这个函数求的是有偏方差。\\n对于三维输入，可以理解成先转化为(N*L,C)，再用二维的处理方法得到结果，再展开。\\n正则化 参考文献 在深度学习中，正则化通常用于约束模型的复杂度、防止过拟合、提高模型的泛化能力和鲁棒性。\\n1. nn.Dropout() 在每次前向传播时，以一定概率让张量中的某些元素归零。防止过拟合。常用于分类任务。不要用于回归任务。\\n接受一个浮点数，用于表示概率，默认值是0.5。\\n2. L1正则化 L1正则化、也称Lasso正则化。就是在损失函数中引入一个与模型权重的L1范数相关的值，作为惩罚项，用于控制（限制）模型的复杂度和防止过拟合。\\n$L_{L1}=L_{data}+\\\\lambda\\\\begin{matrix} \\\\sum_{i=1}^n |w_i| \\\\end{matrix}$\\n3. L2正则化 L2正则化，也称Ridge正则化、权重衰减。与L1正则化类似，就是在损失函数中引入一个与模型权重的L2范数相关的值，作为惩罚项，用于控制（限制）模型的复杂度和防止过拟合。它鼓励模型用小的模型参数。\\n$L_{L1}=L_{data}+\\\\lambda||w||_2^2$\\n这里， $$ ||w|| _2 ^2 = \\\\sum_{i=1}^n w_i^2 $$4. Elastic Net 正则化 采用L1正则化和L2正则化的组合。\\n5. 早停止和数据增强 早停止：即检测模型在验证集上的性能，当模型在验证集的性能出现下降时停止训练，避免过拟合。\\n数据增强：对数据进行一定变换来增强数据的多样性。如在图像分类时，对图片进行旋转、剪裁、翻转等操作。\\n"`,categories:'["深度学习"]',tags:[],series:[],date:'"2025-05-19"'}),searchIndex.push({title:'"Python中的文件操作"',permalink:'"/%E4%BB%A3%E7%A0%81/fileoperations/"',content:`"Python中的文件操作 获取当前路径 os.getcwd()可以返回当前工作目录的绝对路径。cwd可以理解成current work directory。\\n创建多个用于VASP计算的文件夹 任务：假设你要对多个相似的结构进行计算，即他们所需的INCAR、KPOINTS、POTCAR、vasp.pbs（提交任务的脚本）是一样的。\\nINCAR、KPOINTS、POTCAR、vasp.pbs在当前文件夹中\\n我们要在当前文件夹中创建一个名为'distorted'的文件夹，并在其中创建若干个计算文件夹。\\n代码如下：\\nimport os import shutil #先记录下四个计算文件的路径 cwd = os.getcwd() INCARPath = os.path.join(cwd,'INCAR') KPOINTSPath = os.path.join(cwd,'KPOINTS') POTCARPath = os.path.join(cwd,'POTCAR') PBSPath = os.path.join(cwd,'vasp.pbs') #创建一个总路径，用于存放计算文件夹 os.makedirs('distorted') path = os.path.join(cwd,'distorted') #通过循环，在distorted路径下创建若干个计算文件夹，并把计算文件复制过去 for i in range(10): CalPath = os.path.join(path,'{:03d}'.format(i)) os.makedirs(CalPath) shutil.copy(INCARPath,CalPath) shutil.copy(KPOINTSPath,CalPath) shutil.copy(POTCARPath,CalPath) shutil.copy(PBSPath,CalPath) 提交不同深度的VASP任务 实际的任务路径下，POSCAR等输入文件可能位于不同的深度。\\n比如，有的在./1/POSCAR，有的则在./1/11/111/POSCAR。\\n这样简单的循环可能难以完成任务提交，需要一个比较高级的循环方式。\\nsh脚本\\n#!/bin/bash # find POSCAR find . -type f -name \\u0026quot;POSCAR\\u0026quot; | while read -r line; do # get dir dir=$(dirname \\u0026quot;$line\\u0026quot;) # cd dir cd \\u0026quot;$dir\\u0026quot; || continue # just an example qsub vasp.pbs #cd last dir cd - \\u0026gt;/dev/null || exit done find 找到当前路径以及所有子路径下名为POSCAR的文件，并依次输出这些文件的相对路径\\n这其实是一个隐藏的循环，传递给while。\\n然后用dirname获取文件的父路径，进入，投任务。\\n这是已经提交了前10个任务，提交第11-20个任务。\\n#!/bin/bash count=0 find . -type f -name \\u0026quot;POSCAR\\u0026quot; | while read -r line; do ((count++)) if ((count \\u0026lt;= 10)); then continue fi if ((count \\u0026gt; 20)); then break fi dir=$(dirname \\u0026quot;$line\\u0026quot;) cd \\u0026quot;$dir\\u0026quot; || continue qsub vasp.pbs sleep 2 cd - \\u0026gt;/dev/null || exit done os.walk(\\u0026rsquo;.') import os from stretch import stretch_strycture prims = [] for root,dirs,files in os.walk('.'): if 'POSCAR' in files: prims.append(os.path.join(root,'POSCAR')) if 'CONTCAR' in files: prims.append(os.path.join(root,'CONTCAR')) for prim in prims: stretch_strycture(prim) os.walk(\\u0026rsquo;.\\u0026rsquo;)可以很方便地遍历一遍当前路径以及所有子路径的文件。\\n在这个循环中，\\nroot 为某次循环中进入到的路径，\\ndirs 为某次循环中进入到的路径下的所有文件夹名称列表，这里是单纯的名称而不是路径，\\nfiles 为某次循环中进入到的路径下的所有文件名称列表，这里是单纯的名称而不是路径。\\n所以判断 if 'POSCAR' in files: ，如果为真，这用 os.path.join() 将 root 和 POSCAR连接起来获得这个POSCAR的路径。\\n此外，\\nos.path.basename(str),只保留最后一级的路径的名称\\nos.path.dirname(prim),只保留最后一级前的路径\\n"`,categories:'["代码"]',tags:[],series:[],date:'"2025-05-18"'}),searchIndex.push({title:'"花朵分类"',permalink:'"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1/"',content:`"花朵分类 前言 本文主要借助torchvision软件包，简单梳理一下深度学习代码的基本框架。\\n数据集加载 #假设当前工作路径下，存放着一个名为'flower_data'的文件夹，里面存放着训练集和验证集的图片 #用os.path.join()来一级级得获取路径 data_dir = os.path.join(os.getcwd(),'flower_data') train_dir = os.path.join(data_dir, 'train') valid_dir = os.path.join(data_dir, 'valid') # Define batch size batch_size = 32 # Define transforms for the training and validation sets normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #定义对数据预处理的组合，比如旋转图片、改变尺寸等等，让模型有更强的稳定性 train_data_transforms = transforms.Compose([ transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)), transforms.RandomRotation(degrees=15), transforms.ColorJitter(), transforms.RandomHorizontalFlip(), transforms.CenterCrop(size=224), transforms.ToTensor(), normalize, ]) validate_data_transforms = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize, ]) #这里才真正的把图片加载成了二维数据。 train_dataset = datasets.ImageFolder( train_dir, train_data_transforms) validate_dataset = datasets.ImageFolder( valid_dir, validate_data_transforms) #做了那么多铺垫，其实就是为了把可用于训练的数据(二维数据)放到 DataLoader 里面 train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=batch_size, shuffle=True, num_workers=4) validate_loader = torch.utils.data.DataLoader( validate_dataset, batch_size=batch_size, shuffle=True, num_workers=4) data_loader = {} data_loader['train'] = train_loader data_loader['valid'] = validate_loader batch_size，当训练数据很多时，一次性加载全部数据进行训练会是一种挑战。这时就需要用到批次训练。batch_size即是一次训练中使用的数据量。注意这里的一次训练，不是指一epoch。只有遍历所有训练集后，才能叫做完成了一代训练。一代训练包含了诸多这样的一次训练。\\nshuffle参数为True时，随机采样。\\n这是CGCNN（晶体卷积图神经网络）的代码中，用于得到训练集、验证集、测试集的DateLoader。\\ndef get_train_val_test_loader(dataset, collate_fn=default_collate, batch_size=64, train_ratio=None, val_ratio=0.1, test_ratio=0.1, return_test=False, num_workers=1, pin_memory=False, **kwargs): total_size = len(dataset) if kwargs['train_size'] is None: if train_ratio is None: assert val_ratio + test_ratio \\u0026lt; 1 train_ratio = 1 - val_ratio - test_ratio print(f'[Warning] train_ratio is None, using 1 - val_ratio - ' f'test_ratio = {train_ratio} as training data.') else: assert train_ratio + val_ratio + test_ratio \\u0026lt;= 1 indices = list(range(total_size)) if kwargs['train_size']: train_size = kwargs['train_size'] else: train_size = int(train_ratio * total_size) if kwargs['test_size']: test_size = kwargs['test_size'] else: test_size = int(test_ratio * total_size) if kwargs['val_size']: valid_size = kwargs['val_size'] else: valid_size = int(val_ratio * total_size) train_sampler = SubsetRandomSampler(indices[:train_size]) val_sampler = SubsetRandomSampler( indices[-(valid_size + test_size):-test_size]) if return_test: test_sampler = SubsetRandomSampler(indices[-test_size:]) train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory) val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory) if return_test: test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory) if return_test: return train_loader, val_loader, test_loader else: return train_loader, val_loader 这段代码整体分为两部分，其一是根据参数确定训练集、验证集、测试集的数量，其二是装配DataLoader。和花朵分类的代码稍有不同，这样的实现可以让用户自由决定训练集、验证集和测试集。多使用了DataLoader的sampler参数。\\nSubsetRandomSampler是torch.utils.data.sampler模块中的六个类之一。用于随机采样。\\n这里是常规用法，假设有100个数据，80个用于训练集，15个用于验证集，5个用于测试集。\\n那么，\\n训练集的SubsetRandomSampler的初始化参数就是list(range(100))[0:80]。\\n验证集的SubsetRandomSampler的初始化参数就是list(range(100))[-20:-5]。\\n测试集的SubsetRandomSampler的初始化参数就是list(range(100))[-5:]。\\n1. Dataset与DataLoader 在CGCNN中，作者自己手写了CIFData作为自定义的Dataset。\\nCIFData三个重要的魔法方法的功能：\\n1.__init__()方法\\ndef __init__(self, root_dir, max_num_nbr=12, radius=8, dmin=0, step=0.2, random_seed=123): 这里简单初始化了：数据集的路径、晶体的解析细节（比如最大邻居数和截断半径，高斯距离的参数）\\n2.__len__()方法\\n让CIFData的实例可以通过len()函数返回数据集大小。\\n3.__getitem__()方法\\n这里先用了@functools.lru_cache(maxsize=None)修饰器来缓存数据，避免每次读入相同结构时，都要解析晶体。\\nDataLoader中的collate_fn打包函数\\nCGCNN代码中自定义了打包函数，名叫collate_pool()\\nDataLoader实例在初始化同样只是规定了一些加载参数，并没有实际加载数据集。\\n只有当遍历DataLoader实例时，才开始加载、打包、返回数据。(for循环抽打DataLoader，DataLoader抽打Dataset)\\n但是遍历DataLoader并不会得到一个个的数据点，而是得到一个个batch的数据包，这是由collate_fn打包函数完成的，每当经历了batch_size个数据点，打包函数就会把他们合并。\\n这里是打包函数返回的内容\\nreturn (torch.cat(batch_atom_fea, dim=0), torch.cat(batch_nbr_fea, dim=0), torch.cat(batch_nbr_fea_idx, dim=0), crystal_atom_idx),\\\\ torch.stack(batch_target, dim=0),\\\\ batch_cif_ids 以及for循环的例子， for i, (input, target, _) in enumerate(train_loader):\\n得到的input其实是第一个返回，即一个元组，包含了经过拼接后的同一批次内的原子特征、邻居特征、以及邻居特征索引和全局原子索引。\\n写神经网络 model_input = 'resnet152' # Build and train network if model_input == 'vgg16': # Build and train network model = models.vgg16(pretrained=True) elif model_input == 'vgg19': # Build and train network model = models.vgg19(pretrained=True) elif model_input == 'resnet152': model = models.resnet152(pretrained=True) # Freeze training for all layers for param in model.parameters(): param.requires_grad = False # # Newly created modules have require_grad=True by default if 'vgg' in model_input: num_features = model.classifier[-1].in_features model.classifier[6] = nn.Sequential( nn.Linear(num_features, 512), nn.ReLU(), nn.Linear(512, len(cat_to_name)), nn.LogSoftmax(dim=1)) elif 'resnet' in model_input: num_features = model.fc.in_features model.fc = nn.Sequential( nn.Linear(num_features, 512), nn.ReLU(), nn.BatchNorm1d(512), nn.Dropout(0.4), nn.Linear(512, len(cat_to_name)), nn.LogSoftmax(dim=1)) print(model.__class__.__name__) # check to see that your last layer produces the expected number of outputs # print(model.classifier[-1].out_features) 根据参数选择预训练模型并冻结模型参数，然后修改输出层。\\n对于vgg模型，\\nself.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(p=dropout), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(p=dropout), nn.Linear(4096, num_classes), ) 他的分类器本来包含了7层，但是作者觉得最后一层的4096个特征太多了，就改写了一下，减少特征为512个。\\ntrain()函数 让我们看看一个合格的训练函数都应该做哪些工作。\\ndef train(train_loader, model, criterion, optimizer, epoch, normalizer): #每一代epoch都调用一次train函数 batch_time = AverageMeter() data_time = AverageMeter() losses = AverageMeter() if args.task == 'regression': mae_errors = AverageMeter() else: accuracies = AverageMeter() precisions = AverageMeter() recalls = AverageMeter() fscores = AverageMeter() auc_scores = AverageMeter() # switch to train mode model.train() end = time.time() #一个训练函数应该要遍历整个训练集 for i, (input, target, _) in enumerate(train_loader): #遍历train_loader，其实是一个个的批次 # measure data loading time data_time.update(time.time() - end) if args.cuda: input_var = (Variable(input[0].cuda(non_blocking=True)), Variable(input[1].cuda(non_blocking=True)), input[2].cuda(non_blocking=True), [crys_idx.cuda(non_blocking=True) for crys_idx in input[3]]) else: input_var = (Variable(input[0]), Variable(input[1]), input[2], input[3]) # normalize target if args.task == 'regression': target_normed = normalizer.norm(target) else: target_normed = target.view(-1).long() if args.cuda: target_var = Variable(target_normed.cuda(non_blocking=True)) else: target_var = Variable(target_normed) # compute output #把这一批次的数据带入模型 output = model(*input_var) #由模型得到的结果和目标值得到损失函数 loss = criterion(output, target_var) # measure accuracy and record loss if args.task == 'regression': mae_error = mae(normalizer.denorm(output.data.cpu()), target) losses.update(loss.data.cpu(), target.size(0)) mae_errors.update(mae_error, target.size(0)) else: accuracy, precision, recall, fscore, auc_score = \\\\ class_eval(output.data.cpu(), target) losses.update(loss.data.cpu().item(), target.size(0)) accuracies.update(accuracy, target.size(0)) precisions.update(precision, target.size(0)) recalls.update(recall, target.size(0)) fscores.update(fscore, target.size(0)) auc_scores.update(auc_score, target.size(0)) # compute gradient and do SGD step optimizer.zero_grad() loss.backward() #后向传播、计算梯度 optimizer.step() #传递梯度、调整参数 # measure elapsed time batch_time.update(time.time() - end) end = time.time() if i % args.print_freq == 0: if args.task == 'regression': print('Epoch: [{0}][{1}/{2}]\\\\t' 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\\\t' 'Data {data_time.val:.3f} ({data_time.avg:.3f})\\\\t' 'Loss {loss.val:.4f} ({loss.avg:.4f})\\\\t' 'MAE {mae_errors.val:.3f} ({mae_errors.avg:.3f})'.format( epoch, i, len(train_loader), batch_time=batch_time, data_time=data_time, loss=losses, mae_errors=mae_errors) ) else: print('Epoch: [{0}][{1}/{2}]\\\\t' 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\\\t' 'Data {data_time.val:.3f} ({data_time.avg:.3f})\\\\t' 'Loss {loss.val:.4f} ({loss.avg:.4f})\\\\t' 'Accu {accu.val:.3f} ({accu.avg:.3f})\\\\t' 'Precision {prec.val:.3f} ({prec.avg:.3f})\\\\t' 'Recall {recall.val:.3f} ({recall.avg:.3f})\\\\t' 'F1 {f1.val:.3f} ({f1.avg:.3f})\\\\t' 'AUC {auc.val:.3f} ({auc.avg:.3f})'.format( epoch, i, len(train_loader), batch_time=batch_time, data_time=data_time, loss=losses, accu=accuracies, prec=precisions, recall=recalls, f1=fscores, auc=auc_scores) ) def train(train_loader, model, criterion, optimizer, epoch, normalizer): 这里接受的参数有：训练集的DataLoader、模型、损失函数、优化器、epoch、正则化。\\nmodel.train()\\n首先，把模型的训练模式打开，这样网格中一些特殊的函数比如Dropout才会被开启。\\nfor循环遍历训练集的DataLoader\\n把得到的批数据，经过简单的处理，比如把张量上传到CUDA。\\n把批数据流入模型，再后向传播。\\n"`,categories:'["深度学习"]',tags:[],series:[],date:'"2025-05-18"'}),searchIndex.push({title:'"使用rNEMD方法计算热导率的lammps输入文件"',permalink:'"/moleculardynamics/thermal_conductivity_rnemd/"',content:`"使用rNEMD方法计算热导率的LAMMPS输入文件 前言 rNEMD方法 ，又叫MP方法，计算材料热导率。\\nLAMMPS官方提供了计算脚本，但是使用的单位却是lj单位制，非常不实用，这里是我自己写的metal单位制下的脚本。\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\nin file # sample LAMMPS input script for thermal conductivity # Muller-Plathe method via fix thermal_conductivity # settings temperature, kB and timestep variable t equal 300 variable k equal 8.6173e-5 variable dt equal 0.0005 # convert from LAMMPS metal units to SI variable eV2J equal 1.6022e-19 #energy convert variable A2m equal 1.0e-10 #distance convert variable ps2s equal 1.0e-12 #time convert variable convert equal \${eV2J}/\${ps2s}/\${A2m} # setup problem units metal atom_style atomic atom_modify map yes newton on read_data 222010 pair_style mliap unified /home-ssd/Users/nsgm_zcx/macetrain17/YSZH_MACE_model.model-mliap_lammps.pt 0 pair_coeff * * H O Y Zr neighbor 2.0 bin neigh_modify every 1 delay 0 check yes #fix R all box/relax aniso 0.0 vmax 0.001 minimize 0 1e-8 1000 100000 timestep 0.0005 velocity all create 10 12345 dist gaussian mom yes rot yes # npt increase temp reset_timestep 0 fix remove_com all momentum 500 linear 1 1 1 fix 1 all npt temp 10 $t 0.1 x 0.0 0.0 1.0 y 0.0 0.0 1.0 z 0.0 0.0 1.0 couple xy thermo_style custom step temp pe etotal enthalpy lx ly lz vol press thermo 100 dump 1 all custom 100 nvttrj id element x y z fx fy fz dump_modify 1 sort id element H O Y Zr run 20000 undump 1 velocity all scale $t unfix 1 # npt keep temp reset_timestep 0 fix 1 all npt temp $t $t 0.1 x 0.0 0.0 1.0 y 0.0 0.0 1.0 z 0.0 0.0 1.0 couple xy thermo_style custom step temp pe etotal enthalpy lx ly lz vol press thermo 100 dump\\t1 all custom 100 yszhtrj id element x y z fx fy fz dump_modify\\t1 sort id element H O Y Zr run 10000 undump\\t1 velocity all scale $t unfix 1 #nvt keep temp reset_timestep 0 fix 1 all nvt temp $t $t 0.1 thermo_style custom step temp pe etotal enthalpy lx ly lz vol press thermo 100 dump\\t1 all custom 100 yszhtrj id element x y z fx fy fz dump_modify\\t1 sort id element H O Y Zr run 10000 undump\\t1 velocity all scale $t unfix 1 # 2nd equilibration run compute ke all ke/atom variable temp atom c_ke/1.5/\${k} fix 1 all nve compute layers all chunk/atom bin/1d z lower 0.05 units reduced fix 2 all ave/chunk 10 100 1000 layers v_temp file profile.mp fix 3 all thermal/conductivity 20 z 20 variable tdiff equal f_2[11][3]-f_2[1][3] thermo_style custom step temp epair etotal f_3 v_tdiff thermo_modify colname f_3 E_delta colname v_tdiff dTemp_step thermo 1000 run 80000 # thermal conductivity calculation # reset fix thermal/conductivity to zero energy accumulation fix 3 all thermal/conductivity 20 z 20 variable start_time equal time variable kappa equal (f_3/(time-\${start_time})/(lx*ly)/2.0)*(lz/2.0)/f_ave fix ave all ave/time 1 1 1000 v_tdiff ave running thermo_style custom step temp epair etotal f_3 v_tdiff f_ave thermo_modify colname f_3 E_delta colname v_tdiff dTemp_step colname f_ave dTemp run 20000 print \\u0026quot;Running average thermal conductivity units metal: $(v_kappa:%.2f)\\u0026quot; variable tc equal \${kappa}*\${convert} print \\u0026quot;Running average thermal conductivity units SI: $(v_tc:%.2f)\\u0026quot; 细节 fix 3 all thermal/conductivity 20 z 20 这个fix可以实现动量交换，这里Nstep=100，代表每100步进行一次交换，这个值越小，交换频率也越大，施加在物体的温差也越大。\\n可以修改这个Nstep来达到自己满意的温差。\\n后续也可以添加关键词swap，他代表每次交换动能的原子数，默认是1.这个值越大，每次交换的能量也越大，施加在物体的温差也越大。\\n所以他俩对温差的影响是相反的。\\n平衡问题 要设置合理的模拟时间，尽可能长一些。\\nMP方法计算热导率，除去最开始的体系平衡，还剩下两个重要阶段。\\n建立热流\\n也就是fix nve后的第一个run。\\nthermo会输出温差dTemp_step，计算时观察这个值，让他不在单调升高，出现稳定是最好的。\\n动量归零，开始计算热导率\\n也就是fix nve后的第二个run。\\n我们来看一下lammps官方的例子中，两个阶段的特点。\\n这是一阶段的温差随时间的变化，\\n可以看到，第一阶段的目的主要是为了建立起逐渐趋于稳定的温差。\\n这是第二阶段的温差随时间的变化，\\n可以看到，在官方的演示中，平衡后的二阶段，也会有或者说允许一些温差的升高，但总体变化不大。\\n个人观点：\\n第一阶段的步数可以取大一点，直到观察到了温差“饱和”现象。\\n第二阶段不必要和第一阶段等长。本质是稳定后，统计一段时间的平均。\\nNstep与温差大小、良好温度梯度的关系 从计算公式中，可以看出，KAPPA * ΔT 正比于 ΔE（动能交换），\\n那么我们可以假设体系要计算的热导率其实是固定的，我们动能交换的越频繁，ΔE越大，温差也就越大。\\n如果想要小的温差，只需要降低频率，增大Nstep就可以了。\\n但根据实践，Nstep取太大会导致slab间的温度梯度建立的效果很差。\\n其实很好理解，因为温差是通过fix 3 all thermal/conductivity 20 z 20建立的，设想一下，如果20换成2w，\\n那强制交换动能形成温差后，又会在接下来2w步中逐渐通过热平衡消除温度梯度，\\n所以Nstep不能太大，起码要大于自然热平衡的效果。\\n现在是Nstep取得20，第一阶段跑8w步，可以实现一阶段温差呈趋近效果，温度梯度建立得也比较良好。\\n"`,categories:'["MolecularDynamics"]',tags:[],series:'["LAMMPS"]',date:'"2025-05-16"'}),searchIndex.push({title:'"如何构建训练集用于训练机器学习势"',permalink:'"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%BF/mace1/"',content:`"如何构建训练集用于训练机器学习势 前言 本文将介绍如何构建一个训练集，用于训练MACE势以及DP势。需要提前利用AIMD获取DFT数据集。这里的AIMD软件是VASP。\\n软件：ASE、DeepMD-Kit\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\n构建MACE势的训练集 MACE接受的训练集非常简单，一个xyz文件，包含了各种构型和它们对应的DFT数据标签，以及单原子的DFT数据，需要额外的标签config_type=IsolatedAtom。\\n顺带一提，ASE可以输出一种所谓的 Extended XYZ format ，会把各种各样的信息（有点类似OVITO中的全局信息），放到xyz文件的第二行。这一行会很长很长。MACE所采用的训练集输入格式就是它。\\n假设我们要构建水分子的MACE势，当前所处的路径下，有两个文件夹。\\n一个名为H2O的文件夹，里面存放着进行第一性原理分子动力学后得到的OUTCAR.tar.gz文件。路径为H2O/OUTCAR.tar.gz。\\n一个名为IsolatedAtoms的文件夹，里面存放着涉及元素（这里是H、O）的单原子的单点能计算（ISPIN=2）。路径分别为IsolatedAtoms/H/OUTCAR.tar.gz和IsolatedAtoms/O/OUTCAR.tar.gz。\\n代码展示 from ase.io import read,write import random #定义一个简单的函数用于打标签,这里可以自由更改标签的名字 def addlabel(configs,energy_label='energy_dft',forces_label='forces_dft',stress_label='stress_dft',is_isolated=False): if is_isolated == False: for at in configs: at.info[energy_label] = at.get_potential_energy(force_consistent=True) at.arrays[forces_label] = at.get_forces() at.info[stress_label] = at.get_stress(voigt=True) if is_isolated == True: for at in configs: at.info['config_type'] = 'IsolatedAtom' at.info[energy_label] = at.get_potential_energy(force_consistent=True) at.arrays[forces_label] = at.get_forces() at.info[stress_label] = at.get_stress(voigt=True) #read()函数，这里，第一个参数是所读文件路径，第二个参数是切片slice IsolatedH = read('IsolatedAtoms/H/OUTCAR.tar.gz',':') IsolatedO = read('IsolatedAtoms/O/OUTCAR.tar.gz',':') IsolatedAtoms = IsolatedH + IsolatedO addlabel(configs=IsolatedAtoms,is_isolated=True) #这里的slice的意思是从第一个结构开始到最后一个结构，每100个结构取一个 db = read('H2O/OUTCAR.tar.gz','::100') addlabel(configs=db) #打乱训练集，这对训练非常重要 random.seed(42) random.shuffle(db) #将打过标签的数据集合并 db = db + IsolatedAtoms write('trainset.xyz',db) 这里有趣的一点是，为什么对于单个结构的OUTCAR，也要进行切片：IsolatedH = read('IsolatedAtoms/H/OUTCAR.tar.gz',':')，而不是IsolatedH = read('IsolatedAtoms/H/OUTCAR.tar.gz')。\\n这是因为read()函数读取只有一个原子的结构是，会返回atom类的实例，而非atoms类的实例。而atom类不支持info属性，会很麻烦。\\n即便是多原子的单结构，如果想把他们合并成一个轨迹，也一定要用切片的形式\\u0026quot;:\\u0026quot;读取，因为两个atoms类的实例加和会得到一个新的atoms类的实例。\\nfrom ase.io import read,write #假设H2O.cif是一个含有一个水分子的胞 #这样只会输出一个含有两个水分子的胞 db1 = read('H2O.cif') db2 = read('H2O.cif') db = db1 + db2 write('H2O.xyz',db) #使用切片后，read会返回list[atoms],再进行加和得到的是list[atoms1,atoms2],用write()函数写的时候，就能依次形成轨迹了 db1 = read('H2O.cif',':') db2 = read('H2O.cif',':') db = db1 + db2 write('H2O.xyz',db) 所以，如果我们想让单一结构也加入到我们的数据集时，也要记得用切片的形式进行读取。不过要铭记于心的是，用切片的形式读取的其实是一个list[atoms]，要使用其中atoms实例的方法时，记得用for循环遍历其中的atoms实例。\\n拟合势的energy是vasp计算中的哪个energy？ 虽然mace开发者已经指明，使用at.info[energy_label] = at.get_potential_energy(force_consistent=True)获取能量即可。\\n但还是好奇这个能量对应于OSZICAR中的哪个。\\n做了测试，对于单点能，OSZICAR包含F和E0两种能量，上述代码提取的是F，我的体系很多情况下F和E0是相等的。\\n对于AIMD，OSZICAR包含E、F、E0三种能量，第一个E其实是包含了动能，但我们拟合势只需要势能的值，所以提取的也还是F。\\nImportant 其实就是OUTCAR中的free energy TOTEN。\\n至于F和E0到底有什么区别？VASP没说明白，还需要更多实践。\\nAIMD的数据间隔取多少合适 个人认为这个和AIMD的timestep有关键，我的系统因为含氢，所以取得是0.5fs。如果步长比较大，间隔可以适当小一些。\\n取AIMD的间隔，用100和50做了尝试。\\n前者有5k个结构，后者有1w个结构。\\n前者的损失函数最终为0.049，后者为0.042。感觉差不多。\\n"`,categories:'["机器学习势"]',tags:[],series:'["ASE","DeepMD-Kit"]',date:'"2025-05-16"'}),searchIndex.push({title:'"机器学习势MACE的输入文件"',permalink:'"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%BF/mace2/"',content:`"机器学习势MACE的输入文件 前言 不同MACE版本的参数设置会有一定的调查，注意查看自己的MACE版本。这里是0.3.13版本\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\n在超算上用slurm提交python任务 #!/bin/bash #SBATCH -o job.%j.out #SBATCH -J lmp_job_gpu #SBATCH --partition=gpu #SBATCH -N 1 #SBATCH -G 1 #SBATCH --gres=gpu:1 #SBATCH --exclusive export PATH=/home-ssd/Users/nsgm_zcx/openmpi-5.0.5/build/bin:$PATH export LD_LIBRARY_PATH=/home-ssd/Users/nsgm_zcx/openmpi-5.0.5/build/lib:$LD_LIBRARY_PATH export PATH=/home-ssd/Users/nsgm_zcx/lammps-develop/build-mliap:$PATH source /home-ssd/Users/nsgm_zcx/miniconda3/etc/profile.d/conda.sh conda activate cuda mace_run_train --config parameters.yaml 前面几行是为了激活lammps和openmpi，在这里没啥用。\\n重要的是要在sh脚本里激活conda，然后用mace_run_train命令行脚本来提交训练任务。\\n用--config参数和一个yaml文件来提供训练参数。\\n以下是parameters.yaml的内容\\nname: YSZH_MACE_model seed: 123 device: cuda train_file: train.xyz valid_fraction: 0.2 test_file: test.xyz compute_forces: True compute_stress: True energy_key: energy_vasp forces_key: forces_vasp stress_key: stress_vasp E0s: 'isolated' hidden_irreps: '64x0e + 64x1o' r_max: 4.0 batch_size: 20 max_num_epochs: 600 swa: True start_swa: 480 ema: True ema_decay: 0.99 default_dtype: float32 lr: 0.01 scaling: rms_forces_scaling multiheads_finetuning: False enable_cueq: True 要使用mliap，就必须打开参数enable_cueq: True，并确保python环境中安装了cuEquivariance和cupy。\\n"`,categories:'["机器学习势"]',tags:[],series:'["MACE"]',date:'"2025-05-16"'}),searchIndex.push({title:'"均方位移（MSD）-OVITO"',permalink:'"/moleculardynamics/msd_ovito/"',content:`"均方位移（MSD）计算 by OVITO 前言 OVITO Python Reference — OVITO Python Reference 3.12.3 documentation 是一个开源且功能强大的分子动力学后处理软件包。\\n本文将介绍如何利用 OVITO python module 计算某类元素原子在一段轨迹内的均方位移。\\n适用于无机非晶体，其他体系慎用。\\n软件：OVITO、matplotlib、numpy\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\nOVITO版 代码展示 from ovito.io import import_file, export_file from ovito.modifiers import CalculateDisplacementsModifier from ovito.modifiers import SelectTypeModifier,InvertSelectionModifier,DeleteSelectedModifier,ExpressionSelectionModifier import numpy as np import matplotlib.pyplot as plt #万物起源 import_file ，导入一段要计算的轨迹 pipeline = import_file(\\u0026quot;1.dump\\u0026quot;) #添加 SelectTypeModifier 修饰器 #设置参数 property = 'Particle Type' 指定选择的类型（这里我们指定的是原子类型） #设置参数 types = {4} 指定具体的原子类型，这里是这个轨迹中的 4 原子（这个值要根据自己的体系修改），这里用数字代表原子是因为我使用的lammps的输出风格没有记录原子的元素符号，如果你的轨迹里记录的是 元素符号 信息，比如说 VASP 输出的 XDATCAR，则需要用类似于 types = {\\u0026quot;H\\u0026quot;} 的写法 pipeline.modifiers.append(SelectTypeModifier(property = 'Particle Type', types = {4})) #添加 InvertSelectionModifier 修饰器，进行原子反选，为剔除不需要计算的原子做准备 pipeline.modifiers.append(InvertSelectionModifier()) #添加 DeleteSelectedModifier 修饰器，删除上一行代码反选的原子，留下需要计算的原子 pipeline.modifiers.append(DeleteSelectedModifier(operate_on= {'particles'})) #添加 CalculateDisplacementsModifier 修饰器，指定计算 MSD 的参考结构，这里 reference_frame = 0 代表初始结构是参考结构 reference_frame = 0 pipeline.modifiers.append(CalculateDisplacementsModifier(reference_frame=0)) #a subclass of ovito.pipeline.ReferenceConfigurationModifier #自定义一个修饰器函数，用于将 per-particle displacement 转化为相应元素的均方位移 #本文的 OVITO小知识 将简单介绍自定义修饰器是如何工作的 def calculate_msd(frame, data): #用一个变量 displacement_magnitudes 记录 data.particles['Displacement Magnitude']，简化代码 displacement_magnitudes = data.particles['Displacement Magnitude'] #计算 MSD （将所有原子位移的平方加和然后求平均），OVITO 的数据可以直接和 numpy 交互，nice msd = np.sum(displacement_magnitudes ** 2) / len(displacement_magnitudes) #把计算的 MSD 传递给 data (DataCollection类) data.attributes[\\u0026quot;MSD\\u0026quot;] = msd #添加自定义 calculate_msd 修饰器 pipeline.modifiers.append(calculate_msd) #计算 Pipeline, 得到time vs MSD的数据 table = [] #用于存放数据，time vs MSD for frame,data in enumerate(pipeline.frames): if frame \\u0026gt;= reference_frame: #这里的 *10 一定要根据自己的计算调整，我的轨迹在lammps计算设置：时间步是0.5fs，每20步输出一帧，所以轨迹中每帧其实经历了10fs，所以乘以10 #我们 time vs MSD 的x横坐标单位是fs，也可以是别的，自己调整 time = (frame-reference_frame)*10 table.append([time,data.attributes['MSD']]) #.csv文件还是比较高级的，比纯txt好些，delimiter 指定间隔符为 \\u0026quot;,\\u0026quot; ,这样方便直接excel打开 np.savetxt(\\u0026quot;msd_data.csv\\u0026quot;,table,delimiter=\\u0026quot;,\\u0026quot;) OVITO小知识 受限于Python基础和时间精力的限制，以下内容皆为我个人的有限理解，未能严格考究，仅供参考。\\n自定义修饰器 这里介绍的自定义修饰器，按官方说法是 Simple programming interface，它是一个函数，接受两个基本输入：frame 和 data。\\n即，def modify(frame: int, data: DataCollection):\\n注意：不要返回任何值，数据都应保存在 data 这个 DataCollection 类中\\ndef calculate_msd(frame, data): displacement_magnitudes = data.particles['Displacement Magnitude'] msd = np.sum(displacement_magnitudes ** 2) / len(displacement_magnitudes) data.attributes[\\u0026quot;MSD\\u0026quot;] = msd DataCollection 类的 attributes 属性储存了这个实例数据集的所有的global attributes（全局信息）。\\nattributes 属性是由经过@propert装饰器装饰的方法，这个方法返回一个辅助类，也可以叫功能类（_AttributesView）的实例，_AttributesView是抽象基类MutableMapping的子类。\\n_AttributesView类用于实现类似字典的功能。\\n每次访问attributes 属性时，都会返回一个_AttributesView类的实例，这个实例会先通过__init__接收DataCollection实例的数据，并支持对其进行字典操作。\\n所以，当我们data.attributes[\\u0026quot;MSD\\u0026quot;] = msd时，是通过_AttributesView类提供的字典操作，把MSD数据添加到了DataCollection实例中。\\n由于_AttributesView类实现了__repr__方法，我们可以直接print(data.attributes)来查看包含了哪些全局信息。\\nfor frame,data in enumerate(pipeline.frames):语句 DataCollection要pipeline经过compute()方法得到，但这个例子中并没有见到compute()方法。其实隐藏在了这句for循环中。\\npipeline.frames会返回一个迭代器，这个迭代器中yield产生每一帧的DataCollection。\\nReferences 1. OVITO官方的MSD脚本 "`,categories:'["MolecularDynamics"]',tags:[],series:'["OVITO"]',date:'"2025-05-13"'}),searchIndex.push({title:'"稍微深入一些Python中的类（class）"',permalink:'"/%E4%BB%A3%E7%A0%81/class_in_python/"',content:`"稍微深入一些Python中的类（class） 前言 **类（class）**在python代码中几乎无处不在，但在近日的学习中发现，我对它真是了解甚少，甚至基础结构都不能熟稔于心，故开此笔记认真学习。和我一起重新认识一下它吧。\\n一个简单的类 #code 1 class Dog: # 类属性 species = \\u0026quot;Dog\\u0026quot; # 初始化方法 def __init__(self, name, age): self.name = name self.age = age # 实例方法 def bark(self): return print(\\u0026quot;旺旺\\u0026quot;) print(mydog.species) print(mydog.name,mydog.age) mydog.bark() #输出为： #Dog #doudou 2 #旺旺 类是一种对数据进行计算操作的蓝图，离不开属性和方法。\\ncode 1中，我们定义了一个 Dog 类，并对它进行了实例化，生成了一个对象。\\n这个类的结构，很简单。\\n首先是放在第一部分的类属性。类属性是直接在类中定义变量。所有通过这个类生成的对象都具有这些属性。\\n然后是放在第二部分的诸多方法，其实就是一个个的函数。\\ndef __init__(self, name, age): self.name = name self.age = age _init_方法叫初始化方法，是魔法方法的一种。让实例初始化时就具有name和age属性和相应的值。具体表现为mydog = Dog(\\u0026quot;doudou\\u0026quot;,2)，这个类在初始化时就需要两个参数才能转变为实例。\\npython中所有的实例方法，包括__init__方法，第一个参数都必须是self，用于区别普通函数和方法。\\n默认值，在定义方法的时候，可以传入默认值，这可以保证在不传入参数时，也能生成一个默认实例。\\ndef __init__(self, name=\\u0026quot;小卡拉米\\u0026quot;, age=5): self.name = name self.age = age 这样的__init__方法就保证了，即使狗主人忘记了填写信息，mydog = Dog()也能正常工作，但默认的狗狗是小卡拉米，还是应该记得为自己的狗狗正确填写信息呦。（对于其他实例方法也适用）\\n在一般的实例方法中，也可以设置参数，定义属性。我们看一段新的代码:\\n#code 2-1 class phone: owner = \\u0026quot;xiyangyang\\u0026quot; def __init__(self,type=\\u0026quot;huawei\\u0026quot;): self.type = type def call(self,number=10086): self.number = number print(\\u0026quot;the number is {}\\u0026quot;.format(self.number)) print(phone.owner) myphone = phone(\\u0026quot;apple\\u0026quot;) myphone.call(110) print(\\u0026quot;what number is called? \\u0026quot;,myphone.number) 这里在phone类的call方法下，定义了一个实例属性，并需要一个输入number指定值。\\n但实例方法的参数不一定都是实例属性，且看以下代码：\\n#code 2-2 class phone: def __init__(self,type=\\u0026quot;huawei\\u0026quot;): self.type = type def call(self,number=10086): print(\\u0026quot;the number is {}\\u0026quot;.format(number)) myphone = phone(\\u0026quot;apple\\u0026quot;) myphone.call(110) 这样定义call方法，在使用的时候也需要传入一个number，但是这个number不是这个实例的属性，无法随时查看。\\n这就是说，对于类中的方法，目前涉及的参数有三类，一个是默认参数self，一个是实例属性参数，一个是普通参数。\\n截至目前，我们已经了解了一个简单类的结构。即：声明类属性，声明实例方法（其中涉及到添加实例属性）。 由类生成实例时的参数接口，由魔法方法__init__定义，而一般实例方法的接口，在调用方法的时候才会出现。这也就意味着，__init__方法中定义的实例属性，是伴生的，只要生成实例就存在。而随一般实例方法定义的实例属性，则需要首次调用后才存在。 魔法方法 在python中，所有被双下划线包围的方法，统称为魔法方法。太多了，我这里只记录我遇到的，不定时更新。\\n1.__init__方法 这个魔法方法用于类的初始化。规定了实例化类时接受的参数。\\n2.__len__方法 #code 3-1 class menu: restaurant=\\u0026quot;hepingfandian\\u0026quot; def __init__(self,foods): self.foods = foods def __len__(self): count = 0 for food in foods: count = count + 1 return count mymenu = menu([\\u0026quot;宫保鸡丁\\u0026quot;,\\u0026quot;鱼香肉丝\\u0026quot;,\\u0026quot;米饭\\u0026quot;]) print(len(mymenu)) #输出：3 这个魔法方法让实例可以被用len()函数统计长度，核心是要返回一个整数，至于这个整数是如何计算得到的，这部分内容就由自己定义了。\\n3.__getitem__方法 #code 3-2 class menu: restaurant=\\u0026quot;hepingfandian\\u0026quot; def __init__(self,foods): self.foods = foods def __getitem__(self,key): return 10 mymenu = menu([\\u0026quot;宫保鸡丁\\u0026quot;,\\u0026quot;鱼香肉丝\\u0026quot;,\\u0026quot;米饭\\u0026quot;]) print(mymenu[\\u0026quot;霸王餐\\u0026quot;]) #输出：10 这个魔法方法可以让实例像字典或列表一样，实现键值对和索引切片的功能。具体来说，就是根据条件返回不同的值。不然就像上述代码一样，客人要吃霸王餐，对应的值却是10。\\n#code 3-3 class menu: restaurant=\\u0026quot;hepingfandian\\u0026quot; def __init__(self,foods): self.foods = foods def __getitem__(self,key): if not isinstance(key,str): raise TypeError(\\u0026quot;Attribute key must be a string\\u0026quot;) if key == \\u0026quot;霸王餐\\u0026quot;: return \\u0026quot;你吃牛魔\\u0026quot; for food in self.foods: if food == key: return self.foods[key] raise KeyError(f\\u0026quot;Food '{key}' does not exist in menu.\\u0026quot;) mymenu = menu({\\u0026quot;宫保鸡丁\\u0026quot;:25,\\u0026quot;鱼香肉丝\\u0026quot;:10,\\u0026quot;米饭\\u0026quot;:2}) print(mymenu[\\u0026quot;霸王餐\\u0026quot;]) print(mymenu[\\u0026quot;米饭\\u0026quot;]) #输出：你吃牛魔 #输出：2 __getitem__方法可以为实例提供独特的接口，就是[参数]。这里的参数可以是字符串、数字或者切片，使用起来像字典或者列表。而一般方法的使用则要.onemethod(参数1，参数2，...)。\\n#code 3-4 class top3: ip = \\u0026quot;harbin\\u0026quot; def __init__(self,school): self.school = school def __getitem__(self,index): if isinstance(index,int): return self.school[index] if isinstance(index,slice): return self.school[index] Top3InMyheart = top3([\\u0026quot;qinghua\\u0026quot;,\\u0026quot;beida\\u0026quot;,\\u0026quot;hagongda\\u0026quot;]) print(\\u0026quot;who is top3?\\u0026quot;,Top3InMyheart[0:3]) print(Top3InMyheart.ip) 这段代码定义了一个名叫top3类，实现了索引和切片功能，同时可以这个类的实例拥有列表不同的功能，即这个类的实例有一个类属性：ip。让我们可以得知这个排名来自harbin。\\n4.__setitem__方法 #code 3-5 class games: def __init__(self): self.games={} def __setitem__(self,key,value): self.games[key]=value def __getitem__(self,key): return self.games[key] MyLoveGames = games() MyLoveGames[1] = \\u0026quot;原神\\u0026quot; print(MyLoveGames[1]) #输出：原神 这个魔法方法让类的实例可以像字典或者列表一样，添加新的键值对或者添加新的索引和值。\\n5.__delitem__方法 #code 3-6 class games: def __init__(self): self.games={} def __setitem__(self,key,value): self.games[key]=value def __getitem__(self,key): return self.games[key] def __delitem__(self,key): del self.games[key] MyLoveGames = games() MyLoveGames[1] = \\u0026quot;原神\\u0026quot; MyLoveGames[1] = \\u0026quot;DOTA2\\u0026quot; print(MyLoveGames[1]) del MyLoveGames[1] #输出：DOTA2 这个魔法方法可以让类的实例使用del关键字来删除键或索引。\\n6.__iter__方法 #code 3-7 class games: def __init__(self): self.games={} def __setitem__(self,key,value): self.games[key]=value def __getitem__(self,key): return self.games[key] def __delitem__(self,key): del self.games[key] def __iter__(self): for game in self.games: yield game MyLoveGame = games() MyLoveGame[0] = \\u0026quot;原神\\u0026quot; MyLoveGame[3] = \\u0026quot;DOTA2\\u0026quot; MyLoveGame[2] = \\u0026quot;明日方舟\\u0026quot; del MyLoveGame[2] for game in MyLoveGame: print(game) #输出：0 #输出：3 这个魔法方法让实例变为可迭代对象，可以进行for循环。也可以用iter()函数生成迭代器。这里涉及到一个头疼的yield关键字，又涉及到生成器和迭代器。还是另开一文来记录吧。\\n简单来讲，当一个可迭代对象遇到for循环事件时，会自动转到它的__iter__方法，又因为这里的__iter__方法含有yield关键字，所以不会立即执行，而是得到了一个生成器对象。\\n紧接着，for循环又根据这个生成器对象的__next__方法（注意，这里是生成器对象的__next__方法，而非是，我们这个自定义类的__next__方法，其实，我们这里也没有写__next__方法）把得到的值赋值给for game in MyLoveGame:中的game，然后打印。\\n在进行第二次循环的时候，同样地，再次进入实例的__iter__方法，得到同一个生成器对象。并对这个生成器对象再此施加__next__方法，并把得到的值赋值给for game in MyLoveGame:中的game，然后打印。\\n谁是for game in MyLoveGame:中的game值？ __iter__方法本质上定义了一个生成器对象，而非函数。for循环会不断调用这个生成器对象__next__方法并把得到的值传递给game。\\n7.__repr__方法 #code 3-8 import collections.abc class games(collections.abc.MutableMapping): def __init__(self): self.games={} def __len__(self): return 0 def __setitem__(self,key,value): self.games[key]=value def __getitem__(self,key): return self.games[key] def __delitem__(self,key): del self.games[key] def __iter__(self): for game in self.games: yield game def __repr__(self): return repr(dict(self)) MyLoveGame = games() MyLoveGame[0] = \\u0026quot;原神\\u0026quot; MyLoveGame[1] = \\u0026quot;DOTA2\\u0026quot; MyLoveGame[2] = \\u0026quot;明日方舟\\u0026quot; print(MyLoveGame) #输出：{0: '原神', 1: 'DOTA2', 2: '明日方舟'} 这里让自定义的games继承了一个抽象基类，这样return repr(dict(self))才不报错。\\n这个魔法方法让实例可以直接使用实例时，返回一个官方字符串。\\n继承与抽象基类 如何继承父类 #code 4-1 class student: def __init__(self,gender=1,age=18): self.gender = gender self.age = age class xiaoxiaobuaigugujiao(student): def interests(self,data): self.interests = data print(self.interests) me = xiaoxiaobuaigugujiao() print(me.gender) print(me.age) me.interests(\\u0026quot;games\\u0026quot;) 在定义子类的时候，在子类的名字后加上(父类名)即可。\\nsuper()函数 #code 4-2 class student: def __init__(self,gender=1,age=18): self.gender = gender self.age = age class xiaoxiaobuaigugujiao(student): #子类中，当覆盖父类中的同名方法后，与父类同名方法的默认参数也要重新写一篇，不会继承 def __init__(self,gender=1,age=18,height=170): #调用父类的构造方法 super().__init__(gender,age) #额外补充 self.height = height def interests(self,data): self.interests = data print(self.interests) me = xiaoxiaobuaigugujiao() print(me.gender) print(me.age) print(me.height) me.interests(\\u0026quot;games\\u0026quot;) super()函数通常出现在子类定义方法的时候，用于调用父类的构造方法，并可以加以补充，这里我们在子类初始化时，就调用了父类student的构造方法，并额外补充了height。\\n抽象基类 抽象基类只能被继承。它要求子类必须实现某些抽象方法。\\n"`,categories:'["代码"]',tags:[],series:'["python"]',date:'"2025-05-13"'}),searchIndex.push({title:'"径向分布函数（RDF）-OVITO"',permalink:'"/moleculardynamics/rdf_ovito/"',content:`"径向分布函数（RDF）计算 by OVITO 前言 OVITO Python Reference — OVITO Python Reference 3.12.3 documentation 是一个开源且功能强大的分子动力学后处理软件包。\\n本文将介绍如何利用 OVITO python module 计算单个结构以及一段轨迹（多个结构）内的径向分布函数。\\n适用于无机非晶体，其他体系慎用。\\n软件：OVITO、matplotlib、numpy\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\nThe partial RDFs of a single crystal structure 代码展示 #这段代码用于计算 RDF by OVITO from ovito.io import import_file from ovito.modifiers import CoordinationAnalysisModifier import numpy as np #导入一个氧化锆（ZrO2）的cif文件，所有OVITO支持的输入文件格式都可以（确保这个.py文件的路径下有这样一个cif文件，也可以稍微修改指定结构路径） pipeline = import_file(\\u0026quot;ZrO.cif\\u0026quot;) #施加一个名叫 CoordinationAnalysisModifier 的修饰器，cutoff用于控制截断半径，number_of_bins用于控制网格细分度（大小100-1000内都可以试试） pipeline.modifiers.append(CoordinationAnalysisModifier(cutoff = 5.0, number_of_bins = 500,partial=True)) #进行计算 rdf_table = pipeline.compute().tables['coordination-rdf'] #得到用于画图的横纵坐标，默认第一列是x轴数据，其余列是y轴数据 total_rdf = rdf_table.xy() #记录total_rdf中y轴数据对应是什么类型的pair-wise #这个例子中，输出为： #g(r) for pair-wise type combination O-O: #g(r) for pair-wise type combination O-Zr: #g(r) for pair-wise type combination Zr-Zr: #说明total_rdf是一个四列的数据，第一列是x轴坐标（其实是bin），第二列就是不同pair-wise的RDF数据，依次为 O-O,O-Zr,Zr-Zr rdf_names = rdf_table.y.component_names for component, name in enumerate(rdf_names): print(\\u0026quot;g(r) for pair-wise type combination %s:\\u0026quot; % name) #将total_rdf保存为txt文件，用于后续画图 np.savetxt(\\u0026quot;total_rdf.txt\\u0026quot;, total_rdf) #这段代码用于绘图 import numpy as np import matplotlib.pyplot as plt rdf_table = np.loadtxt('total_rdf.txt') #g(r) for pair-wise type combination O-O: #g(r) for pair-wise type combination O-Zr: #g(r) for pair-wise type combination Zr-Zr: #这里取的是 total_rdf.txt 中的第一列（对应[:,0]）和第三列（对应[:,2]），所以绘制的是 Zr-O pair-wise的partial RDF plt.plot(rdf_table[:,0], rdf_table[:,2]) #matplotlib的常规设置，问问万能的小迪老师吧 title_font = {'fontsize': 24, 'fontfamily': 'Times New Roman'} xlabel_font = {'fontsize': 22, 'fontfamily': 'Times New Roman'} ylabel_font = {'fontsize': 22, 'fontfamily': 'Times New Roman'} plt.title(\\u0026quot;RDF Zr-O\\u0026quot;, fontdict=title_font,pad=8) plt.xlabel(xlabel='distance r',fontdict=xlabel_font,loc='center',labelpad=8) plt.ylabel(ylabel='g(r)',fontdict=ylabel_font,loc='center',labelpad=8) plt.tick_params(axis='both', which='major', labelsize=16, direction='in') ax = plt.subplot() #因为只有一个静态结构，pair-wise的某些峰很高，所以这里的y轴坐标上限设置大一些，为200，可灵活改变 #x轴设置为6，稍大于截断半径cutoff即可，因为本身也只在截断半径以内统计 ax.set_ylim(0,200) plt.xlim(0,6) fig = plt.gcf() fig.set_size_inches(1200/100, 800/100) plt.savefig('output.png', dpi=100) plt.show() 结果展示 The partial RDFs of trajectories 代码展示 #这段代码用于计算一定时间内（一段轨迹）的平均 RDFs from ovito.io import import_file from ovito.modifiers import CoordinationAnalysisModifier,TimeAveragingModifier import numpy as np #读入轨迹文件，这里是利用 VASP 进行 AIMD 后得到的 XDATCAR 文件 pipeline = import_file(\\u0026quot;XDATCAR\\u0026quot;) #打印轨迹中的结构数 print(\\u0026quot;Number of MD frames:\\u0026quot;, pipeline.num_frames) #添加修饰器，与单个晶体结构相比，多了 TimeAveragingModifier 修饰器 pipeline.modifiers.append(CoordinationAnalysisModifier(cutoff = 5.0, number_of_bins = 500,partial=True)) pipeline.modifiers.append(TimeAveragingModifier(operate_on='table:coordination-rdf')) #计算 RDFs 数据 total_rdf = pipeline.compute().tables['coordination-rdf[average]'].xy() #记录pair-wise类型 rdf_names = pipeline.compute().tables['coordination-rdf[average]'].y.component_names for name in rdf_names: print(\\u0026quot;g(r) for pair-wise type combination %s:\\u0026quot; % name) #输出数据，用于后续绘图，不再重复 np.savetxt('rdf.txt', total_rdf, delimiter='\\\\t') OVITO小知识 受限于Python基础和时间精力的限制，以下内容皆为我个人的有限理解，未能严格考究，仅供参考。\\n在 The partial RDFs of a single crystal structure 的计算代码中 pipeline = import_file(\\u0026quot;ZrO.cif\\u0026quot;) 这段代码将外来的 cif 文件转化为 ovito 的一个 Pipeline 类的实例。\\n然后我们添加了修饰器，代码省略。\\n再然后是调用 Pipeline 类的 compute() 方法，得到一个非常重要的 DataCollection 类的实例。\\n再访问这个 DataCollection 实例的 tables 属性就得到一个DataTable 类的实例（在DataCollection 类的定义中，用 @property 的 装饰器语法将 tables() 方法转化为了 tables 属性，所以与 compute() 方法相比，不需要 () 了）\\nDataTable 类用于储存绘制直方和2d图的数据，当我们添加 CoordinationAnalysisModifier 修饰器后，经过 compute() 方法后，相应的 partial RDFs 数据就会被储存在这里，可以通过名为 'coordination-rdf' 的 键(key) 来检索。\\n我觉得 DataTable 类的实例绝非一个简单的字典，但把它当作字典来理解会比较容易。\\nrdf_table = pipeline.compute().tables['coordination-rdf'] 即，rdf_table 仍然属于 DataTable 类。接着用 .y.component_names 来获取 多组y轴数据 的表头。这里太复杂了，我也没看太懂，不展开了。\\nrdf_names = rdf_table.y.component_names 在 The partial RDFs of trajectories 的计算代码中 原本 compute() 方法只能用于单个结构，对于 trajectories 需要指定是哪个结构。\\n但添加 TimeAveragingModifier 修饰器后，允许我们计算时间相关量的平均值。详见：https://docs.ovito.org/python/modules/ovito_modifiers.html#ovito.modifiers.TimeAveragingModifier\\nReferences ovito.modifiers — OVITO Python Reference 3.12.3 documentation "`,categories:'["MolecularDynamics"]',tags:[],series:'["OVITO"]',date:'"2025-05-12"'}),searchIndex.push({title:'""',permalink:'"/dft/yambo/"',content:'"yambo Background theory Single particle Green\\u0026rsquo;s function "',categories:[],tags:[],series:[],date:'"0001-01-01"'}),searchIndex.push({title:'""',permalink:'"/dft/%E6%B0%A2%E5%92%8C%E6%B0%B4%E5%BB%BA%E6%A8%A1/"',content:`"氢水建模 import numpy as np from ase.build import molecule from ase import Atoms from ase.io import read from ase.io.vasp import write_vasp def rotation_matrix_from_euler(alpha, beta, gamma, degrees=True): \\u0026quot;\\u0026quot;\\u0026quot; 生成绕 x, y, z 轴依次旋转 alpha, beta, gamma 的旋转矩阵。 Rotation applied as: first Rx(alpha), then Ry(beta), then Rz(gamma). 返回一个 3x3 的 numpy 矩阵 R，使得 r' = R @ r (列向量)。 \\u0026quot;\\u0026quot;\\u0026quot; if degrees: a = np.deg2rad(alpha) b = np.deg2rad(beta) g = np.deg2rad(gamma) else: a, b, g = alpha, beta, gamma Rx = np.array([[1, 0, 0], [0, np.cos(a), -np.sin(a)], [0, np.sin(a), np.cos(a)]]) Ry = np.array([[ np.cos(b), 0, np.sin(b)], [0, 1, 0], [-np.sin(b), 0, np.cos(b)]]) Rz = np.array([[np.cos(g), -np.sin(g), 0], [np.sin(g), np.cos(g), 0], [0, 0, 1]]) # 总旋转：先 Rx 然后 Ry 然后 Rz -\\u0026gt; R = Rz @ Ry @ Rx R = Rz @ Ry @ Rx return R def add_water(atoms, target_pos, orientation=(0, 0, 0), anchor='COM', use_fractional=True, cell=None): if cell is None: cell = atoms.get_cell() # 计算目标笛卡尔坐标 if use_fractional: if cell is None or np.linalg.norm(cell) == 0: raise ValueError(\\u0026quot;没有可用的 cell 来把分数坐标转换为笛卡尔。传入 cell 或将 use_fractional=False。\\u0026quot;) target_cart = np.dot(np.asarray(target_pos), cell) # frac @ cell else: target_cart = np.asarray(target_pos, dtype=float) # 生成水分子 water = molecule(\\u0026quot;H2O\\u0026quot;) # ASE 内置 # 选择锚点并将其移动到原点 if anchor.upper() == 'COM': ref = water.get_center_of_mass() elif anchor.upper() == 'O': syms = water.get_chemical_symbols() try: o_idx = syms.index('O') ref = water.positions[o_idx].copy() except ValueError: # 退回到 COM ref = water.get_center_of_mass() else: raise ValueError(\\u0026quot;anchor 必须是 'COM' 或 'O'。\\u0026quot;) # 把参考点移动到原点 water.translate(-ref) # 应用旋转（关于原点） R = rotation_matrix_from_euler(*orientation, degrees=True) # positions 为 (N,3)，对每个原子应用 R: pos' = (R @ pos.T).T water.positions = (R @ water.positions.T).T # 平移到目标笛卡尔位置 water.translate(target_cart) # 合并并返回 new_atoms = atoms + water return new_atoms from ase.io import read prim = read('../../CONTCAR') new_atoms = add_water(prim, target_pos=(0.5, 0.5, 0.75), orientation=(10, 20, 90)) from ase.io import write write_vasp(\\u0026quot;0Vo/ori2/POSCAR\\u0026quot;, new_atoms, direct=True,sort=True) import numpy as np from ase.build import molecule from ase import Atoms from ase.io import write, read from ase.io.vasp import write_vasp def rotation_matrix_from_euler(alpha, beta, gamma, degrees=True): \\u0026quot;\\u0026quot;\\u0026quot;生成绕 x, y, z 轴依次旋转的旋转矩阵。\\u0026quot;\\u0026quot;\\u0026quot; if degrees: a = np.deg2rad(alpha) b = np.deg2rad(beta) g = np.deg2rad(gamma) else: a, b, g = alpha, beta, gamma Rx = np.array([[1, 0, 0], [0, np.cos(a), -np.sin(a)], [0, np.sin(a), np.cos(a)]]) Ry = np.array([[ np.cos(b), 0, np.sin(b)], [0, 1, 0], [-np.sin(b), 0, np.cos(b)]]) Rz = np.array([[np.cos(g), -np.sin(g), 0], [np.sin(g), np.cos(g), 0], [0, 0, 1]]) return Rz @ Ry @ Rx def add_h2(atoms, target_pos, orientation=(0, 0, 0), anchor='COM', use_fractional=True, cell=None): \\u0026quot;\\u0026quot;\\u0026quot; 在 atoms 中添加一分子 H2。 参数: atoms : ASE Atoms 对象 target_pos : 目标位置。如果 use_fractional=True，传入分数坐标 (fx,fy,fz)，否则笛卡尔坐标 (Å)。 orientation : (alpha, beta, gamma)，绕 x,y,z 轴的欧拉角（度）。 anchor : 'COM' 或 'H1'。决定氢分子哪个点对齐到 target_pos。 'COM' -\\u0026gt; 质心对齐； 'H1' -\\u0026gt; 把第一个氢原子对齐到目标。 use_fractional: 是否把 target_pos 当作分数坐标，默认 True。 cell : 可选晶胞。如果 None，则用 atoms.get_cell()。 返回: new_atoms : 含有氢分子的 ASE Atoms 对象 \\u0026quot;\\u0026quot;\\u0026quot; if cell is None: cell = atoms.get_cell() # 计算目标笛卡尔坐标 if use_fractional: target_cart = np.dot(np.asarray(target_pos), cell) # frac -\\u0026gt; cart else: target_cart = np.asarray(target_pos, dtype=float) # 生成氢分子 h2 = molecule(\\u0026quot;H2\\u0026quot;) # 锚点 if anchor.upper() == 'COM': ref = h2.get_center_of_mass() elif anchor.upper() == 'H1': ref = h2.positions[0].copy() else: raise ValueError(\\u0026quot;anchor 必须是 'COM' 或 'H1'。\\u0026quot;) # 移动锚点到原点 h2.translate(-ref) # 旋转 R = rotation_matrix_from_euler(*orientation, degrees=True) h2.positions = (R @ h2.positions.T).T # 平移到目标 h2.translate(target_cart) # 合并 new_atoms = atoms + h2 return new_atoms from ase.io import read prim = read('../../CONTCAR') new_atoms = add_h2(prim, target_pos=(0.5, 0.5, 0.25), orientation=(45, 45, 0)) from ase.io import write write_vasp(\\u0026quot;ori2/POSCAR\\u0026quot;, new_atoms, direct=True,sort=True) "`,categories:[],tags:[],series:[],date:'"0001-01-01"'}),debug("当前本站共有 "+searchIndex.length+" 篇文章"),console.log("搜索索引:",searchIndex);const options={includeScore:!0,threshold:.6,keys:[{name:"title",weight:.7},{name:"content",weight:.3},{name:"tags",weight:.5},{name:"categories",weight:.5},{name:"series",weight:.5}]},fuse=new Fuse(searchIndex,options);document.addEventListener("keydown",function(e){if(e.key==="Escape"){document.getElementById("search-input").value="";for(var t=document.getElementById("search-results");t.firstChild;)t.removeChild(t.firstChild);debug("当前本站共有 "+searchIndex.length+" 篇文章"),document.getElementById("search-input").focus()}}),document.getElementById("search-input").addEventListener("input",function(e){const n=e.target.value.trim(),s=document.getElementById("search-results");if(s.innerHTML="",n===""){debug("当前本站共有 "+searchIndex.length+" 篇文章");return}debug("搜索词: '"+n+"'");const t=fuse.search(n);if(debug("找到 "+t.length+" 条结果"),console.log("搜索结果:",t),t.length===0){s.innerHTML="<p>没有找到相关文章</p>";return}t.forEach(e=>{const o=e.item,t=document.createElement("div");t.className="search-result-item";const i=document.createElement("span");i.textContent=o.date.replace(/^"|"$/g,"")+"   ",i.className="search-result-date";const n=document.createElement("a");n.href=o.permalink.replace(/^"|"$/g,""),n.textContent=o.title.replace(/^"|"$/g,""),n.className="search-result-title",t.appendChild(i),t.appendChild(n),s.appendChild(t)})})</script><style>.search-container{margin:2rem 0;text-align:left}#search-input{width:100%;padding:.8rem;font-size:1rem;border:1px solid #ddd;border-radius:4px;margin-bottom:1rem}.search-results-list{list-style:none;padding:0;margin:1rem 0}.search-result-item{padding:.7rem;margin-bottom:.5rem;border-bottom:1px solid #eee;display:flex;align-items:center}.search-result-date{color:#666;font-size:.9rem;margin-right:.5rem;text-wrap:nowrap}.search-result-title{font-weight:700;color:#1a73e8;text-decoration:none}.search-result-title:hover{text-decoration:underline}</style><footer><hr>©
<a href=https://liubaoshuai1402.github.io/ target=_blank rel="noopener noreferrer">xiaoxiaobuaigugujiao</a>
2025 &ndash; 2025 | 🎨
<a href=https://github.com/captainwc/ksimple target=_blank rel="noopener noreferrer">KSimple</a></footer></body></html>