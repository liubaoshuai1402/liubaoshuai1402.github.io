<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>深度学习s on BOGHTW</title><link>https://liubaoshuai1402.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 深度学习s on BOGHTW</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Wed, 28 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://liubaoshuai1402.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>图池化</title><link>https://liubaoshuai1402.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E6%B1%A0%E5%8C%96/</link><pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate><guid>https://liubaoshuai1402.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9B%BE%E6%B1%A0%E5%8C%96/</guid><description/></item><item><title>Pytorch中的张量操作</title><link>https://liubaoshuai1402.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C/</link><pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate><guid>https://liubaoshuai1402.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C/</guid><description>&lt;h1 id="pytorch中的张量操作"&gt;Pytorch中的张量操作&lt;/h1&gt;
&lt;h2 id="数学符号"&gt;数学符号&lt;/h2&gt;
&lt;h3 id="1--运算符"&gt;1. * 运算符&lt;/h3&gt;
&lt;p&gt;如果张量的形状完全一样，张量 * 张量，其实就是对应元素相乘，也可以$\odot$表示。&lt;/p&gt;
&lt;p&gt;如果张量的形状不一样，则要求他们的形状只能在一个维度上不一样，且其中一个张量这个维度的&lt;code&gt;size&lt;/code&gt;为1，这样可以用广播机制补全后相乘。&lt;/p&gt;
&lt;p&gt;如果张量 * 数字，则是全体元素乘以这个数字。&lt;/p&gt;
&lt;h2 id="函数方法"&gt;函数、方法&lt;/h2&gt;
&lt;h3 id="1-permute"&gt;1. &lt;code&gt;permute()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;可以作为张量对象的方法使用，接受置换后的维度。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import torch

table = torch.randn(2,3,4)
print(table)
print(table.permute(2,1,0))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这段代码就是将第一维和第三维进行置换。&lt;/p&gt;
&lt;p&gt;由已知的张量还有置换后的顺序，怎么写出新的张量呢，这个问题一直让我困扰。&lt;/p&gt;
&lt;img src="https://xiaoxiaobuaigugujiao.oss-cn-beijing.aliyuncs.com/img/permute.jpg" style="zoom:50%;" /&gt;
&lt;p&gt;今天的学习，我脑子里突然蹦出一个邻居的概念。对于一个四维的张量元素，它应该有四个不同维度的邻居，但如果仅仅把视角局限在矩阵里，那它就只有两个邻居，一个行维度上的，一个列维度上的，这很不对。得把视野打开，通道维度和批次维度的邻居可能隔了很远，但它们也是邻居。&lt;/p&gt;
&lt;h3 id="2-reshape和view"&gt;2. &lt;code&gt;reshape()&lt;/code&gt;和&lt;code&gt;view()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;深度学习中，经常会把高维张量降维，计算后再展开，要小心翼翼，错位会带来灾难性的错误。&lt;/p&gt;
&lt;p&gt;对于一个（N，F，M）的张量，F是其特征维度。如果想把N和M合并，&lt;mark&gt;应该先把F变成最高维度或者最低维度，让需要合并的维度接壤&lt;/mark&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import torch

table = torch.randn(2,3,4)
print(table)
a = table.reshape(3,8)
b = table.permute(1,0,2).reshape(3,8)

print(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里先把张量形状变成了（3，2，4），再合并，可以保证张量维度不会错位，如果直接改写性质为（3，8），则会错位。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;reshape()&lt;/code&gt;和&lt;code&gt;view()&lt;/code&gt;是一样的，前者可以作用于numpy数组。&lt;/p&gt;
&lt;h3 id="3-flatten"&gt;3. &lt;code&gt;flatten()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;将高维张量展平为一维张量。&lt;/p&gt;
&lt;h3 id="4-torchcat"&gt;4. &lt;code&gt;torch.cat()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;把张量按给定的顺序和维度进行拼接，除了拼接维度外，张量必须具有相同的形状。&lt;/p&gt;
&lt;p&gt;可用于特征维度的拓展，加入新的特征。&lt;/p&gt;
&lt;h3 id="5-unsqueeze"&gt;5. &lt;code&gt;unsqueeze()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;
&lt;a href="https://docs.pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze" target="_blank" rel="noopener noreferrer" &gt;torch.unsqueeze&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;torch.unsqueeze(&lt;em&gt;input&lt;/em&gt;, &lt;em&gt;dim&lt;/em&gt;) → Tensor&lt;/p&gt;
&lt;p&gt;为张量插入一个新的&lt;code&gt;size&lt;/code&gt;为1的维度。用于升维。配合&lt;code&gt;expand()&lt;/code&gt;食用更佳。&lt;/p&gt;
&lt;h3 id="6-expand"&gt;6. &lt;code&gt;expand()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;
&lt;a href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.expand.html#torch.Tensor.expand" target="_blank" rel="noopener noreferrer" &gt;torch.Tensor.expand&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;这个只有张量方法，没有函数。&lt;/p&gt;</description></item><item><title>用于搭建神经网络的函数</title><link>https://liubaoshuai1402.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch2/</link><pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate><guid>https://liubaoshuai1402.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch2/</guid><description>&lt;h1 id="用于搭建神经网络的函数"&gt;用于搭建神经网络的函数&lt;/h1&gt;
&lt;h2 id="前言"&gt;前言&lt;/h2&gt;
&lt;p&gt;此处记录常见的神经网络函数，排名不分先后。&lt;/p&gt;
&lt;h2 id="全连接层"&gt;全连接层&lt;/h2&gt;
&lt;h3 id="1-nnlinear"&gt;1. &lt;code&gt;nn.Linear()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;对输入数据施加一个仿射线性变换，一般使用指定两个参数，是输入矩阵和输出矩阵的最高维的&lt;code&gt;size&lt;/code&gt;。（因为最高维一般是特征维度，这个函数就是用来控制特征维度&lt;code&gt;size&lt;/code&gt;大小的）&lt;/p&gt;
&lt;img src="https://xiaoxiaobuaigugujiao.oss-cn-beijing.aliyuncs.com/img/%E5%9B%BE%E5%8D%B7%E7%A7%AF.png" style="zoom:50%;" /&gt;
&lt;p&gt;这里是CGCNN的图卷积操作公式，这里的W（权重），b（偏置）其实全连接层决定的。看到这样的写法就要明白其实是经历了一个全连接层。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;total_gated_fea = self.fc_full(total_nbr_fea)
 
total_gated_fea = self.bn1(total_gated_fea.view(
	-1, self.atom_fea_len*2)).view(N, M, self.atom_fea_len*2)
nbr_filter, nbr_core = total_gated_fea.chunk(2, dim=2)
nbr_filter = self.sigmoid(nbr_filter)
nbr_core = self.softplus1(nbr_core)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对应代码就是进过全连接层后，（再进行批标准化，chunk分割，这些公式里没有体现），然后施加$\sigma$函数和激活函数（g）。&lt;/p&gt;
&lt;h2 id="激活函数"&gt;激活函数&lt;/h2&gt;
&lt;h3 id="1-nnrelu"&gt;1. &lt;code&gt;nn.ReLU()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;这个激活函数不用指定输入输出特征的维度，它只是把所有特征变为非负，对于正值保留原始值，对于负值则转化为0。&lt;/p&gt;
&lt;img src="https://xiaoxiaobuaigugujiao.oss-cn-beijing.aliyuncs.com/img/ReLU.png"/&gt;
&lt;h3 id="2-nnsoftmax"&gt;2. &lt;code&gt;nn.Softmax()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;对一个n维的张量施加Softmax()函数，使得其沿某个维度的元素值的和为1。所以接受一个&lt;code&gt;dim&lt;/code&gt;参数来指定维度。
&lt;/p&gt;
$$
\mathrm{Softmax}(x_i)=\frac{\mathrm{exp}(x_i)}{\sum_{j}\mathrm{exp}(x_j)}
$$&lt;p&gt;
这里简单插入一下Pytorch中有关&lt;code&gt;dim&lt;/code&gt;的实践。&lt;/p&gt;
&lt;p&gt;比如说一个张量的size是(4,2,3)，那么他的dim=0指的是4，dim=1指的是2，dim=2指的是3。&lt;/p&gt;
&lt;p&gt;比如说对于这个张量，我有个和Pytorch相反的习惯，我习惯先看每行有多少个元素，是3。我就误以为它的dim=0对应的是3。&lt;/p&gt;
&lt;p&gt;其实不然，深度学习中，dim最大值对应维度的size，往往对应样本的特征数。&lt;/p&gt;
&lt;p&gt;一个简单的二维的深度学习的输入张量的size一般是这样的：(batch_size,features)。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;tensor([[[0.3299, 0.4336, 0.2365],
 [0.0695, 0.0668, 0.8638]],

 [[0.8114, 0.1116, 0.0770],
 [0.3142, 0.1086, 0.5772]],

 [[0.3178, 0.4508, 0.2315],
 [0.1620, 0.2610, 0.5770]],

 [[0.4454, 0.4082, 0.1464],
 [0.2974, 0.5297, 0.1729]]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="3-nnlogsoftmax"&gt;3. &lt;code&gt;nn.LogSoftmax()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;对一个n维的张量施加log(Softmax())函数，通常用于获取对数概率，并与损失函数&lt;code&gt;nn.NLLLoss()&lt;/code&gt;一起使用&lt;/p&gt;</description></item><item><title>花朵分类</title><link>https://liubaoshuai1402.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1/</link><pubDate>Sun, 18 May 2025 00:00:00 +0000</pubDate><guid>https://liubaoshuai1402.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1/</guid><description>&lt;h1 id="花朵分类"&gt;花朵分类&lt;/h1&gt;
&lt;h2 id="前言"&gt;前言&lt;/h2&gt;
&lt;p&gt;本文主要借助torchvision软件包，简单梳理一下深度学习代码的基本框架。&lt;/p&gt;
&lt;h2 id="数据集加载"&gt;数据集加载&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;#假设当前工作路径下，存放着一个名为'flower_data'的文件夹，里面存放着训练集和验证集的图片
#用os.path.join()来一级级得获取路径
data_dir = os.path.join(os.getcwd(),'flower_data')
train_dir = os.path.join(data_dir, 'train')
valid_dir = os.path.join(data_dir, 'valid')
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# Define batch size
batch_size = 32

# Define transforms for the training and validation sets
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
 std=[0.229, 0.224, 0.225])

#定义对数据预处理的组合，比如旋转图片、改变尺寸等等，让模型有更强的稳定性
train_data_transforms = transforms.Compose([
 transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),
 transforms.RandomRotation(degrees=15),
 transforms.ColorJitter(),
 transforms.RandomHorizontalFlip(),
 transforms.CenterCrop(size=224),
 transforms.ToTensor(),
 normalize,
 ])

validate_data_transforms = transforms.Compose([
 transforms.Resize(256),
 transforms.CenterCrop(224),
 transforms.ToTensor(),
 normalize,
 ])


#这里才真正的把图片加载成了二维数据。
train_dataset = datasets.ImageFolder(
 train_dir,
 train_data_transforms)

validate_dataset = datasets.ImageFolder(
 valid_dir,
 validate_data_transforms)



#做了那么多铺垫，其实就是为了把可用于训练的数据(二维数据)放到 DataLoader 里面
train_loader = torch.utils.data.DataLoader(
 train_dataset, batch_size=batch_size, shuffle=True,
 num_workers=4)

validate_loader = torch.utils.data.DataLoader(
 validate_dataset, batch_size=batch_size, shuffle=True,
 num_workers=4)

data_loader = {}
data_loader['train'] = train_loader
data_loader['valid'] = validate_loader
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;batch_size&lt;/code&gt;，当训练数据很多时，一次性加载全部数据进行训练会是一种挑战。这时就需要用到批次训练。&lt;code&gt;batch_size&lt;/code&gt;即是一次训练中使用的数据量。注意这里的一次训练，不是指一epoch。只有遍历所有训练集后，才能叫做完成了一代训练。一代训练包含了诸多这样的一次训练。&lt;/p&gt;</description></item></channel></rss>