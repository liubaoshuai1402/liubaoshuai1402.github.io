<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习s on BOGHTW</title>
    <link>http://localhost:1313/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 深度学习s on BOGHTW</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 19 May 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>用于搭建神经网络的函数</title>
      <link>http://localhost:1313/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch2/</link>
      <pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch2/</guid>
      <description>&lt;h1 id=&#34;用于搭建神经网络的函数&#34;&gt;用于搭建神经网络的函数&lt;/h1&gt;&#xA;&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;&#xA;&lt;p&gt;此处记录常见的神经网络函数，排名不分先后。&lt;/p&gt;&#xA;&lt;h2 id=&#34;激活函数&#34;&gt;激活函数&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-nnrelu&#34;&gt;1. &lt;code&gt;nn.ReLU()&lt;/code&gt;&lt;/h3&gt;&#xA;&lt;p&gt;这个激活函数不用指定输入输出特征的维度，它只是把所有特征变为非负，对于正值保留原始值，对于负值则转化为0。&lt;/p&gt;&#xA;&lt;img src=&#34;https://xiaoxiaobuaigugujiao.oss-cn-beijing.aliyuncs.com/img/ReLU.png&#34;/&gt;&#xD;&#xA;&lt;h3 id=&#34;2-nnsoftmax&#34;&gt;2. &lt;code&gt;nn.Softmax()&lt;/code&gt;&lt;/h3&gt;&#xA;&lt;p&gt;对一个n维的张量施加Softmax()函数，使得其沿某个维度的元素值的和为1。所以接受一个&lt;code&gt;dim&lt;/code&gt;参数来指定维度。&#xA;&lt;/p&gt;&#xA;$$&#xD;&#xA;\mathrm{Softmax}(x_i)=\frac{\mathrm{exp}(x_i)}{\sum_{j}\mathrm{exp}(x_j)}&#xD;&#xA;$$&lt;p&gt;&#xA;这里简单插入一下Pytorch中有关&lt;code&gt;dim&lt;/code&gt;的实践。&lt;/p&gt;&#xA;&lt;p&gt;比如说一个张量的size是(4,2,3)，那么他的dim=0指的是4，dim=1指的是2，dim=2指的是3。&lt;/p&gt;&#xA;&lt;p&gt;比如说对于这个张量，我有个和Pytorch相反的习惯，我习惯先看每行有多少个元素，是3。我就误以为它的dim=0对应的是3。&lt;/p&gt;&#xA;&lt;p&gt;其实不然，深度学习中，dim最大值对应维度的size，往往对应样本的特征数。&lt;/p&gt;&#xA;&lt;p&gt;一个简单的二维的深度学习的输入张量的size一般是这样的：(batch_size,features)。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tensor([[[0.3299, 0.4336, 0.2365],&#xD;&#xA;         [0.0695, 0.0668, 0.8638]],&#xD;&#xA;&#xD;&#xA;        [[0.8114, 0.1116, 0.0770],&#xD;&#xA;         [0.3142, 0.1086, 0.5772]],&#xD;&#xA;&#xD;&#xA;        [[0.3178, 0.4508, 0.2315],&#xD;&#xA;         [0.1620, 0.2610, 0.5770]],&#xD;&#xA;&#xD;&#xA;        [[0.4454, 0.4082, 0.1464],&#xD;&#xA;         [0.2974, 0.5297, 0.1729]]])&#xD;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h3 id=&#34;3-nnlogsoftmax&#34;&gt;3. &lt;code&gt;nn.LogSoftmax()&lt;/code&gt;&lt;/h3&gt;&#xA;&lt;p&gt;对一个n维的张量施加log(Softmax())函数，通常用于获取对数概率，并与损失函数&lt;code&gt;nn.NLLLoss()&lt;/code&gt;一起使用&lt;/p&gt;&#xA;&lt;h3 id=&#34;4-nnsoftmax2d&#34;&gt;4. &lt;code&gt;nn.Softmax2d()&lt;/code&gt;&lt;/h3&gt;&#xA;&lt;p&gt;针对3维(C,H,W)或4维(N,C,H,W)的输入，它总是沿着C对应的维度对张量施加Softmax()函数&lt;/p&gt;&#xA;&lt;h2 id=&#34;批标准化&#34;&gt;批标准化&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-nnbatchnorm1d&#34;&gt;1. &lt;code&gt;nn.BatchNorm1d()&lt;/code&gt;&lt;/h3&gt;&#xA;&lt;p&gt;对二维(N,C)或三维(N,C,L)的输入进行批标准化。&lt;/p&gt;&#xA;&lt;p&gt;对于二维输入，其实N是batch大小，C是特征数，这里叫Channels，是沿着列求平均和方差的。并且这个函数求的是有偏方差。&lt;/p&gt;&#xA;&lt;p&gt;对于三维输入，可以理解成先转化为(N*L,C)，再用二维的处理方法得到结果，再展开。&lt;/p&gt;</description>
    </item>
    <item>
      <title>花朵分类</title>
      <link>http://localhost:1313/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1/</link>
      <pubDate>Sun, 18 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1/</guid>
      <description>&lt;h1 id=&#34;花朵分类&#34;&gt;花朵分类&lt;/h1&gt;&#xA;&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;&#xA;&lt;p&gt;本文主要借助torchvision软件包，简单梳理一下深度学习代码的基本框架。&lt;/p&gt;&#xA;&lt;h2 id=&#34;数据集加载&#34;&gt;数据集加载&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#假设当前工作路径下，存放着一个名为&#39;flower_data&#39;的文件夹，里面存放着训练集和验证集的图片&#xD;&#xA;#用os.path.join()来一级级得获取路径&#xD;&#xA;data_dir = os.path.join(os.getcwd(),&#39;flower_data&#39;)&#xD;&#xA;train_dir = os.path.join(data_dir, &#39;train&#39;)&#xD;&#xA;valid_dir = os.path.join(data_dir, &#39;valid&#39;)&#xD;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define batch size&#xD;&#xA;batch_size = 32&#xD;&#xA;&#xD;&#xA;# Define transforms for the training and validation sets&#xD;&#xA;normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],&#xD;&#xA;                                 std=[0.229, 0.224, 0.225])&#xD;&#xA;&#xD;&#xA;#定义对数据预处理的组合，比如旋转图片、改变尺寸等等，让模型有更强的稳定性&#xD;&#xA;train_data_transforms = transforms.Compose([&#xD;&#xA;        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),&#xD;&#xA;        transforms.RandomRotation(degrees=15),&#xD;&#xA;        transforms.ColorJitter(),&#xD;&#xA;        transforms.RandomHorizontalFlip(),&#xD;&#xA;        transforms.CenterCrop(size=224),&#xD;&#xA;        transforms.ToTensor(),&#xD;&#xA;        normalize,&#xD;&#xA;    ])&#xD;&#xA;&#xD;&#xA;validate_data_transforms = transforms.Compose([&#xD;&#xA;        transforms.Resize(256),&#xD;&#xA;        transforms.CenterCrop(224),&#xD;&#xA;        transforms.ToTensor(),&#xD;&#xA;        normalize,&#xD;&#xA;    ])&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;#这里才真正的把图片加载成了二维数据。&#xD;&#xA;train_dataset = datasets.ImageFolder(&#xD;&#xA;    train_dir,&#xD;&#xA;    train_data_transforms)&#xD;&#xA;&#xD;&#xA;validate_dataset = datasets.ImageFolder(&#xD;&#xA;    valid_dir,&#xD;&#xA;    validate_data_transforms)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;#做了那么多铺垫，其实就是为了把可用于训练的数据(二维数据)放到 DataLoader 里面&#xD;&#xA;train_loader = torch.utils.data.DataLoader(&#xD;&#xA;    train_dataset, batch_size=batch_size, shuffle=True,&#xD;&#xA;    num_workers=4)&#xD;&#xA;&#xD;&#xA;validate_loader = torch.utils.data.DataLoader(&#xD;&#xA;    validate_dataset, batch_size=batch_size, shuffle=True,&#xD;&#xA;    num_workers=4)&#xD;&#xA;&#xD;&#xA;data_loader = {}&#xD;&#xA;data_loader[&#39;train&#39;] = train_loader&#xD;&#xA;data_loader[&#39;valid&#39;] = validate_loader&#xD;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;batch_size&lt;/code&gt;，当训练数据很多时，一次性加载全部数据进行训练会是一种挑战。这时就需要用到批次训练。&lt;code&gt;batch_size&lt;/code&gt;即是一次训练中使用的数据量。注意这里的一次训练，不是指一epoch。只有遍历所有训练集后，才能叫做完成了一代训练。一代训练包含了诸多这样的一次训练。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
