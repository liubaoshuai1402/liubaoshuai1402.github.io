
<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> | BOGHTW</title>
    <link rel="icon" href= /images/favicon.png />
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">主页</a></li>
      
      <li><a href="/series/">专栏</a></li>
      
      <li><a href="/categories/">分类</a></li>
      
      <li><a href="/tags/">标签</a></li>
      
      <li><a href="/archives/">归档</a></li>
      
      <li><a href="/search/">搜索</a></li>
      
      <li><a href="/about/">关于</a></li>
      
    </ul>
    <hr/>
    </nav>

<p style="margin: 1.8rem 0 -1rem 0; color: #191919;">可以根据标题、分类、标签、系列等条目检索本站文章</p>

<div class="search-container">
  <input type="text" id="search-input" placeholder="输入关键词搜索..." aria-label="搜索框" autocomplete="off">
  <div id="search-debug" style="margin-bottom: 10px; color: #999;"></div>
  <div id="search-results"></div>
</div>


<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.6.2"></script>


<script>
  
  function debug(text) {
    document.getElementById('search-debug').textContent = text;
  }

  
  let searchIndex = [];

  
  
  
    
  

  debug("索引初始化中...");

  
    searchIndex.push({
      title: "\"晶界建模\"",
      permalink: "\"/pymatgen/gainbroundary/\"",
      content: "\"晶界建模 前言 基于 共格点阵模型 （Coincidence Site Lattice），使用pymatgen进行晶界建模。\\n晶界的类型 1. 扭转（twist）晶界 旋转轴垂直于晶界面，也就是两者的密勒指数应该成比列。\\n2. 倾斜（tilt）晶界 旋转轴平行于晶界面，也就是两者的密勒指数的点乘为0。\\n3. 混合（mixed）晶界 旋转轴既不垂直也不平行于晶界面。\\n基于共格点阵模型的晶界命名法，Σ+number(must be odd)+(hkl)/[uvw]。\\n举一个栗子， Σ13 (510)/[001] symmetric tilt grain-boundary。\\n这里Σ的大小，是指旋转后重合点阵的单胞的提及是原始晶体单胞的体积的多少倍。通常，这个数字越大，代表两个晶粒的取向相差越远，晶界能也往往越远。\\nΣ1则代表趋向一致，那些小角度晶界也被认为Σ的值近似于1。\\n当指明是twist or tilt晶界时，有时可以省略晶向，也不会造成歧义，比如，the Σ5(310) tilt GB，这是一个YSZ中典型的低能量晶界。\\n但它没有给出旋转轴，因为没有必要，tilt GB 要求晶界面与旋转轴平行，所以只能是[001]。\\n使用pymatgen进行晶界建模 首先，我们假设一个应用场景，就是说，我们建模肯定是根据实验来的，实验上对哪些晶界感兴趣，我们就去建模研究。\\n所以在这个假设的基础上，我们是知道Σ的值以及旋转轴、晶界面的。\\n这样，用以下代码我们可以得到旋转角。\\nfrom pymatgen.core import Structure\\rfrom pymatgen.core.interface import GrainBoundaryGenerator\\r# 1. 读取结构文件\\rstructure = Structure.from_file(\\u0026quot;ZrO.cif\\u0026quot;)\\rstructure = structure.to_conventional()\\r#创建一个晶界生成器，实例化需要一个晶体结构，最好是conventional cell。\\rgb_gen = GrainBoundaryGenerator(structure)\\r# 2. 构建 Σ5 晶界，参数分别对应Σ的值、旋转轴、晶格类型，对非立方体系需要指定轴比\\r#其实这里感觉很奇怪，轴比和晶格类型，pymatgen不应该自己判断吗，感觉这块代码写的不好\\rrotation_anglen = gb_gen.get_rotation_angle_from_sigma(5,(0,0,1),lat_type='c')\\rprint(rotation_anglen)\\r这里的输出是，[36.86989764584402, 53.13010235415597, 126.86989764584402, 143.13010235415598]。\\n第一个值，36.8°与[参考文献]( Sci-Hub | Structure and Chemistry of Yttria-Stabilized Cubic-Zirconia Symmetric Tilt Grain Boundaries. Journal of the American Ceramic Society, 84(6), 1361–1368 | 10.1111/j.1151-2916.2001.tb00842.x )一致。\\n同样的，如果我们用rotation_anglen = gb_gen.get_rotation_angle_from_sigma(13,(0,0,1),lat_type='c')\\n则会得到输出，[22.61986494804043, 67.38013505195957, 112.61986494804043, 157.3801350519596]，第一个值与24°差1.4。\\n#指定旋转轴，旋转角，晶界面\\rgb = gb_gen.gb_from_parameters(\\rrotation_axis=(0,0,1),\\rrotation_angle=rotation_anglen[0],\\rexpand_times=1,\\rvacuum_thickness=0,\\rplane=(3,1,0)\\r)\\r# 3. 获取晶界结构\\rgb.to(\\u0026quot;POSCAR\\u0026quot;, \\u0026quot;poscar\\u0026quot;)\\r再写上这块代码就可以生成晶界了。\\n\"",
      categories: "[\"pymatgen\"]",
      tags: [],
      series: [],
      date: "\"2025-05-24\""
    });
  
    searchIndex.push({
      title: "\"Pytorch中的张量操作\"",
      permalink: "\"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C/\"",
      content: "\"Pytorch中的张量操作 数学符号 1. * 运算符 如果张量的形状完全一样，张量 * 张量，其实就是对应元素相乘，也可以$\\\\odot$表示。\\n如果张量的形状不一样，则要求他们的形状只能在一个维度上不一样，且其中一个张量这个维度的size为1，这样可以用广播机制补全后相乘。\\n如果张量 * 数字，则是全体元素乘以这个数字。\\n函数、方法 1. permute() 可以作为张量对象的方法使用，接受置换后的维度。\\nimport torch\\rtable = torch.randn(2,3,4)\\rprint(table)\\rprint(table.permute(2,1,0))\\r这段代码就是将第一维和第三维进行置换。\\n由已知的张量还有置换后的顺序，怎么写出新的张量呢，这个问题一直让我困扰。\\n今天的学习，我脑子里突然蹦出一个邻居的概念。对于一个四维的张量元素，它应该有四个不同维度的邻居，但如果仅仅把视角局限在矩阵里，那它就只有两个邻居，一个行维度上的，一个列维度上的，这很不对。得把视野打开，通道维度和批次维度的邻居可能隔了很远，但它们也是邻居。\\n2. reshape()和view() 深度学习中，经常会把高维张量降维，计算后再展开，要小心翼翼，错位会带来灾难性的错误。\\n对于一个（N，F，M）的张量，F是其特征维度。如果想把N和M合并，应该先把F变成最高维度或者最低维度，让需要合并的维度接壤。\\nimport torch\\rtable = torch.randn(2,3,4)\\rprint(table)\\ra = table.reshape(3,8)\\rb = table.permute(1,0,2).reshape(3,8)\\rprint(a)\\rprint(b)\\r这里先把张量形状变成了（3，2，4），再合并，可以保证张量维度不会错位，如果直接改写性质为（3，8），则会错位。\\nreshape()和view()是一样的，前者可以作用于numpy数组。\\n3. flatten() 将高维张量展平为一维张量。\\n4. torch.cat() 把张量按给定的顺序和维度进行拼接，除了拼接维度外，张量必须具有相同的形状。\\n可用于特征维度的拓展，加入新的特征。\\n5. unsqueeze() torch.unsqueeze torch.unsqueeze(input, dim) → Tensor\\n为张量插入一个新的size为1的维度。用于升维。配合expand()食用更佳。\\n6. expand() torch.Tensor.expand 这个只有张量方法，没有函数。\\nTensor.expand(*sizes) → Tensor\\n将张量沿着某个size为1的维度进行复制拓展。\\ntotal_nbr_fea = torch.cat(\\r[atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len),\\ratom_nbr_fea, nbr_fea], dim=2)\\ratom_in_fea张量的形状本是(N,self.atom_fea_len)，通过unsqueeze(1)，变成(N,1,self.atom_fea_len)。\\n再通过expand(N, M, self.atom_fea_len),变成(N, M, self.atom_fea_len)形状。\\n7. chunk() torch.chunk torch.chunk(input: Tensor , chunks: int , dim: int = 0) → Tuple[Tensor, \\u0026hellip;]\\n尝试把张量分成指定数量的块。也就是分割张量。可指定维度分割。但，数量可能会比指定的少，一般最好成倍数时使用。\\n可用于分割特征维度。\\n8. torch.sum() torch.sum torch.sum(input, dim, keepdim=False, ***, dtype=None) → Tensor\\n将张量元素沿指定维度进行加和。\\n切片 1. 简单切片 import torch\\ra = torch.tensor(range(12)).view(3,4)\\rb = a[1,2]\\rprint(a)\\rprint(b)\\r在张量切片中，逗号表示维度间隔。在这个简单切片中，整数代表索引。\\n也可以接受start:stop:step格式的索引进行切片，一个简单:代表全选这一维度。\\nimport torch\\ra = torch.tensor(range(24)).view(2,3,4)\\rb = a[1,:]\\rprint(a)\\rprint(b)\\r这里是一个三维张量，但是只用了一个逗号，这是一种简写，其实是逗号后续的所有维度都全选。\\n2. 高级切片 张量的某个维度的索引也可以是张量。\\n如果索引是一维张量，据测试，其实是列表也行。就是多个整数索引组成的列表或者一维张量。根据这些整数确定这个维度怎么被取。\\nimport torch\\rimport numpy as np\\ratom_in_fea = torch.randn(4,5,6)\\rnbr_fea_idx = torch.tensor([0, 1, 4])\\ratom_nbr_fea1 = atom_in_fea[:,nbr_fea_idx,:]\\ratom_nbr_fea2 = atom_in_fea[:,[0,1,4],:]\\rprint(atom_nbr_fea1==atom_nbr_fea2)\\r这里dim=1维度的切片索引就是一个一维张量，里面的整数是0，1，4。说明切片时，取这个维度的第1，第2，第5个数据。\\n如果索引是二维张量，在切片的同时其实进行了升维。\\nimport torch\\rimport numpy as np\\ratom_in_fea = torch.randn(4,5,6)\\r#shape(2,2)\\rnbr_fea_idx = torch.tensor([[0, 1],[1,2]])\\ratom_nbr_fea = atom_in_fea[:,nbr_fea_idx,:]\\rprint(atom_nbr_fea.shape)\\r#torch.Size([4, 2, 2, 6])\\r这里dim=1维度的切片索引就是一个二维张量，它会按照内部每一行的整数对这个维度进行切片，并根据内部的行数形成新的一维。\\n听起来很复杂，这里给出一个具体的应用场景，在CGCNN的卷积操作中。\\natom_in_fea的性质为（N，features），N是批次内的总原子数\\nnbr_fea_idx是一个二维张量（N，M），记录了N个原子的邻居原子的原子id。\\natom_nbr_fea = atom_in_fea[nbr_fea_idx, :]\\r从形状上来看，这样的切片结果就是把（N，features）中的N替换成（N，M），最终变成（N，M，features）\\n这样，就得到了，每个原子的M个邻居原子的原子特征。\\n这里nbr_fea_idx的记录很关键，CIFData给出的是每个晶体内的，collate_pool函数会把它更新成batch内的。模型接受的参数是经collate_pool函数集成后的。\\n\"",
      categories: "[\"深度学习\"]",
      tags: [],
      series: [],
      date: "\"2025-05-20\""
    });
  
    searchIndex.push({
      title: "\"用于搭建神经网络的函数\"",
      permalink: "\"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch2/\"",
      content: "\"用于搭建神经网络的函数 前言 此处记录常见的神经网络函数，排名不分先后。\\n全连接层 1. nn.Linear() 对输入数据施加一个仿射线性变换，一般使用指定两个参数，是输入矩阵和输出矩阵的最高维的size。（因为最高维一般是特征维度，这个函数就是用来控制特征维度size大小的）\\n这里是CGCNN的图卷积操作公式，这里的W（权重），b（偏置）其实全连接层决定的。看到这样的写法就要明白其实是经历了一个全连接层。\\ntotal_gated_fea = self.fc_full(total_nbr_fea)\\rtotal_gated_fea = self.bn1(total_gated_fea.view(\\r-1, self.atom_fea_len*2)).view(N, M, self.atom_fea_len*2)\\rnbr_filter, nbr_core = total_gated_fea.chunk(2, dim=2)\\rnbr_filter = self.sigmoid(nbr_filter)\\rnbr_core = self.softplus1(nbr_core)\\r对应代码就是进过全连接层后，（再进行批标准化，chunk分割，这些公式里没有体现），然后施加$\\\\sigma$函数和激活函数（g）。\\n激活函数 1. nn.ReLU() 这个激活函数不用指定输入输出特征的维度，它只是把所有特征变为非负，对于正值保留原始值，对于负值则转化为0。\\n2. nn.Softmax() 对一个n维的张量施加Softmax()函数，使得其沿某个维度的元素值的和为1。所以接受一个dim参数来指定维度。 $$\\r\\\\mathrm{Softmax}(x_i)=\\\\frac{\\\\mathrm{exp}(x_i)}{\\\\sum_{j}\\\\mathrm{exp}(x_j)}\\r$$ 这里简单插入一下Pytorch中有关dim的实践。\\n比如说一个张量的size是(4,2,3)，那么他的dim=0指的是4，dim=1指的是2，dim=2指的是3。\\n比如说对于这个张量，我有个和Pytorch相反的习惯，我习惯先看每行有多少个元素，是3。我就误以为它的dim=0对应的是3。\\n其实不然，深度学习中，dim最大值对应维度的size，往往对应样本的特征数。\\n一个简单的二维的深度学习的输入张量的size一般是这样的：(batch_size,features)。\\ntensor([[[0.3299, 0.4336, 0.2365],\\r[0.0695, 0.0668, 0.8638]],\\r[[0.8114, 0.1116, 0.0770],\\r[0.3142, 0.1086, 0.5772]],\\r[[0.3178, 0.4508, 0.2315],\\r[0.1620, 0.2610, 0.5770]],\\r[[0.4454, 0.4082, 0.1464],\\r[0.2974, 0.5297, 0.1729]]])\\r3. nn.LogSoftmax() 对一个n维的张量施加log(Softmax())函数，通常用于获取对数概率，并与损失函数nn.NLLLoss()一起使用\\n4. nn.Softmax2d() 针对3维(C,H,W)或4维(N,C,H,W)的输入，它总是沿着C对应的维度对张量施加Softmax()函数\\n5. nn.Softplus() 对每个张量元素施加 Softplus 函数。这是一个ReLU函数的光滑近似，用于保证正的输出。\\n默认$\\\\beta$值为1，当输入值*$\\\\beta$大于一定门槛（默认值20）时，会变成线性。\\n$\\\\mathrm{Softplus}(x)=\\\\frac{1}{\\\\beta}\\\\log(1+\\\\exp(\\\\betax))$\\n6. nn.Sigmoid() 对张量的每个元素施加Sigmoid函数。对于小于0的输入，输出0-0.5；对于大于0的输入，输出0.5-1。整体是非负的。\\n$\\\\mathrm{Sigmoid}(x)=\\\\sigma(x)=\\\\frac{1}{1+\\\\exp(-x)}$\\n7. nn.Tanh 对张量的每个元素施加双曲正切函数，输出值范围为(-1,1)。\\n$\\\\mathrm{Tanh}(x)=\\\\mathrm{tanh}(x)=\\\\frac{\\\\exp(x)-\\\\exp(-x)}{\\\\exp(x)+\\\\exp(-x)}$\\n批标准化 1. nn.BatchNorm1d() 对二维(N,C)或三维(N,C,L)的输入进行批标准化。\\n对于二维输入，其实N是batch大小，C是特征数，这里叫Channels，是沿着列求平均和方差的。并且这个函数求的是有偏方差。\\n对于三维输入，可以理解成先转化为(N*L,C)，再用二维的处理方法得到结果，再展开。\\n池化 正则化 参考文献 在深度学习中，正则化通常用于约束模型的复杂度、防止过拟合、提高模型的泛化能力和鲁棒性。\\n1. nn.Dropout() 在每次前向传播时，以一定概率让张量中的某些元素归零。防止过拟合。常用于分类任务。不要用于回归任务。\\n接受一个浮点数，用于表示概率，默认值是0.5。\\n2. L1正则化 L1正则化、也称Lasso正则化。就是在损失函数中引入一个与模型权重的L1范数相关的值，作为惩罚项，用于控制（限制）模型的复杂度和防止过拟合。\\n$L_{L1}=L_{data}+\\\\lambda\\\\begin{matrix} \\\\sum_{i=1}^n |w_i| \\\\end{matrix}$\\n3. L2正则化 L2正则化，也称Ridge正则化、权重衰减。与L1正则化类似，就是在损失函数中引入一个与模型权重的L2范数相关的值，作为惩罚项，用于控制（限制）模型的复杂度和防止过拟合。它鼓励模型用小的模型参数。\\n$L_{L1}=L_{data}+\\\\lambda||w||_2^2$\\n这里，$||w||2^2=\\\\begin{matrix} \\\\sum{i=1}n w_i2 \\\\end{matrix}$\\n4. Elastic Net 正则化 采用L1正则化和L2正则化的组合。\\n5. 早停止和数据增强 早停止：即检测模型在验证集上的性能，当模型在验证集的性能出现下降时停止训练，避免过拟合。\\n数据增强：对数据进行一定变换来增强数据的多样性。如在图像分类时，对图片进行旋转、剪裁、翻转等操作。\\n\"",
      categories: "[\"深度学习\"]",
      tags: [],
      series: [],
      date: "\"2025-05-19\""
    });
  
    searchIndex.push({
      title: "\"Python中的文件操作\"",
      permalink: "\"/%E4%BB%A3%E7%A0%81/fileoperations/\"",
      content: "\"Python中的文件操作 1.获取当前路径 os.getcwd()可以返回当前工作目录的绝对路径。cwd可以理解成current work directory。\\n\"",
      categories: "[\"代码\"]",
      tags: [],
      series: [],
      date: "\"2025-05-18\""
    });
  
    searchIndex.push({
      title: "\"花朵分类\"",
      permalink: "\"/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch1/\"",
      content: "\"花朵分类 前言 本文主要借助torchvision软件包，简单梳理一下深度学习代码的基本框架。\\n数据集加载 #假设当前工作路径下，存放着一个名为'flower_data'的文件夹，里面存放着训练集和验证集的图片\\r#用os.path.join()来一级级得获取路径\\rdata_dir = os.path.join(os.getcwd(),'flower_data')\\rtrain_dir = os.path.join(data_dir, 'train')\\rvalid_dir = os.path.join(data_dir, 'valid')\\r# Define batch size\\rbatch_size = 32\\r# Define transforms for the training and validation sets\\rnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\\rstd=[0.229, 0.224, 0.225])\\r#定义对数据预处理的组合，比如旋转图片、改变尺寸等等，让模型有更强的稳定性\\rtrain_data_transforms = transforms.Compose([\\rtransforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\\rtransforms.RandomRotation(degrees=15),\\rtransforms.ColorJitter(),\\rtransforms.RandomHorizontalFlip(),\\rtransforms.CenterCrop(size=224),\\rtransforms.ToTensor(),\\rnormalize,\\r])\\rvalidate_data_transforms = transforms.Compose([\\rtransforms.Resize(256),\\rtransforms.CenterCrop(224),\\rtransforms.ToTensor(),\\rnormalize,\\r])\\r#这里才真正的把图片加载成了二维数据。\\rtrain_dataset = datasets.ImageFolder(\\rtrain_dir,\\rtrain_data_transforms)\\rvalidate_dataset = datasets.ImageFolder(\\rvalid_dir,\\rvalidate_data_transforms)\\r#做了那么多铺垫，其实就是为了把可用于训练的数据(二维数据)放到 DataLoader 里面\\rtrain_loader = torch.utils.data.DataLoader(\\rtrain_dataset, batch_size=batch_size, shuffle=True,\\rnum_workers=4)\\rvalidate_loader = torch.utils.data.DataLoader(\\rvalidate_dataset, batch_size=batch_size, shuffle=True,\\rnum_workers=4)\\rdata_loader = {}\\rdata_loader['train'] = train_loader\\rdata_loader['valid'] = validate_loader\\rbatch_size，当训练数据很多时，一次性加载全部数据进行训练会是一种挑战。这时就需要用到批次训练。batch_size即是一次训练中使用的数据量。注意这里的一次训练，不是指一epoch。只有遍历所有训练集后，才能叫做完成了一代训练。一代训练包含了诸多这样的一次训练。\\nshuffle参数为True时，随机采样。\\n这是CGCNN（晶体卷积图神经网络）的代码中，用于得到训练集、验证集、测试集的DateLoader。\\ndef get_train_val_test_loader(dataset, collate_fn=default_collate,\\rbatch_size=64, train_ratio=None,\\rval_ratio=0.1, test_ratio=0.1, return_test=False,\\rnum_workers=1, pin_memory=False, **kwargs):\\rtotal_size = len(dataset)\\rif kwargs['train_size'] is None:\\rif train_ratio is None:\\rassert val_ratio + test_ratio \\u0026lt; 1\\rtrain_ratio = 1 - val_ratio - test_ratio\\rprint(f'[Warning] train_ratio is None, using 1 - val_ratio - '\\rf'test_ratio = {train_ratio} as training data.')\\relse:\\rassert train_ratio + val_ratio + test_ratio \\u0026lt;= 1\\rindices = list(range(total_size))\\rif kwargs['train_size']:\\rtrain_size = kwargs['train_size']\\relse:\\rtrain_size = int(train_ratio * total_size)\\rif kwargs['test_size']:\\rtest_size = kwargs['test_size']\\relse:\\rtest_size = int(test_ratio * total_size)\\rif kwargs['val_size']:\\rvalid_size = kwargs['val_size']\\relse:\\rvalid_size = int(val_ratio * total_size)\\rtrain_sampler = SubsetRandomSampler(indices[:train_size])\\rval_sampler = SubsetRandomSampler(\\rindices[-(valid_size + test_size):-test_size])\\rif return_test:\\rtest_sampler = SubsetRandomSampler(indices[-test_size:])\\rtrain_loader = DataLoader(dataset, batch_size=batch_size,\\rsampler=train_sampler,\\rnum_workers=num_workers,\\rcollate_fn=collate_fn, pin_memory=pin_memory)\\rval_loader = DataLoader(dataset, batch_size=batch_size,\\rsampler=val_sampler,\\rnum_workers=num_workers,\\rcollate_fn=collate_fn, pin_memory=pin_memory)\\rif return_test:\\rtest_loader = DataLoader(dataset, batch_size=batch_size,\\rsampler=test_sampler,\\rnum_workers=num_workers,\\rcollate_fn=collate_fn, pin_memory=pin_memory)\\rif return_test:\\rreturn train_loader, val_loader, test_loader\\relse:\\rreturn train_loader, val_loader\\r这段代码整体分为两部分，其一是根据参数确定训练集、验证集、测试集的数量，其二是装配DataLoader。和花朵分类的代码稍有不同，这样的实现可以让用户自由决定训练集、验证集和测试集。多使用了DataLoader的sampler参数。\\nSubsetRandomSampler是torch.utils.data.sampler模块中的六个类之一。用于随机采样。\\n这里是常规用法，假设有100个数据，80个用于训练集，15个用于验证集，5个用于测试集。\\n那么，\\n训练集的SubsetRandomSampler的初始化参数就是list(range(100))[0:80]。\\n验证集的SubsetRandomSampler的初始化参数就是list(range(100))[-20:-5]。\\n测试集的SubsetRandomSampler的初始化参数就是list(range(100))[-5:]。\\n1. Dataset与DataLoader 在CGCNN中，作者自己手写了CIFData作为自定义的Dataset。\\nCIFData三个重要的魔法方法的功能：\\n1.__init__()方法\\ndef __init__(self, root_dir, max_num_nbr=12, radius=8, dmin=0, step=0.2,\\rrandom_seed=123):\\r这里简单初始化了：数据集的路径、晶体的解析细节（比如最大邻居数和截断半径，高斯距离的参数）\\n2.__len__()方法\\n让CIFData的实例可以通过len()函数返回数据集大小。\\n3.__getitem__()方法\\n这里先用了@functools.lru_cache(maxsize=None)修饰器来缓存数据，避免每次读入相同结构时，都要解析晶体。\\nDataLoader中的collate_fn打包函数\\nCGCNN代码中自定义了打包函数，名叫collate_pool()\\nDataLoader实例在初始化同样只是规定了一些加载参数，并没有实际加载数据集。\\n只有当遍历DataLoader实例时，才开始加载、打包、返回数据。(for循环抽打DataLoader，DataLoader抽打Dataset)\\n但是遍历DataLoader并不会得到一个个的数据点，而是得到一个个batch的数据包，这是由collate_fn打包函数完成的，每当经历了batch_size个数据点，打包函数就会把他们合并。\\n这里是打包函数返回的内容\\nreturn (torch.cat(batch_atom_fea, dim=0),\\rtorch.cat(batch_nbr_fea, dim=0),\\rtorch.cat(batch_nbr_fea_idx, dim=0),\\rcrystal_atom_idx),\\\\\\rtorch.stack(batch_target, dim=0),\\\\\\rbatch_cif_ids\\r以及for循环的例子， for i, (input, target, _) in enumerate(train_loader):\\n得到的input其实是第一个返回，即一个元组，包含了经过拼接后的同一批次内的原子特征、邻居特征、以及邻居特征索引和全局原子索引。\\n写神经网络 model_input = 'resnet152'\\r# Build and train network\\rif model_input == 'vgg16':\\r# Build and train network\\rmodel = models.vgg16(pretrained=True)\\relif model_input == 'vgg19':\\r# Build and train network\\rmodel = models.vgg19(pretrained=True)\\relif model_input == 'resnet152':\\rmodel = models.resnet152(pretrained=True)\\r# Freeze training for all layers\\rfor param in model.parameters():\\rparam.requires_grad = False\\r# # Newly created modules have require_grad=True by default\\rif 'vgg' in model_input:\\rnum_features = model.classifier[-1].in_features\\rmodel.classifier[6] = nn.Sequential(\\rnn.Linear(num_features, 512), nn.ReLU(), nn.Linear(512, len(cat_to_name)),\\rnn.LogSoftmax(dim=1))\\relif 'resnet' in model_input:\\rnum_features = model.fc.in_features\\rmodel.fc = nn.Sequential(\\rnn.Linear(num_features, 512), nn.ReLU(), nn.BatchNorm1d(512),\\rnn.Dropout(0.4),\\rnn.Linear(512, len(cat_to_name)),\\rnn.LogSoftmax(dim=1))\\rprint(model.__class__.__name__)\\r# check to see that your last layer produces the expected number of outputs\\r# print(model.classifier[-1].out_features)\\r根据参数选择预训练模型并冻结模型参数，然后修改输出层。\\n对于vgg模型，\\nself.classifier = nn.Sequential(\\rnn.Linear(512 * 7 * 7, 4096),\\rnn.ReLU(True),\\rnn.Dropout(p=dropout),\\rnn.Linear(4096, 4096),\\rnn.ReLU(True),\\rnn.Dropout(p=dropout),\\rnn.Linear(4096, num_classes),\\r)\\r他的分类器本来包含了7层，但是作者觉得最后一层的4096个特征太多了，就改写了一下，减少特征为512个。\\ntrain()函数 让我们看看一个合格的训练函数都应该做哪些工作。\\ndef train(train_loader, model, criterion, optimizer, epoch, normalizer):\\r#每一代epoch都调用一次train函数\\rbatch_time = AverageMeter()\\rdata_time = AverageMeter()\\rlosses = AverageMeter()\\rif args.task == 'regression':\\rmae_errors = AverageMeter()\\relse:\\raccuracies = AverageMeter()\\rprecisions = AverageMeter()\\rrecalls = AverageMeter()\\rfscores = AverageMeter()\\rauc_scores = AverageMeter()\\r# switch to train mode\\rmodel.train()\\rend = time.time()\\r#一个训练函数应该要遍历整个训练集\\rfor i, (input, target, _) in enumerate(train_loader):\\r#遍历train_loader，其实是一个个的批次\\r# measure data loading time\\rdata_time.update(time.time() - end)\\rif args.cuda:\\rinput_var = (Variable(input[0].cuda(non_blocking=True)),\\rVariable(input[1].cuda(non_blocking=True)),\\rinput[2].cuda(non_blocking=True),\\r[crys_idx.cuda(non_blocking=True) for crys_idx in input[3]])\\relse:\\rinput_var = (Variable(input[0]),\\rVariable(input[1]),\\rinput[2],\\rinput[3])\\r# normalize target\\rif args.task == 'regression':\\rtarget_normed = normalizer.norm(target)\\relse:\\rtarget_normed = target.view(-1).long()\\rif args.cuda:\\rtarget_var = Variable(target_normed.cuda(non_blocking=True))\\relse:\\rtarget_var = Variable(target_normed)\\r# compute output\\r#把这一批次的数据带入模型\\routput = model(*input_var)\\r#由模型得到的结果和目标值得到损失函数\\rloss = criterion(output, target_var)\\r# measure accuracy and record loss\\rif args.task == 'regression':\\rmae_error = mae(normalizer.denorm(output.data.cpu()), target)\\rlosses.update(loss.data.cpu(), target.size(0))\\rmae_errors.update(mae_error, target.size(0))\\relse:\\raccuracy, precision, recall, fscore, auc_score = \\\\\\rclass_eval(output.data.cpu(), target)\\rlosses.update(loss.data.cpu().item(), target.size(0))\\raccuracies.update(accuracy, target.size(0))\\rprecisions.update(precision, target.size(0))\\rrecalls.update(recall, target.size(0))\\rfscores.update(fscore, target.size(0))\\rauc_scores.update(auc_score, target.size(0))\\r# compute gradient and do SGD step\\roptimizer.zero_grad()\\rloss.backward() #后向传播、计算梯度\\roptimizer.step() #传递梯度、调整参数\\r# measure elapsed time\\rbatch_time.update(time.time() - end)\\rend = time.time()\\rif i % args.print_freq == 0:\\rif args.task == 'regression':\\rprint('Epoch: [{0}][{1}/{2}]\\\\t'\\r'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\\\t'\\r'Data {data_time.val:.3f} ({data_time.avg:.3f})\\\\t'\\r'Loss {loss.val:.4f} ({loss.avg:.4f})\\\\t'\\r'MAE {mae_errors.val:.3f} ({mae_errors.avg:.3f})'.format(\\repoch, i, len(train_loader), batch_time=batch_time,\\rdata_time=data_time, loss=losses, mae_errors=mae_errors)\\r)\\relse:\\rprint('Epoch: [{0}][{1}/{2}]\\\\t'\\r'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\\\t'\\r'Data {data_time.val:.3f} ({data_time.avg:.3f})\\\\t'\\r'Loss {loss.val:.4f} ({loss.avg:.4f})\\\\t'\\r'Accu {accu.val:.3f} ({accu.avg:.3f})\\\\t'\\r'Precision {prec.val:.3f} ({prec.avg:.3f})\\\\t'\\r'Recall {recall.val:.3f} ({recall.avg:.3f})\\\\t'\\r'F1 {f1.val:.3f} ({f1.avg:.3f})\\\\t'\\r'AUC {auc.val:.3f} ({auc.avg:.3f})'.format(\\repoch, i, len(train_loader), batch_time=batch_time,\\rdata_time=data_time, loss=losses, accu=accuracies,\\rprec=precisions, recall=recalls, f1=fscores,\\rauc=auc_scores)\\r)\\rdef train(train_loader, model, criterion, optimizer, epoch, normalizer): 这里接受的参数有：训练集的DataLoader、模型、损失函数、优化器、epoch、正则化。\\nmodel.train()\\n首先，把模型的训练模式打开，这样网格中一些特殊的函数比如Dropout才会被开启。\\nfor循环遍历训练集的DataLoader\\n把得到的批数据，经过简单的处理，比如把张量上传到CUDA。\\n把批数据流入模型，再后向传播。\\n\"",
      categories: "[\"深度学习\"]",
      tags: [],
      series: [],
      date: "\"2025-05-18\""
    });
  
    searchIndex.push({
      title: "\"使用rNEMD方法计算热导率的lammps输入文件\"",
      permalink: "\"/%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/thermal_conductivity_rnemd/\"",
      content: "\"使用rNEMD方法计算热导率的LAMMPS输入文件 前言 rNEMD方法 ，又叫MP方法，计算材料热导率。\\nLAMMPS官方提供了计算脚本，但是使用的单位却是lj单位制，非常不实用，这里是我自己写的metal单位制下的脚本。\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\nin file # sample LAMMPS input script for thermal conductivity of liquid LJ\\r# Muller-Plathe method via fix thermal_conductivity\\r# settings temperature, kB and timestep\\rvariable t equal 1500\\rvariable k equal 8.6173e-5 variable dt equal 0.0005\\r# convert from LAMMPS metal units to SI\\rvariable eV2J equal 1.6022e-19 #energy convert\\rvariable A2m equal 1.0e-10 #distance convert\\rvariable ps2s equal 1.0e-12 #time convert variable convert equal ${eV2J}/${ps2s}/${A2m} # setup problem\\runits metal\\ratom_style atomic\\ratom_modify map yes\\rnewton on\\rread_data ./333\\rpair_style mace no_domain_decomposition\\rpair_coeff * * /home-ssd/Users/nsgm_lbs/train/MACE_model/MACE_model_run-123_stagetwo.model-lammps.pt O Zr Y H\\rneighbor 1.0 bin\\rneigh_modify every 500 delay 0 check no\\rminimize 1e-5 1e-7 1000 1000\\rtimestep ${dt}\\rvelocity all create $t 87287\\r# 1st equilibration run\\rreset_timestep 0\\rfix 1 all npt temp $t $t 0.05 iso 0 0 0.5\\rthermo_style custom step temp pe etotal enthalpy lx ly lz vol press\\rthermo 100\\rrun 5000\\runfix 1\\rvelocity all scale $t\\rfix 1 all nvt temp $t $t 0.05\\rrun 5000\\runfix 1\\r# 2nd equilibration run\\rcompute ke all ke/atom\\rvariable temp atom c_ke/1.5/${k}\\rfix 1 all nve\\rcompute layers all chunk/atom bin/1d z lower 0.05 units reduced\\rfix 2 all ave/chunk 10 100 1000 layers v_temp file profile.mp\\rfix 3 all thermal/conductivity 100 z 20\\rvariable tdiff equal f_2[11][3]-f_2[1][3]\\rthermo_style custom step temp epair etotal f_3 v_tdiff\\rthermo_modify colname f_3 E_delta colname v_tdiff dTemp_step\\rthermo 1000\\rrun 20000\\r# thermal conductivity calculation\\r# reset fix thermal/conductivity to zero energy accumulation\\rfix 3 all thermal/conductivity 100 z 20\\rvariable start_time equal time\\rvariable kappa equal (f_3/(time-${start_time})/(lx*ly)/2.0)*(lz/2.0)/f_ave\\rfix ave all ave/time 1 1 1000 v_tdiff ave running\\rthermo_style custom step temp epair etotal f_3 v_tdiff f_ave\\rthermo_modify colname f_3 E_delta colname v_tdiff dTemp_step colname f_ave dTemp\\rrun 20000\\rprint \\u0026quot;Running average thermal conductivity units metal: $(v_kappa)\\u0026quot;\\rvariable tc equal ${kappa}*${convert}\\rprint \\u0026quot;Running average thermal conductivity units SI: $(v_tc:%.2f)\\u0026quot;\\r\"",
      categories: "[\"分子动力学\"]",
      tags: [],
      series: "[\"LAMMPS\"]",
      date: "\"2025-05-16\""
    });
  
    searchIndex.push({
      title: "\"如何构建训练集用于训练机器学习势\"",
      permalink: "\"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%BF/mace1/\"",
      content: "\"如何构建训练集用于训练机器学习势 前言 本文将介绍如何构建一个训练集，用于训练MACE势以及DP势。需要提前利用AIMD获取DFT数据集。这里的AIMD软件是VASP。\\n软件：ASE、DeepMD-Kit\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\n构建MACE势的训练集 MACE接受的训练集非常简单，一个xyz文件，包含了各种构型和它们对应的DFT数据标签，以及单原子的DFT数据，需要额外的标签config_type=IsolatedAtom。\\n顺带一提，ASE可以输出一种所谓的 Extended XYZ format ，会把各种各样的信息（有点类似OVITO中的全局信息），放到xyz文件的第二行。这一行会很长很长。MACE所采用的训练集输入格式就是它。\\n假设我们要构建水分子的MACE势，当前所处的路径下，有两个文件夹。\\n一个名为H2O的文件夹，里面存放着进行第一性原理分子动力学后得到的OUTCAR.tar.gz文件。路径为H2O/OUTCAR.tar.gz。\\n一个名为IsolatedAtoms的文件夹，里面存放着涉及元素（这里是H、O）的单原子的单点能计算（ISPIN=2）。路径分别为IsolatedAtoms/H/OUTCAR.tar.gz和IsolatedAtoms/O/OUTCAR.tar.gz。\\n代码展示 from ase.io import read,write\\r#定义一个简单的函数用于打标签,这里可以自由更改标签的名字\\rdef addlabel(configs,energy_label='energy_dft',forces_label='forces_dft',stress_label='stress_dft',is_isolated=False):\\rif is_isolated == False:\\rfor at in configs:\\rat.info[energy_label] = at.get_potential_energy(force_consistent=True)\\rat.arrays[forces_label] = at.get_forces()\\rat.info[stress_label] = at.get_stress(voigt=True)\\rif is_isolated == True:\\rfor at in configs:\\rat.info['config_type'] = 'IsolatedAtom'\\rat.info[energy_label] = at.get_potential_energy(force_consistent=True)\\rat.arrays[forces_label] = at.get_forces()\\rat.info[stress_label] = at.get_stress(voigt=True)\\r#read()函数，这里，第一个参数是所读文件路径，第二个参数是切片slice\\rIsolatedH = read('IsolatedAtoms/H/OUTCAR.tar.gz',':')\\rIsolatedO = read('IsolatedAtoms/O/OUTCAR.tar.gz',':')\\rIsolatedAtoms = IsolatedH + IsolatedO\\raddlabel(configs=IsolatedAtoms,is_isolated=True)\\r#这里的slice的意思是从第一个结构开始到最后一个结构，每100个结构取一个\\rdb = read('H2O/OUTCAR.tar.gz','::100')\\raddlabel(configs=db)\\r#将打过标签的数据集合并\\rdb = db + IsolatedAtoms\\rwrite('trainset.xyz',db)\\r这里有趣的一点是，为什么对于单个结构的OUTCAR，也要进行切片：IsolatedH = read('IsolatedAtoms/H/OUTCAR.tar.gz',':')，而不是IsolatedH = read('IsolatedAtoms/H/OUTCAR.tar.gz')。\\n这是因为read()函数读取只有一个原子的结构是，会返回atom类的实例，而非atoms类的实例。而atom类不支持info属性，会很麻烦。\\n即便是多原子的单结构，如果想把他们合并成一个轨迹，也一定要用切片的形式\\u0026quot;:\\u0026quot;读取，因为两个atoms类的实例加和会得到一个新的atoms类的实例。\\nfrom ase.io import read,write\\r#假设H2O.cif是一个含有一个水分子的胞\\r#这样只会输出一个含有两个水分子的胞\\rdb1 = read('H2O.cif')\\rdb2 = read('H2O.cif')\\rdb = db1 + db2\\rwrite('H2O.xyz',db)\\r#使用切片后，read会返回list[atoms],再进行加和得到的是list[atoms1,atoms2],用write()函数写的时候，就能依次形成轨迹了\\rdb1 = read('H2O.cif',':')\\rdb2 = read('H2O.cif',':')\\rdb = db1 + db2\\rwrite('H2O.xyz',db)\\r所以，如果我们想让单一结构也加入到我们的数据集时，也要记得用切片的形式进行读取。不过要铭记于心的是，用切片的形式读取的其实是一个list[atoms]，要使用其中atoms实例的方法时，记得用for循环遍历其中的atoms实例。\\n\"",
      categories: "[\"机器学习势\"]",
      tags: [],
      series: "[\"ASE\",\"DeepMD-Kit\"]",
      date: "\"2025-05-16\""
    });
  
    searchIndex.push({
      title: "\"机器学习势MACE的输入文件\"",
      permalink: "\"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%BF/mace2/\"",
      content: "\"机器学习势MACE的输入文件 前言 不同MACE版本的参数设置会有一定的调查，注意查看自己的MACE版本。\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\n\"",
      categories: "[\"机器学习势\"]",
      tags: [],
      series: "[\"MACE\"]",
      date: "\"2025-05-16\""
    });
  
    searchIndex.push({
      title: "\"均方位移（MSD）-OVITO\"",
      permalink: "\"/%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/msd_ovito/\"",
      content: "\"均方位移（MSD）计算 by OVITO 前言 OVITO Python Reference — OVITO Python Reference 3.12.3 documentation 是一个开源且功能强大的分子动力学后处理软件包。\\n本文将介绍如何利用 OVITO python module 计算某类元素原子在一段轨迹内的均方位移。\\n适用于无机非晶体，其他体系慎用。\\n软件：OVITO、matplotlib、numpy\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\nOVITO版 代码展示 from ovito.io import import_file, export_file\\rfrom ovito.modifiers import CalculateDisplacementsModifier\\rfrom ovito.modifiers import SelectTypeModifier,InvertSelectionModifier,DeleteSelectedModifier,ExpressionSelectionModifier\\rimport numpy as np\\rimport matplotlib.pyplot as plt\\r#万物起源 import_file ，导入一段要计算的轨迹\\rpipeline = import_file(\\u0026quot;1.dump\\u0026quot;)\\r#添加 SelectTypeModifier 修饰器\\r#设置参数 property = 'Particle Type' 指定选择的类型（这里我们指定的是原子类型）\\r#设置参数 types = {4} 指定具体的原子类型，这里是这个轨迹中的 4 原子（这个值要根据自己的体系修改），这里用数字代表原子是因为我使用的lammps的输出风格没有记录原子的元素符号，如果你的轨迹里记录的是 元素符号 信息，比如说 VASP 输出的 XDATCAR，则需要用类似于 types = {\\u0026quot;H\\u0026quot;} 的写法\\rpipeline.modifiers.append(SelectTypeModifier(property = 'Particle Type', types = {4}))\\r#添加 InvertSelectionModifier 修饰器，进行原子反选，为剔除不需要计算的原子做准备\\rpipeline.modifiers.append(InvertSelectionModifier()) #添加 DeleteSelectedModifier 修饰器，删除上一行代码反选的原子，留下需要计算的原子\\rpipeline.modifiers.append(DeleteSelectedModifier(operate_on= {'particles'})) #添加 CalculateDisplacementsModifier 修饰器，指定计算 MSD 的参考结构，这里 reference_frame = 0 代表初始结构是参考结构\\rreference_frame = 0\\rpipeline.modifiers.append(CalculateDisplacementsModifier(reference_frame=0)) #a subclass of ovito.pipeline.ReferenceConfigurationModifier\\r#自定义一个修饰器函数，用于将 per-particle displacement 转化为相应元素的均方位移\\r#本文的 OVITO小知识 将简单介绍自定义修饰器是如何工作的\\rdef calculate_msd(frame, data):\\r#用一个变量 displacement_magnitudes 记录 data.particles['Displacement Magnitude']，简化代码\\rdisplacement_magnitudes = data.particles['Displacement Magnitude']\\r#计算 MSD （将所有原子位移的平方加和然后求平均），OVITO 的数据可以直接和 numpy 交互，nice\\rmsd = np.sum(displacement_magnitudes ** 2) / len(displacement_magnitudes) #把计算的 MSD 传递给 data (DataCollection类)\\rdata.attributes[\\u0026quot;MSD\\u0026quot;] = msd #添加自定义 calculate_msd 修饰器\\rpipeline.modifiers.append(calculate_msd)\\r#计算 Pipeline, 得到time vs MSD的数据\\rtable = [] #用于存放数据，time vs MSD\\rfor frame,data in enumerate(pipeline.frames):\\rif frame \\u0026gt;= reference_frame:\\r#这里的 *10 一定要根据自己的计算调整，我的轨迹在lammps计算设置：时间步是0.5fs，每20步输出一帧，所以轨迹中每帧其实经历了10fs，所以乘以10\\r#我们 time vs MSD 的x横坐标单位是fs，也可以是别的，自己调整\\rtime = (frame-reference_frame)*10 table.append([time,data.attributes['MSD']])\\r#.csv文件还是比较高级的，比纯txt好些，delimiter 指定间隔符为 \\u0026quot;,\\u0026quot; ,这样方便直接excel打开\\rnp.savetxt(\\u0026quot;msd_data.csv\\u0026quot;,table,delimiter=\\u0026quot;,\\u0026quot;)\\rOVITO小知识 受限于Python基础和时间精力的限制，以下内容皆为我个人的有限理解，未能严格考究，仅供参考。\\n自定义修饰器 这里介绍的自定义修饰器，按官方说法是 Simple programming interface，它是一个函数，接受两个基本输入：frame 和 data。\\n即，def modify(frame: int, data: DataCollection):\\n注意：不要返回任何值，数据都应保存在 data 这个 DataCollection 类中\\ndef calculate_msd(frame, data):\\rdisplacement_magnitudes = data.particles['Displacement Magnitude']\\rmsd = np.sum(displacement_magnitudes ** 2) / len(displacement_magnitudes) data.attributes[\\u0026quot;MSD\\u0026quot;] = msd DataCollection 类的 attributes 属性储存了这个实例数据集的所有的global attributes（全局信息）。\\nattributes 属性是由经过@propert装饰器装饰的方法，这个方法返回一个辅助类，也可以叫功能类（_AttributesView）的实例，_AttributesView是抽象基类MutableMapping的子类。\\n_AttributesView类用于实现类似字典的功能。\\n每次访问attributes 属性时，都会返回一个_AttributesView类的实例，这个实例会先通过__init__接收DataCollection实例的数据，并支持对其进行字典操作。\\n所以，当我们data.attributes[\\u0026quot;MSD\\u0026quot;] = msd时，是通过_AttributesView类提供的字典操作，把MSD数据添加到了DataCollection实例中。\\n由于_AttributesView类实现了__repr__方法，我们可以直接print(data.attributes)来查看包含了哪些全局信息。\\nfor frame,data in enumerate(pipeline.frames):语句 DataCollection要pipeline经过compute()方法得到，但这个例子中并没有见到compute()方法。其实隐藏在了这句for循环中。\\npipeline.frames会返回一个迭代器，这个迭代器中yield产生每一帧的DataCollection。\\nReferences 1. OVITO官方的MSD脚本 \"",
      categories: "[\"分子动力学\"]",
      tags: [],
      series: "[\"OVITO\"]",
      date: "\"2025-05-13\""
    });
  
    searchIndex.push({
      title: "\"稍微深入一些Python中的类（class）\"",
      permalink: "\"/%E4%BB%A3%E7%A0%81/class_in_python/\"",
      content: "\"稍微深入一些Python中的类（class） 前言 **类（class）**在python代码中几乎无处不在，但在近日的学习中发现，我对它真是了解甚少，甚至基础结构都不能熟稔于心，故开此笔记认真学习。和我一起重新认识一下它吧。\\n一个简单的类 #code 1\\rclass Dog:\\r# 类属性\\rspecies = \\u0026quot;Dog\\u0026quot;\\r# 初始化方法\\rdef __init__(self, name, age):\\rself.name = name\\rself.age = age\\r# 实例方法\\rdef bark(self):\\rreturn print(\\u0026quot;旺旺\\u0026quot;)\\rprint(mydog.species)\\rprint(mydog.name,mydog.age)\\rmydog.bark()\\r#输出为：\\r#Dog\\r#doudou 2\\r#旺旺\\r类是一种对数据进行计算操作的蓝图，离不开属性和方法。\\ncode 1中，我们定义了一个 Dog 类，并对它进行了实例化，生成了一个对象。\\n这个类的结构，很简单。\\n首先是放在第一部分的类属性。类属性是直接在类中定义变量。所有通过这个类生成的对象都具有这些属性。\\n然后是放在第二部分的诸多方法，其实就是一个个的函数。\\ndef __init__(self, name, age):\\rself.name = name\\rself.age = age\\r_init_方法叫初始化方法，是魔法方法的一种。让实例初始化时就具有name和age属性和相应的值。具体表现为mydog = Dog(\\u0026quot;doudou\\u0026quot;,2)，这个类在初始化时就需要两个参数才能转变为实例。\\npython中所有的实例方法，包括__init__方法，第一个参数都必须是self，用于区别普通函数和方法。\\n默认值，在定义方法的时候，可以传入默认值，这可以保证在不传入参数时，也能生成一个默认实例。\\ndef __init__(self, name=\\u0026quot;小卡拉米\\u0026quot;, age=5):\\rself.name = name\\rself.age = age\\r这样的__init__方法就保证了，即使狗主人忘记了填写信息，mydog = Dog()也能正常工作，但默认的狗狗是小卡拉米，还是应该记得为自己的狗狗正确填写信息呦。（对于其他实例方法也适用）\\n在一般的实例方法中，也可以设置参数，定义属性。我们看一段新的代码:\\n#code 2-1\\rclass phone:\\rowner = \\u0026quot;xiyangyang\\u0026quot;\\rdef __init__(self,type=\\u0026quot;huawei\\u0026quot;):\\rself.type = type\\rdef call(self,number=10086):\\rself.number = number\\rprint(\\u0026quot;the number is {}\\u0026quot;.format(self.number))\\rprint(phone.owner)\\rmyphone = phone(\\u0026quot;apple\\u0026quot;)\\rmyphone.call(110)\\rprint(\\u0026quot;what number is called? \\u0026quot;,myphone.number)\\r这里在phone类的call方法下，定义了一个实例属性，并需要一个输入number指定值。\\n但实例方法的参数不一定都是实例属性，且看以下代码：\\n#code 2-2\\rclass phone:\\rdef __init__(self,type=\\u0026quot;huawei\\u0026quot;):\\rself.type = type\\rdef call(self,number=10086):\\rprint(\\u0026quot;the number is {}\\u0026quot;.format(number))\\rmyphone = phone(\\u0026quot;apple\\u0026quot;)\\rmyphone.call(110)\\r这样定义call方法，在使用的时候也需要传入一个number，但是这个number不是这个实例的属性，无法随时查看。\\n这就是说，对于类中的方法，目前涉及的参数有三类，一个是默认参数self，一个是实例属性参数，一个是普通参数。\\n截至目前，我们已经了解了一个简单类的结构。即：声明类属性，声明实例方法（其中涉及到添加实例属性）。 由类生成实例时的参数接口，由魔法方法__init__定义，而一般实例方法的接口，在调用方法的时候才会出现。这也就意味着，__init__方法中定义的实例属性，是伴生的，只要生成实例就存在。而随一般实例方法定义的实例属性，则需要首次调用后才存在。 魔法方法 在python中，所有被双下划线包围的方法，统称为魔法方法。太多了，我这里只记录我遇到的，不定时更新。\\n1.__init__方法 这个魔法方法用于类的初始化。规定了实例化类时接受的参数。\\n2.__len__方法 #code 3-1\\rclass menu:\\rrestaurant=\\u0026quot;hepingfandian\\u0026quot;\\rdef __init__(self,foods):\\rself.foods = foods\\rdef __len__(self):\\rcount = 0\\rfor food in foods:\\rcount = count + 1\\rreturn count\\rmymenu = menu([\\u0026quot;宫保鸡丁\\u0026quot;,\\u0026quot;鱼香肉丝\\u0026quot;,\\u0026quot;米饭\\u0026quot;])\\rprint(len(mymenu))\\r#输出：3\\r这个魔法方法让实例可以被用len()函数统计长度，核心是要返回一个整数，至于这个整数是如何计算得到的，这部分内容就由自己定义了。\\n3.__getitem__方法 #code 3-2\\rclass menu:\\rrestaurant=\\u0026quot;hepingfandian\\u0026quot;\\rdef __init__(self,foods):\\rself.foods = foods\\rdef __getitem__(self,key):\\rreturn 10\\rmymenu = menu([\\u0026quot;宫保鸡丁\\u0026quot;,\\u0026quot;鱼香肉丝\\u0026quot;,\\u0026quot;米饭\\u0026quot;])\\rprint(mymenu[\\u0026quot;霸王餐\\u0026quot;])\\r#输出：10\\r这个魔法方法可以让实例像字典或列表一样，实现键值对和索引切片的功能。具体来说，就是根据条件返回不同的值。不然就像上述代码一样，客人要吃霸王餐，对应的值却是10。\\n#code 3-3\\rclass menu:\\rrestaurant=\\u0026quot;hepingfandian\\u0026quot;\\rdef __init__(self,foods):\\rself.foods = foods\\rdef __getitem__(self,key):\\rif not isinstance(key,str):\\rraise TypeError(\\u0026quot;Attribute key must be a string\\u0026quot;)\\rif key == \\u0026quot;霸王餐\\u0026quot;:\\rreturn \\u0026quot;你吃牛魔\\u0026quot;\\rfor food in self.foods:\\rif food == key:\\rreturn self.foods[key]\\rraise KeyError(f\\u0026quot;Food '{key}' does not exist in menu.\\u0026quot;)\\rmymenu = menu({\\u0026quot;宫保鸡丁\\u0026quot;:25,\\u0026quot;鱼香肉丝\\u0026quot;:10,\\u0026quot;米饭\\u0026quot;:2})\\rprint(mymenu[\\u0026quot;霸王餐\\u0026quot;])\\rprint(mymenu[\\u0026quot;米饭\\u0026quot;])\\r#输出：你吃牛魔\\r#输出：2\\r__getitem__方法可以为实例提供独特的接口，就是[参数]。这里的参数可以是字符串、数字或者切片，使用起来像字典或者列表。而一般方法的使用则要.onemethod(参数1，参数2，...)。\\n#code 3-4\\rclass top3:\\rip = \\u0026quot;harbin\\u0026quot;\\rdef __init__(self,school):\\rself.school = school\\rdef __getitem__(self,index):\\rif isinstance(index,int):\\rreturn self.school[index]\\rif isinstance(index,slice):\\rreturn self.school[index]\\rTop3InMyheart = top3([\\u0026quot;qinghua\\u0026quot;,\\u0026quot;beida\\u0026quot;,\\u0026quot;hagongda\\u0026quot;])\\rprint(\\u0026quot;who is top3?\\u0026quot;,Top3InMyheart[0:3])\\rprint(Top3InMyheart.ip)\\r这段代码定义了一个名叫top3类，实现了索引和切片功能，同时可以这个类的实例拥有列表不同的功能，即这个类的实例有一个类属性：ip。让我们可以得知这个排名来自harbin。\\n4.__setitem__方法 #code 3-5\\rclass games:\\rdef __init__(self):\\rself.games={}\\rdef __setitem__(self,key,value):\\rself.games[key]=value\\rdef __getitem__(self,key):\\rreturn self.games[key]\\rMyLoveGames = games()\\rMyLoveGames[1] = \\u0026quot;原神\\u0026quot;\\rprint(MyLoveGames[1])\\r#输出：原神\\r这个魔法方法让类的实例可以像字典或者列表一样，添加新的键值对或者添加新的索引和值。\\n5.__delitem__方法 #code 3-6\\rclass games:\\rdef __init__(self):\\rself.games={}\\rdef __setitem__(self,key,value):\\rself.games[key]=value\\rdef __getitem__(self,key):\\rreturn self.games[key]\\rdef __delitem__(self,key):\\rdel self.games[key]\\rMyLoveGames = games()\\rMyLoveGames[1] = \\u0026quot;原神\\u0026quot;\\rMyLoveGames[1] = \\u0026quot;DOTA2\\u0026quot;\\rprint(MyLoveGames[1])\\rdel MyLoveGames[1]\\r#输出：DOTA2\\r这个魔法方法可以让类的实例使用del关键字来删除键或索引。\\n6.__iter__方法 #code 3-7\\rclass games:\\rdef __init__(self):\\rself.games={}\\rdef __setitem__(self,key,value):\\rself.games[key]=value\\rdef __getitem__(self,key):\\rreturn self.games[key]\\rdef __delitem__(self,key):\\rdel self.games[key]\\rdef __iter__(self):\\rfor game in self.games:\\ryield game\\rMyLoveGame = games()\\rMyLoveGame[0] = \\u0026quot;原神\\u0026quot;\\rMyLoveGame[3] = \\u0026quot;DOTA2\\u0026quot;\\rMyLoveGame[2] = \\u0026quot;明日方舟\\u0026quot;\\rdel MyLoveGame[2]\\rfor game in MyLoveGame:\\rprint(game)\\r#输出：0\\r#输出：3\\r这个魔法方法让实例变为可迭代对象，可以进行for循环。也可以用iter()函数生成迭代器。这里涉及到一个头疼的yield关键字，又涉及到生成器和迭代器。还是另开一文来记录吧。\\n简单来讲，当一个可迭代对象遇到for循环事件时，会自动转到它的__iter__方法，又因为这里的__iter__方法含有yield关键字，所以不会立即执行，而是得到了一个生成器对象。\\n紧接着，for循环又根据这个生成器对象的__next__方法（注意，这里是生成器对象的__next__方法，而非是，我们这个自定义类的__next__方法，其实，我们这里也没有写__next__方法）把得到的值赋值给for game in MyLoveGame:中的game，然后打印。\\n在进行第二次循环的时候，同样地，再次进入实例的__iter__方法，得到同一个生成器对象。并对这个生成器对象再此施加__next__方法，并把得到的值赋值给for game in MyLoveGame:中的game，然后打印。\\n谁是for game in MyLoveGame:中的game值？ __iter__方法本质上定义了一个生成器对象，而非函数。for循环会不断调用这个生成器对象__next__方法并把得到的值传递给game。\\n7.__repr__方法 #code 3-8\\rimport collections.abc\\rclass games(collections.abc.MutableMapping):\\rdef __init__(self):\\rself.games={}\\rdef __len__(self):\\rreturn 0\\rdef __setitem__(self,key,value):\\rself.games[key]=value\\rdef __getitem__(self,key):\\rreturn self.games[key]\\rdef __delitem__(self,key):\\rdel self.games[key]\\rdef __iter__(self):\\rfor game in self.games:\\ryield game\\rdef __repr__(self):\\rreturn repr(dict(self))\\rMyLoveGame = games()\\rMyLoveGame[0] = \\u0026quot;原神\\u0026quot;\\rMyLoveGame[1] = \\u0026quot;DOTA2\\u0026quot;\\rMyLoveGame[2] = \\u0026quot;明日方舟\\u0026quot;\\rprint(MyLoveGame)\\r#输出：{0: '原神', 1: 'DOTA2', 2: '明日方舟'}\\r这里让自定义的games继承了一个抽象基类，这样return repr(dict(self))才不报错。\\n这个魔法方法让实例可以直接使用实例时，返回一个官方字符串。\\n继承与抽象基类 如何继承父类 #code 4-1\\rclass student:\\rdef __init__(self,gender=1,age=18):\\rself.gender = gender\\rself.age = age\\rclass xiaoxiaobuaigugujiao(student):\\rdef interests(self,data):\\rself.interests = data\\rprint(self.interests)\\rme = xiaoxiaobuaigugujiao()\\rprint(me.gender)\\rprint(me.age)\\rme.interests(\\u0026quot;games\\u0026quot;)\\r在定义子类的时候，在子类的名字后加上(父类名)即可。\\nsuper()函数 #code 4-2\\rclass student:\\rdef __init__(self,gender=1,age=18):\\rself.gender = gender\\rself.age = age\\rclass xiaoxiaobuaigugujiao(student):\\r#子类中，当覆盖父类中的同名方法后，与父类同名方法的默认参数也要重新写一篇，不会继承\\rdef __init__(self,gender=1,age=18,height=170):\\r#调用父类的构造方法\\rsuper().__init__(gender,age)\\r#额外补充\\rself.height = height\\rdef interests(self,data):\\rself.interests = data\\rprint(self.interests)\\rme = xiaoxiaobuaigugujiao()\\rprint(me.gender)\\rprint(me.age)\\rprint(me.height)\\rme.interests(\\u0026quot;games\\u0026quot;)\\rsuper()函数通常出现在子类定义方法的时候，用于调用父类的构造方法，并可以加以补充，这里我们在子类初始化时，就调用了父类student的构造方法，并额外补充了height。\\n抽象基类 抽象基类只能被继承。它要求子类必须实现某些抽象方法。\\n\"",
      categories: "[\"代码\"]",
      tags: [],
      series: "[\"python\"]",
      date: "\"2025-05-13\""
    });
  
    searchIndex.push({
      title: "\"径向分布函数（RDF）-OVITO\"",
      permalink: "\"/%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/rdf_ovito/\"",
      content: "\"径向分布函数（RDF）计算 by OVITO 前言 OVITO Python Reference — OVITO Python Reference 3.12.3 documentation 是一个开源且功能强大的分子动力学后处理软件包。\\n本文将介绍如何利用 OVITO python module 计算单个结构以及一段轨迹（多个结构）内的径向分布函数。\\n适用于无机非晶体，其他体系慎用。\\n软件：OVITO、matplotlib、numpy\\n注意：本文仅供参考，欢迎指出错误或分享补充。无能力提供任何指导，求教者切勿留言。\\nThe partial RDFs of a single crystal structure 代码展示 #这段代码用于计算 RDF by OVITO from ovito.io import import_file from ovito.modifiers import CoordinationAnalysisModifier import numpy as np #导入一个氧化锆（ZrO2）的cif文件，所有OVITO支持的输入文件格式都可以（确保这个.py文件的路径下有这样一个cif文件，也可以稍微修改指定结构路径） pipeline = import_file(\\u0026quot;ZrO.cif\\u0026quot;) #施加一个名叫 CoordinationAnalysisModifier 的修饰器，cutoff用于控制截断半径，number_of_bins用于控制网格细分度（大小100-1000内都可以试试） pipeline.modifiers.append(CoordinationAnalysisModifier(cutoff = 5.0, number_of_bins = 500,partial=True)) #进行计算 rdf_table = pipeline.compute().tables['coordination-rdf'] #得到用于画图的横纵坐标，默认第一列是x轴数据，其余列是y轴数据 total_rdf = rdf_table.xy() #记录total_rdf中y轴数据对应是什么类型的pair-wise #这个例子中，输出为： #g(r) for pair-wise type combination O-O: #g(r) for pair-wise type combination O-Zr: #g(r) for pair-wise type combination Zr-Zr: #说明total_rdf是一个四列的数据，第一列是x轴坐标（其实是bin），第二列就是不同pair-wise的RDF数据，依次为 O-O,O-Zr,Zr-Zr rdf_names = rdf_table.y.component_names for component, name in enumerate(rdf_names): print(\\u0026quot;g(r) for pair-wise type combination %s:\\u0026quot; % name) #将total_rdf保存为txt文件，用于后续画图 np.savetxt(\\u0026quot;total_rdf.txt\\u0026quot;, total_rdf) #这段代码用于绘图 import numpy as np import matplotlib.pyplot as plt rdf_table = np.loadtxt('total_rdf.txt') #g(r) for pair-wise type combination O-O: #g(r) for pair-wise type combination O-Zr: #g(r) for pair-wise type combination Zr-Zr: #这里取的是 total_rdf.txt 中的第一列（对应[:,0]）和第三列（对应[:,2]），所以绘制的是 Zr-O pair-wise的partial RDF plt.plot(rdf_table[:,0], rdf_table[:,2]) #matplotlib的常规设置，问问万能的小迪老师吧 title_font = {'fontsize': 24, 'fontfamily': 'Times New Roman'} xlabel_font = {'fontsize': 22, 'fontfamily': 'Times New Roman'} ylabel_font = {'fontsize': 22, 'fontfamily': 'Times New Roman'} plt.title(\\u0026quot;RDF Zr-O\\u0026quot;, fontdict=title_font,pad=8) plt.xlabel(xlabel='distance r',fontdict=xlabel_font,loc='center',labelpad=8) plt.ylabel(ylabel='g(r)',fontdict=ylabel_font,loc='center',labelpad=8) plt.tick_params(axis='both', which='major', labelsize=16, direction='in') ax = plt.subplot() #因为只有一个静态结构，pair-wise的某些峰很高，所以这里的y轴坐标上限设置大一些，为200，可灵活改变 #x轴设置为6，稍大于截断半径cutoff即可，因为本身也只在截断半径以内统计 ax.set_ylim(0,200) plt.xlim(0,6) fig = plt.gcf() fig.set_size_inches(1200/100, 800/100) plt.savefig('output.png', dpi=100) plt.show() 结果展示 The partial RDFs of trajectories 代码展示 #这段代码用于计算一定时间内（一段轨迹）的平均 RDFs from ovito.io import import_file from ovito.modifiers import CoordinationAnalysisModifier,TimeAveragingModifier import numpy as np #读入轨迹文件，这里是利用 VASP 进行 AIMD 后得到的 XDATCAR 文件 pipeline = import_file(\\u0026quot;XDATCAR\\u0026quot;) #打印轨迹中的结构数 print(\\u0026quot;Number of MD frames:\\u0026quot;, pipeline.num_frames) #添加修饰器，与单个晶体结构相比，多了 TimeAveragingModifier 修饰器 pipeline.modifiers.append(CoordinationAnalysisModifier(cutoff = 5.0, number_of_bins = 500,partial=True)) pipeline.modifiers.append(TimeAveragingModifier(operate_on='table:coordination-rdf')) #计算 RDFs 数据 total_rdf = pipeline.compute().tables['coordination-rdf[average]'].xy() #记录pair-wise类型 rdf_names = pipeline.compute().tables['coordination-rdf[average]'].y.component_names for name in rdf_names: print(\\u0026quot;g(r) for pair-wise type combination %s:\\u0026quot; % name) #输出数据，用于后续绘图，不再重复 np.savetxt('rdf.txt', total_rdf, delimiter='\\\\t') OVITO小知识 受限于Python基础和时间精力的限制，以下内容皆为我个人的有限理解，未能严格考究，仅供参考。\\n在 The partial RDFs of a single crystal structure 的计算代码中 pipeline = import_file(\\u0026quot;ZrO.cif\\u0026quot;) 这段代码将外来的 cif 文件转化为 ovito 的一个 Pipeline 类的实例。\\n然后我们添加了修饰器，代码省略。\\n再然后是调用 Pipeline 类的 compute() 方法，得到一个非常重要的 DataCollection 类的实例。\\n再访问这个 DataCollection 实例的 tables 属性就得到一个DataTable 类的实例（在DataCollection 类的定义中，用 @property 的 装饰器语法将 tables() 方法转化为了 tables 属性，所以与 compute() 方法相比，不需要 () 了）\\nDataTable 类用于储存绘制直方和2d图的数据，当我们添加 CoordinationAnalysisModifier 修饰器后，经过 compute() 方法后，相应的 partial RDFs 数据就会被储存在这里，可以通过名为 'coordination-rdf' 的 键(key) 来检索。\\n我觉得 DataTable 类的实例绝非一个简单的字典，但把它当作字典来理解会比较容易。\\nrdf_table = pipeline.compute().tables['coordination-rdf'] 即，rdf_table 仍然属于 DataTable 类。接着用 .y.component_names 来获取 多组y轴数据 的表头。这里太复杂了，我也没看太懂，不展开了。\\nrdf_names = rdf_table.y.component_names 在 The partial RDFs of trajectories 的计算代码中 原本 compute() 方法只能用于单个结构，对于 trajectories 需要指定是哪个结构。\\n但添加 TimeAveragingModifier 修饰器后，允许我们计算时间相关量的平均值。详见：https://docs.ovito.org/python/modules/ovito_modifiers.html#ovito.modifiers.TimeAveragingModifier\\nReferences ovito.modifiers — OVITO Python Reference 3.12.3 documentation \"",
      categories: "[\"分子动力学\"]",
      tags: [],
      series: "[\"OVITO\"]",
      date: "\"2025-05-12\""
    });
  

  debug("当前本站共有 " + searchIndex.length + " 篇文章");
  console.log("搜索索引:", searchIndex);

  
  const options = {
    includeScore: true,
    threshold: 0.6, 
    keys: [
      { name: 'title', weight: 0.7 },
      { name: 'content', weight: 0.3 },
      { name: 'tags', weight: 0.5 },
      { name: 'categories', weight: 0.5 },
      { name: 'series', weight: 0.5 }
    ]
  };

  
  const fuse = new Fuse(searchIndex, options);

  document.addEventListener('keydown', function(event) {
    if (event.key === 'Escape') {
      document.getElementById('search-input').value = "";
      var res = document.getElementById('search-results');
      while (res.firstChild) {
        res.removeChild(res.firstChild);
      }
      debug("当前本站共有 " + searchIndex.length + " 篇文章");
      document.getElementById('search-input').focus();
    }
  });

  
  document.getElementById('search-input').addEventListener('input', function(e) {
    const searchTerm = e.target.value.trim();
    const resultsContainer = document.getElementById('search-results');

    
    resultsContainer.innerHTML = '';

    if (searchTerm === '') {
        debug("当前本站共有 " + searchIndex.length + " 篇文章");
      return;
    }

    debug("搜索词: '" + searchTerm + "'");

    
    const results = fuse.search(searchTerm);
    debug("找到 " + results.length + " 条结果");
    console.log("搜索结果:", results);

    if (results.length === 0) {
      resultsContainer.innerHTML = '<p>没有找到相关文章</p>';
      return;
    }

    results.forEach(result => {
      const item = result.item;
      const resultItem = document.createElement('div');
      resultItem.className = 'search-result-item';

      const date = document.createElement('span');
      date.textContent = item.date.replace(/^"|"$/g, '') + '   ';
      date.className = 'search-result-date';

      const link = document.createElement('a');
      link.href = item.permalink.replace(/^"|"$/g, '');
      link.textContent = item.title.replace(/^"|"$/g, '');
      link.className = 'search-result-title';

      resultItem.appendChild(date);
      resultItem.appendChild(link);
      resultsContainer.appendChild(resultItem);
    });
  });
</script>

<style>
 
.search-container {
    margin: 2rem 0;
    text-align: left;
}

#search-input {
    width: 100%;
    padding: 0.8rem;
    font-size: 1rem;
    border: 1px solid #ddd;
    border-radius: 4px;
    margin-bottom: 1rem;
}

.search-results-list {
    list-style: none;
    padding: 0;
    margin: 1rem 0;
}

.search-result-item {
    padding: 0.7rem;
    margin-bottom: 0.5rem;
    border-bottom: 1px solid #eee;
    display: flex;
    align-items: center;
}

.search-result-date {
    color: #666;
    font-size: 0.9rem;
    margin-right: 0.5rem;
    text-wrap: nowrap;
}

.search-result-title {
    font-weight: bold;
    color: #1a73e8;
    text-decoration: none;
}

.search-result-title:hover {
    text-decoration: underline;
}
</style>

<footer>
  
  <hr />
  © 
<a href="https://liubaoshuai1402.github.io/"  target="_blank" rel="noopener noreferrer" >xiaoxiaobuaigugujiao</a>
 2025 &ndash; 2025 | 🎨
<a href="https://github.com/captainwc/captainwc.github.io/tree/main/themes/ksimple"  target="_blank" rel="noopener noreferrer" >KSimple</a>
  

</footer>
</body>

</html>
